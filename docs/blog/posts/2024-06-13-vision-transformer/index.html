<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-06-13">

<title>Zohaib Khan - Implementing a Vision Transformer from Scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../files/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Zohaib Khan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../files/cv.pdf"> 
<span class="menu-text">CV</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/zohaib-khan-5a76611a6/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zohaib-khan5040"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Implementing a Vision Transformer from Scratch</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 13, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction-to-vision-transformers" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-vision-transformers">Introduction to Vision Transformers</h2>
<p>Vision Transformers are a rather recent innovation in the field of Computer Vision (though in the fast-paced world of Machine Learning, they are rather old) introduced in the paper <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>.</p>
<p>What’s so special about this model is how it departs from traditional Convolutional Neural Network (CNN) type processing, and leverages ideas from Natural Language Processing - the Transformer architecture introduced in 2017 by <a href="https://arxiv.org/abs/1706.03762">Vaswani et al.</a> to perform Machine Translation (an inherently language-based task).</p>
<p>At the heart of the Vision Transformer is the concept of self-attention. This mechanism allows the model to weigh the importance of different elements in the input, considering the global context of the data. In the case of NLP, self-attention helps the model understand the relationships between words in a sentence, regardless of their position. Similarly, in the Vision Transformer, self-attention enables the model to capture dependencies between different <strong>patches</strong> of an image, allowing it to learn rich and complex features that are crucial for tasks like image classification and object detection.</p>
<p>The coolest part of Transformers generally is its ubiquitous nature: with very minor changes (specifically just the Positional and Patch Embedding modules), we can adapt the original architecture to the world of Computer Vision. This means that the same lessons we can learn from this model can be applied just as easily to understanding models like GPT-2 and members of the LLaMA family.</p>
<p>In this article, we will implement a Vision Transformer from the ground up, load in weights from a pretrained checkpoint, and adapt it to a simple task of Image Classification.</p>
<p>This implementation has been inspired a lot from the <a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py">timm implementation</a> - many of the snippets will be similar but this simplified implementation is intended to give more flexibility in terms of tinkering with the model and bringing in your own tweaks.</p>
<div id="cell-2" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional, Tuple, Callable, Optional, Type, Union</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> clear_output</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="getting-our-data" class="level2">
<h2 class="anchored" data-anchor-id="getting-our-data">Getting our Data</h2>
<p>Let’s start off by bringing in our dataset that we’ll be using going forward.</p>
<p>We could just implement the components willy-nilly but it helps to perform the forward passes on actual tensors as we go, and understand the shapes.</p>
<p>Let’s keep it simple and use the CIFAR-100 dataset; <code>torchvision</code> makes it very easy to use.</p>
<div id="cell-4" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define some variables pertaining to our dataset</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">224</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>TRAIN_TFMS <span class="op">=</span> transforms.Compose([</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">256</span>, <span class="dv">256</span>)),</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    transforms.RandomHorizontalFlip(),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize([<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>TEST_TFMS <span class="op">=</span> transforms.Compose([</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">256</span>, <span class="dv">256</span>)),</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize([<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_cifar100_dataset(root: <span class="bu">str</span>):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    trainset <span class="op">=</span> datasets.CIFAR100(</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        root, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>TRAIN_TFMS</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    valset <span class="op">=</span> datasets.CIFAR100(</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        root, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>TEST_TFMS</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trainset, valset</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the Dataset</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>train_ds, val_ds <span class="op">=</span> get_cifar100_dataset(<span class="st">"./data"</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn these into DataLoaders</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> DataLoader(train_ds, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>val_dl <span class="op">=</span> DataLoader(val_ds, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>clear_output()</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's visualize our data for fun</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>grid_img <span class="op">=</span> torchvision.utils.make_grid(</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">=</span><span class="bu">next</span>(<span class="bu">iter</span>(train_dl))[<span class="dv">0</span>], <span class="co"># grab a batch of images from the DataLoader</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    nrow<span class="op">=</span><span class="dv">4</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>plt.imshow(grid_img.permute(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)) <span class="co"># move the channel axis to the end</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-3-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-5" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get an instance from the dataset - returns a tuple of (input, ground truth)</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> train_ds[<span class="dv">0</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([3, 224, 224])</code></pre>
</div>
</div>
<p>Some remarks about the above cells:</p>
<ul>
<li><p>The <code>IMAGE_SIZE</code> being <span class="math inline">\(224\)</span> specifically is because we will later load in a Vision Transformer checkpoint that was pretrained on images that were <span class="math inline">\((3, 224, 224)\)</span> in shape.</p></li>
<li><p>The <code>Resize</code> and <code>CenterCrop</code> combination here is a common technique for processing images into a specific size.</p></li>
<li><p>The <code>Normalize</code> transform’s values for the mean and standard deviation looks rather strange, but it again follows what the checkpoint we will load in later was trained on. This is rather irritating since the statistics from the ImageNet dataset are very different, so this feels a bit of an anomaly in some ways.</p></li>
</ul>
</section>
<section id="embedding-an-input-image" class="level2">
<h2 class="anchored" data-anchor-id="embedding-an-input-image">Embedding an Input Image</h2>
<p>To leverage the transformer architecture for images, we first need to convert an image into a format that the model can process, similar to how words are tokenized and embedded in NLP tasks. This process involves dividing the image into smaller, non-overlapping patches and then embedding these patches into vectors—a step analogous to generating token embeddings from words.</p>
<p>Imagine an image as a grid of pixels, where each pixel carries information about color and intensity. Instead of processing the entire image at once, the Vision Transformer <strong>splits the image into fixed-size patches</strong>, treating each patch as a “token” in the sequence. For instance, an image of size <span class="math inline">\((224,224)\)</span> pixels could be divided into patches of size <span class="math inline">\((16,16)\)</span>, resulting in a grid of <span class="math inline">\((14 \times 14)\)</span> patches if each patch is also <span class="math inline">\((16,16)\)</span> pixels. This transformation effectively turns a 2D image into a 1D sequence of patches, where each patch contains a small portion of the image’s data.</p>
<p>Once the image is divided into patches, the next step is to <strong>embed each patch into a vector space</strong> that the transformer can work with. This is where convolutional layers come into play. By applying a convolutional layer with an appropriate kernel size and stride, we can extract features from each patch and represent these features as embedding vectors. Each embedding vector corresponds to a specific patch and captures its local information—such as edges, textures, and colors—while also compressing the data into a more manageable form for the transformer.</p>
<p>This patch embedding process is akin to the embedding of words in NLP. Just as words are embedded into vectors that encapsulate their semantic meaning, image patches are embedded into vectors that represent visual features. These embeddings are then fed into the transformer model, which applies self-attention to learn relationships and dependencies between different parts of the image. This enables the Vision Transformer to understand the global structure of the image, recognizing patterns and objects in a manner similar to how it understands sentences in language tasks.</p>
<p>TLDR: - The Vision Transformer can treat an image as a sequence of <strong>fixed-size patches</strong>.</p>
<ul>
<li>Each patch is converted into a <strong>feature vector</strong>, and this is easily done using convolutional layers.</li>
</ul>
<div id="cell-7" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>PATCH_SIZE <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's explore how we end up with 14x14 patches with the hyperparameters defined so far</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>number_of_patches <span class="op">=</span> <span class="bu">int</span>((IMAGE_SIZE<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> PATCH_SIZE<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>PATCH_SIZE<span class="op">=</span><span class="sc">}</span><span class="ss">, we get </span><span class="sc">{</span>number_of_patches<span class="op">=</span><span class="sc">}</span><span class="ss"> for each channel."</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This is what we expected as </span><span class="sc">{</span><span class="dv">14</span><span class="op">*</span><span class="dv">14</span><span class="op">=</span><span class="sc">}</span><span class="ss">."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using PATCH_SIZE=16, we get number_of_patches=196 for each channel.
This is what we expected as 14*14=196.</code></pre>
</div>
</div>
<div id="cell-8" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now if we consider the output as a long sequence of patches, we can compute the expected output shape</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original input shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>patchified <span class="op">=</span> x.contiguous().view(number_of_patches, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Transformed input shape: </span><span class="sc">{</span>patchified<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original input shape: torch.Size([3, 224, 224])
Transformed input shape: torch.Size([196, 768])</code></pre>
</div>
</div>
<section id="using-convolutions-to-generate-patch-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="using-convolutions-to-generate-patch-embeddings">Using Convolutions to Generate Patch Embeddings</h3>
<p>Recall that to convert our image into a sequence of patch embeddings, we need to:</p>
<ol type="1">
<li>Generate fixed-size patches from the image.</li>
<li>Embed these patches into a vector space.</li>
</ol>
<p>We can achieve both of these steps in one go using <code>nn.Conv2d</code>.</p>
<p>If the convolution operation consists of a kernel of size <span class="math inline">\((k, k)\)</span> and a stride of <span class="math inline">\(k\)</span>, it effectively breaks the input image into non-overlapping patches of size <span class="math inline">\(k \times k\)</span>. The kernel, also known as the filter, slides across the image, covering different sections, or patches, of the input. At each position, the kernel performs a dot product between the filter weights and the corresponding input pixels, followed by summing these products and applying an activation function. The output from each position becomes an element in the resulting feature map.</p>
<p>In this context, the filter’s role is twofold. First, it serves as a mechanism for slicing the image into smaller patches, with each patch being a subset of the image’s pixel data. Second, the convolution operation inherently compresses and transforms these patches into a set of feature maps, which can be interpreted as the patch embeddings. By adjusting the kernel size and stride, we control the size of the patches and the level of overlap between them.</p>
<p>For example, using a kernel size of <span class="math inline">\(k = 16\)</span> and stride <span class="math inline">\(s = 16\)</span> on an image of size <span class="math inline">\(224 \times 224\)</span> will produce a grid of <span class="math inline">\(14 \times 14\)</span> patches, each of size <span class="math inline">\(16 \times 16\)</span>. The convolutional layer outputs a feature map for each filter used, where each feature map represents a different aspect of the image’s data, such as edges, textures, or colors. The depth of the feature map, determined by the number of filters, defines the dimension of the patch embedding vectors.</p>
<p>Thus, using <code>nn.Conv2d</code>, we efficiently transform the input image into a sequence of patch embeddings, with each embedding vector encapsulating the salient features of its corresponding image patch.</p>
<div id="cell-10" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the patchifier</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>patchifier <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                       out_channels<span class="op">=</span><span class="dv">768</span>, <span class="co"># as we computed above</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                       kernel_size<span class="op">=</span>PATCH_SIZE,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>                       stride<span class="op">=</span>PATCH_SIZE,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>                       padding<span class="op">=</span><span class="dv">0</span>                       </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform a batch of inputs to see how it works</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>x, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dl))</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> patchifier(x)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patchified shape: </span><span class="sc">{</span>out<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([16, 3, 224, 224])
Patchified shape: torch.Size([16, 768, 14, 14])</code></pre>
</div>
</div>
<p><strong>Quick note about the shape:</strong></p>
<p><span class="math display">\[(16, 768, 14, 14)\]</span> <span class="math display">\[\text{(batch size, embedding dim, number of patches horizontally, number of patches vertically)}\]</span></p>
<p>Since we want to treat this as a sequence, i.e.&nbsp;losing the 2D structure, we can simply flatten this along the last two axes.</p>
<p>We will also transpose the tensor so that we have the number of channels/features at the end, this is just convention.</p>
<div id="cell-12" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>patch_emb <span class="op">=</span> out.flatten(start_dim<span class="op">=</span><span class="dv">2</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># NCHW -&gt; NLC</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final shape: </span><span class="sc">{</span>patch_emb<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Final shape: torch.Size([16, 196, 768])</code></pre>
</div>
</div>
<p>Before moving forward, another important point to note is how we will incorporate a “CLS token” or “classification token” later on for our task of image classification.</p>
<p>This is a technique borrowed from <a href="https://arxiv.org/abs/1810.04805">BERT</a>, where you have a learnable embedding meant to represent the entire input sequence. In the context of the Vision Transformer, the CLS token is a special token added to the sequence of patch embeddings. It serves as a summary or a representation of the entire image. The model learns this token’s embedding along with the patch embeddings during training.</p>
<p>At the end of the transformer layers, the CLS token’s final embedding is used as the input to a classification head, typically a fully connected layer, to predict the class label. This approach allows the model to consider the entire image’s context when making a classification decision, leveraging the self-attention mechanism to gather information from all patches into this single, informative vector.</p>
<div style="text-align: center;">
<p><img src="vit.png" alt="vit-layout" style="width:50%;"></p>
</div>
<div id="cell-14" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Quick demonstration of prepending a learnable embedding to this activation (along the "patch length" axis)</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>cls_token <span class="op">=</span> nn.Parameter(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">768</span>) <span class="co"># channels-last</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>toks <span class="op">=</span> torch.cat([</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    cls_token.expand(BATCH_SIZE, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), <span class="co"># have to expand out the batch axis</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    patch_emb,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final shape of embeddings with the CLS token: </span><span class="sc">{</span>toks<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Final shape of embeddings with the CLS token: torch.Size([16, 197, 768])</code></pre>
</div>
</div>
</section>
<section id="explicitly-adding-in-positional-information" class="level3">
<h3 class="anchored" data-anchor-id="explicitly-adding-in-positional-information">Explicitly adding in positional information</h3>
<p>Self-attention, while powerful, is inherently <strong>permutation invariant</strong>. This means that it does not take into account the order of the patches, treating them as a set rather than a sequence. However, in vision tasks, the spatial arrangement of patches is crucial for understanding the structure and relationships within an image.</p>
<p>To address this, we introduce positional encodings or embeddings. These are additional vectors added to the patch embeddings to inject information about the relative or absolute position of each patch in the image. In our implementation, we’ll use a set of learnable weights for these positional encodings, allowing the model to learn the most effective way to incorporate positional information during training.</p>
<p>By combining these positional encodings with the patch embeddings, we create a richer input representation that not only captures the visual content of the patches but also their spatial arrangement. This enriched input is then fed into the next main component of the transformer, enabling the model to leverage both the content and the position of patches for more accurate understanding and processing of the image.</p>
<p>In the cell below, note how we use <code>nn.Parameter</code> to intialize the tensor and the shape following the patch embeddings. We rely on broadcasting to create copies over the batch axis since we do not want there to be different positional encodings for elements in different batches (that would make no sense).</p>
<div id="cell-16" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a randomly initialized set of positional encodings</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>pos_embed <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, toks.shape[<span class="dv">1</span>], toks.shape[<span class="dv">2</span>]) <span class="op">*</span> <span class="fl">0.02</span>) <span class="co"># this factor is from the timm implementation</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> toks <span class="op">+</span> pos_embed</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final shape of input: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Final shape of input: torch.Size([16, 197, 768])</code></pre>
</div>
</div>
</section>
</section>
<section id="multi-head-self-attention" class="level2">
<h2 class="anchored" data-anchor-id="multi-head-self-attention">Multi-Head Self Attention</h2>
<p>Self-attention is a fundamental mechanism in the Vision Transformer that enables patches, or tokens, to communicate with one another across the entire image. This mechanism allows each patch to consider the information from all other patches, effectively sharing and enriching the context of each patch’s representation. In the scene of computer vision, this means that the model can capture relationships and dependencies between different parts of the image, such as identifying that certain shapes or colors in one part of the image may relate to features in another part. This global interaction helps the model build a more comprehensive understanding of the image, crucial for tasks like image classification.</p>
<p>The Self-Attention mechanism can be expressed with the following expression from <a href="https://arxiv.org/abs/1706.03762">the 2017 paper</a>: <span class="math display">\[\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)V\]</span></p>
<p>Here, the components are:</p>
<ul>
<li>Queries (<span class="math inline">\(Q\)</span>): These represent the specific aspects or “questions” that each patch wants to know about other patches.</li>
<li>Keys (<span class="math inline">\(K\)</span>): These act like tags or “keywords” that help identify relevant information in the patches.</li>
<li>Values (<span class="math inline">\(V\)</span>): These are the actual data or “answers” that the patches contain.</li>
</ul>
<p>To explain with an analogy, think of queries, keys, and values as parts of a search engine system:</p>
<ul>
<li><strong>Queries</strong> are like the search terms you enter. They specify what you’re looking for.</li>
<li><strong>Keys</strong> are the tags or metadata associated with each document or webpage, helping to determine if a document matches a search query.</li>
<li><strong>Values</strong> are the actual content of the documents that are retrieved and displayed.</li>
</ul>
<p>In the Vision Transformer, these components are extracted from the patch embeddings using learned linear transformations. The three matrices are computed as follows: <span class="math display">\[Q = W^Q X\]</span> <span class="math display">\[K = W^K X\]</span> <span class="math display">\[V = W^V X\]</span></p>
<p>where <span class="math inline">\(W^Q, W^K, W^V\)</span> are learnable weight matrices, and <span class="math inline">\(X\)</span> is the tensor corresponding to the input embeddings.</p>
<div id="cell-18" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define variables before moving further</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>B, P, C <span class="op">=</span> x.shape</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We can use nn.Linear layers (without the bias) to perform these computations</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> nn.Linear(embed_dim, embed_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> nn.Linear(embed_dim, embed_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> nn.Linear(embed_dim, embed_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the projections, these are the Q, K, V matrices in the equation above</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> query(x)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> key(x)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> value(x)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the shapes</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>q.shape, k.shape, v.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>(torch.Size([16, 197, 768]),
 torch.Size([16, 197, 768]),
 torch.Size([16, 197, 768]))</code></pre>
</div>
</div>
<section id="the-multiple-heads" class="level3">
<h3 class="anchored" data-anchor-id="the-multiple-heads">The Multiple “Head(s)”</h3>
<p>In the context of self-attention, a “head” refers to an individual attention mechanism within the multi-head attention framework. Each head operates independently, learning different aspects of the relationships between patches or tokens. By having multiple heads, the model can capture a diverse range of interactions and dependencies, enhancing its ability to understand complex patterns in the data.</p>
<p>Each head has its own set of learnable parameters for queries, keys, and values. The use of multiple heads allows the model to focus on different types of relationships or features in parallel. For instance, one head might learn to focus on spatial locality, while another might capture more global interactions.</p>
<p>Note how the <code>in_features</code> and <code>out_features</code> are the same for this setup. This actually makes it much easier for us to work with multiple heads in one go: we can partition the projections from these matrices by introducing a “head axis”, this would then let us perform computations with <code>H</code> of these vectors of size <code>C//H</code> in parallel.</p>
<div id="cell-20" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> <span class="dv">12</span> <span class="co"># hyperparam from timm's implementation</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>head_size <span class="op">=</span> C <span class="op">//</span> H</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># MHSA is usually implemented as such</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> query(x).view(B, P, H, head_size).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> key(x).view(B, P, H, head_size).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> value(x).view(B, P, H, head_size).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>q.shape, k.shape, v.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(torch.Size([16, 12, 197, 64]),
 torch.Size([16, 12, 197, 64]),
 torch.Size([16, 12, 197, 64]))</code></pre>
</div>
</div>
<p>With the projections ready, we can finally implement the meat of the component: the equation.</p>
<p><span class="math display">\[\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)V\]</span></p>
<p>Here’s a few things to note: 1. <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> are 4D tensors so the notion of a “tranpose” sounds rather strange. Note however, that the output we want from this operation is an <strong>Attention Map</strong> - a map that lays out the affinities between each and every pair of patches in the input tensor. This means what we really want is a <code>(B, H, P, P)</code> tensor - this means all we have to do is swap the last two axes of <span class="math inline">\(K\)</span> and follow the rules of Batched Matrix Multiplication.</p>
<ol start="2" type="1">
<li><p>The scale factor <span class="math inline">\(\sqrt{d_k}\)</span> is helpful for stable training. Without this, the activations would blow up exactly on the order of <span class="math inline">\(d_k\)</span>, and this can lead to unstable gradients - so we scale everything down by this amount to end up with activations of unit-variance.</p></li>
<li><p>The Softmax here is applied on a row axis - i.e.&nbsp;the rows of the 2D slice must contain values that sum up to 1. This is important to note since we consider the dot product of the row of the query matrix with the columns of the transpose of the key (so if you ask a specific question following the analogy above, you’d want the answers to be weighed according to the question, not all questions weighed according to a single answer).</p></li>
</ol>
<div id="cell-22" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's implement this all in one go</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>head_size <span class="op">=</span> C <span class="op">//</span> H</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># MHSA is usually implemented as such</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> query(x).view(B, P, H, head_size).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> key(x).view(B, P, H, head_size).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> value(x).view(B, P, H, head_size).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> q.size(<span class="op">-</span><span class="dv">1</span>) <span class="co"># the head size</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> q<span class="op">*</span>scale</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Take the dot product</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>attn <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (B, H, P, head_size) @ (B, H, head_size, P) --&gt; (B, H, P, P)</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>attn<span class="sc">.</span>shape<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Softmax along the final axis, i.e. rows</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> attn <span class="op">@</span> v <span class="co"># (B, H, P, P) @ (B, H, P, head_size) --&gt; (B, H, P, head_size)</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>out<span class="sc">.</span>shape<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Collapse the head dimension to get back </span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>sa_out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, P, C)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>sa_out<span class="sc">.</span>shape<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.shape=torch.Size([16, 197, 768])
attn.shape=torch.Size([16, 12, 197, 197])
out.shape=torch.Size([16, 12, 197, 64])
sa_out.shape=torch.Size([16, 197, 768])</code></pre>
</div>
</div>
<p>The shapes can help give us a very interesting interpretation of what Self-Attention really does: if the input and output shapes are exactly the same (i.e.&nbsp;<code>[16, 197, 768]</code>) but with so much of processing with learnable parameters in the middle, then we can think of this module as simply <strong>refining the representations</strong>.</p>
<p>To expand on this, your input is provided as a sequence of embeddings (order matters here) - the Self-Attention operation allows the elements to communicate and share information with one another, enriching each other with context, so that the final consolidated output is simply an enriched version of the input. It also helps that having the same shape allows you to stack these components on top of one another very easily, as we will see later.</p>
</section>
<section id="building-mhsa" class="level3">
<h3 class="anchored" data-anchor-id="building-mhsa">Building <code>MHSA</code></h3>
<p>Let’s put all of these operations, with a few other bells and whistles into a single <code>nn.Module</code> class.</p>
<p>Note how we add two things here: 1. The <code>proj</code> component: this is a projection back into the same vector space, so it again acts like simply <em>refining</em> what you already have. Andrej Karpathy calls this as <em>thinking</em> on what you have just computed from the Self-Attention operation.</p>
<ol start="2" type="1">
<li>The <code>proj_drop</code> component: This is a form of Dropout which acts as a regularizing mechanism for the Vision Transformer. This becomes very important since they are prone to overfit on simplistic datasets, so this and <code>attn_drop</code> (functionally the same thing) can help mitigate this heinous activity.</li>
</ol>
<div id="cell-24" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MHSA(nn.Module):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>            dim: <span class="bu">int</span>,</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>            num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>            qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>            attn_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>            proj_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> dim <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">'dim should be divisible by num_heads'</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dim <span class="op">=</span> dim</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_drop <span class="op">=</span> nn.Dropout(attn_drop)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj_drop <span class="op">=</span> nn.Dropout(proj_drop)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>        B, P, C <span class="op">=</span> x.shape</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.q(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.k(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.v(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> <span class="va">self</span>.attn_drop(attn)</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> attn <span class="op">@</span> v</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, P, C)</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.proj(x)</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.proj_drop(x)</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-25" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward pass on a dummy input</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>mhsa <span class="op">=</span> MHSA(dim<span class="op">=</span><span class="dv">768</span>,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>            num_heads<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>            qkv_bias<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample input tensor</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>B, P, C <span class="op">=</span> <span class="dv">16</span>, <span class="dv">196</span>, <span class="dv">768</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B, P, C)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass the input through the MHSA layer to trigger the hook</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> mhsa(x)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="op">=</span><span class="sc">}</span><span class="ss"> --&gt; </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> mhsa</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.shape=torch.Size([16, 196, 768]) --&gt; output.shape=torch.Size([16, 196, 768])</code></pre>
</div>
</div>
</section>
</section>
<section id="the-encoder-block" class="level2">
<h2 class="anchored" data-anchor-id="the-encoder-block">The “Encoder Block”</h2>
<p>Let’s go back to the main illustration in the original paper:</p>
<div style="text-align: center;">
<p><img src="vit.png" alt="vit-layout" style="width:50%;"></p>
</div>
<p>The grayed out block on the right side represents a single “Encoder Block”.</p>
<p>The Encoder Block is a fundamental component of the Vision Transformer, inheriting its architecture from the original Transformer model used in natural language processing. Each Encoder Block is designed to process and refine the input sequence—here, the sequence of patch embeddings enriched with positional encodings.</p>
<p>The Encoder Block consists of two main sub-layers:</p>
<p>Multi-Head Self-Attention Mechanism: This layer allows the model to focus on different parts of the input sequence simultaneously, capturing various relationships and dependencies among the patches. As discussed earlier, multiple heads enable the model to learn different aspects of the data, providing a comprehensive representation.</p>
<p><strong>Feed-Forward Neural Network (FFN)</strong>: Following the self-attention layer, a position-wise feed-forward neural network processes the output. This network typically consists of two linear transformations separated by a non-linear activation function, such as GeLU. It acts on each position independently and helps in further transforming and refining the representation learned from the self-attention layer.</p>
<p>To ensure the effective flow of gradients during training and to preserve the original input information, each sub-layer is equipped with <strong>Skip Connections</strong> (also known as Residual Connections). These connections add the input of each sub-layer to its output, forming a residual path that facilitates better gradient propagation and helps prevent the vanishing gradient problem. Mathematically, this can be expressed as:</p>
<p><span class="math display">\[\text{Output} = \text{Layernorm}(x + \text{SubLayer}(x))\]</span></p>
<p>In the equation above, <span class="math inline">\(x\)</span> represents the input to the sub-layer, and the sum <span class="math inline">\(x + \text{SubLayer}(x)\)</span> forms the residual connection. This is a very old idea in Deep Learning, going <a href="https://arxiv.org/abs/1512.03385">back to 2015 with ResNets</a>, and can perhaps be better understood with the following illustration:</p>
<div style="text-align: center;">
<p><img src="skip-connection.png" alt="skip-connection" style="width:30%;"></p>
</div>
<p>The output is then normalized using Layer Normalization (<code>LayerNorm</code>), which stabilizes the training by normalizing the summed outputs across each patch, ensuring that the model’s activations are within a stable range. LayerNorm adjusts the output by scaling and shifting, allowing the model to maintain useful information while preventing excessive internal covariate shifts.</p>
<p>The Encoder Block’s design, with its combination of self-attention, feed-forward neural networks, skip connections, and layer normalization, enables the Vision Transformer to learn rich, hierarchical representations of the input data. This structure is repeated multiple times in the model, each block building upon the representations learned by the previous one, gradually refining the understanding of the input image.</p>
<p>Let’s go ahead and implement this directly.</p>
<div id="cell-27" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Mlp(nn.Module):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>                 in_features: <span class="bu">int</span>,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>                 hidden_features: <span class="bu">int</span>,</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>                 drop: <span class="bu">float</span>,</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>                 norm_layer: nn.Module <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># There are two Linear layers in the conventional implementation</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(in_features, hidden_features)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_features, in_features)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout is used twice, once after each linear layer</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop1 <span class="op">=</span> nn.Dropout(drop)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop2 <span class="op">=</span> nn.Dropout(drop)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The paper uses the GeLU activation function</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act <span class="op">=</span> nn.GELU()</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optional normalization layer (following timm's convention)</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> norm_layer(hidden_features) <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> nn.Identity()</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.act(x)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.drop1(x)</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.drop2(x)</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>            dim: <span class="bu">int</span>,</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>            num_heads: <span class="bu">int</span>,</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>            mlp_ratio: <span class="bu">float</span> <span class="op">=</span> <span class="fl">4.</span>,</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>            qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>            proj_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>            attn_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> Mlp(</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>            in_features<span class="op">=</span>dim,</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>            hidden_features<span class="op">=</span><span class="bu">int</span>(dim <span class="op">*</span> mlp_ratio),</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>            drop<span class="op">=</span>proj_drop,</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> MHSA(</span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>            dim<span class="op">=</span>dim,</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>            qkv_bias<span class="op">=</span>qkv_bias,</span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>            attn_drop<span class="op">=</span>attn_drop,</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>            proj_drop<span class="op">=</span>proj_drop</span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.norm1(x))</span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.norm2(x))</span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>block <span class="op">=</span> Block(dim<span class="op">=</span><span class="dv">768</span>,</span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>              num_heads<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a>block_out <span class="op">=</span> block(x)</span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="op">=</span><span class="sc">}</span><span class="ss"> --&gt; </span><span class="sc">{</span>block_out<span class="sc">.</span>shape<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> block</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.shape=torch.Size([16, 196, 768]) --&gt; block_out.shape=torch.Size([16, 196, 768])</code></pre>
</div>
</div>
<p>A wonderful observation here is again that the <strong>input and output shapes are exactly the same!</strong></p>
<p>This means we can again fall back on the interpretation that each Block (on top of each MHSA module) is simply <em>refining</em> and embedding rich context into whatever input is fed into it.</p>
<p>Plus, having the same shape as the input allows us to stack these encoders nicely on top of each other without much thought or care.</p>
</section>
<section id="putting-it-all-together" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together">Putting it all together</h2>
<p>Now with everything we’ve learned so far, let’s make one final class that aggregates everything.</p>
<p>Recall what we had to do with</p>
<ol type="1">
<li><p>The Patch Embeddings to represent our image as a sequence and let Self-Attention link parts of it with one another,</p></li>
<li><p>The Positional Encodings/Embeddings to move past the permutation invariant nature of Self-Attention and embed information regarding the position of each patch into the mix,</p></li>
<li><p>The CLS Token to let the model have an overall representation of the entire image which would provide a means of performing classification,</p></li>
<li><p>The Multi-Head Self-Attention class (<code>MHSA</code>) to let the patches communicate and share information with one another in the hopes of enriching the representations,</p></li>
<li><p>The Block class (<code>Block</code>) to be able to string together the computations performed by the Self-Attention, Feedforward, and Layer Normalization modules.</p></li>
</ol>
<p>Now we put it all together.</p>
<div id="cell-29" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PatchEmbed(nn.Module):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>                 img_size: <span class="bu">int</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>                 patch_size: <span class="bu">int</span>,</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>                 in_chans: <span class="bu">int</span>,</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>                 embed_dim: <span class="bu">int</span>,</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>                 bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>                 norm_layer: Optional[Callable] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid_size <span class="op">=</span> (<span class="va">self</span>.img_size <span class="op">//</span> <span class="va">self</span>.patch_size, ) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> <span class="va">self</span>.grid_size[<span class="dv">0</span>] <span class="op">*</span> <span class="va">self</span>.grid_size[<span class="dv">1</span>]</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Conv2d(in_chans, </span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>                              embed_dim, </span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>                              kernel_size<span class="op">=</span>patch_size, </span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>                              stride<span class="op">=</span>patch_size, </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>                              bias<span class="op">=</span>bias, </span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> norm_layer(embed_dim) <span class="cf">if</span> norm_layer <span class="cf">else</span> nn.Identity()</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.proj(x)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>).transpose(<span class="dv">1</span>,<span class="dv">2</span>) <span class="co"># flatten and transpose: BCHW -&gt; BLC</span></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MHSA(nn.Module):</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>            dim: <span class="bu">int</span>,</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>            num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>,</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>            qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>            attn_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>            proj_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> dim <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">'dim should be divisible by num_heads'</span></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dim <span class="op">=</span> dim</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_drop <span class="op">=</span> nn.Dropout(attn_drop)</span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj_drop <span class="op">=</span> nn.Dropout(proj_drop)</span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb29-59"><a href="#cb29-59" aria-hidden="true" tabindex="-1"></a>        B, P, C <span class="op">=</span> x.shape</span>
<span id="cb29-60"><a href="#cb29-60" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb29-61"><a href="#cb29-61" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.q(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb29-62"><a href="#cb29-62" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.k(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb29-63"><a href="#cb29-63" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.v(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb29-64"><a href="#cb29-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-65"><a href="#cb29-65" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb29-66"><a href="#cb29-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-67"><a href="#cb29-67" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb29-68"><a href="#cb29-68" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb29-69"><a href="#cb29-69" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> <span class="va">self</span>.attn_drop(attn)</span>
<span id="cb29-70"><a href="#cb29-70" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> attn <span class="op">@</span> v</span>
<span id="cb29-71"><a href="#cb29-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-72"><a href="#cb29-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, P, C)</span>
<span id="cb29-73"><a href="#cb29-73" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.proj(x)</span>
<span id="cb29-74"><a href="#cb29-74" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.proj_drop(x)</span>
<span id="cb29-75"><a href="#cb29-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb29-76"><a href="#cb29-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-77"><a href="#cb29-77" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Mlp(nn.Module):</span>
<span id="cb29-78"><a href="#cb29-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb29-79"><a href="#cb29-79" aria-hidden="true" tabindex="-1"></a>                 in_features: <span class="bu">int</span>,</span>
<span id="cb29-80"><a href="#cb29-80" aria-hidden="true" tabindex="-1"></a>                 hidden_features: <span class="bu">int</span>,</span>
<span id="cb29-81"><a href="#cb29-81" aria-hidden="true" tabindex="-1"></a>                 drop: <span class="bu">float</span>,</span>
<span id="cb29-82"><a href="#cb29-82" aria-hidden="true" tabindex="-1"></a>                 norm_layer: nn.Module <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb29-83"><a href="#cb29-83" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-84"><a href="#cb29-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-85"><a href="#cb29-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># There are two Linear layers in the conventional implementation</span></span>
<span id="cb29-86"><a href="#cb29-86" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(in_features, hidden_features)</span>
<span id="cb29-87"><a href="#cb29-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_features, in_features)</span>
<span id="cb29-88"><a href="#cb29-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-89"><a href="#cb29-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout is used twice, once after each linear layer</span></span>
<span id="cb29-90"><a href="#cb29-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop1 <span class="op">=</span> nn.Dropout(drop)</span>
<span id="cb29-91"><a href="#cb29-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop2 <span class="op">=</span> nn.Dropout(drop)</span>
<span id="cb29-92"><a href="#cb29-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-93"><a href="#cb29-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The paper uses the GeLU activation function</span></span>
<span id="cb29-94"><a href="#cb29-94" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act <span class="op">=</span> nn.GELU()</span>
<span id="cb29-95"><a href="#cb29-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-96"><a href="#cb29-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optional normalization layer (following timm's convention)</span></span>
<span id="cb29-97"><a href="#cb29-97" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> norm_layer(hidden_features) <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> nn.Identity()</span>
<span id="cb29-98"><a href="#cb29-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-99"><a href="#cb29-99" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb29-100"><a href="#cb29-100" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb29-101"><a href="#cb29-101" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.act(x)</span>
<span id="cb29-102"><a href="#cb29-102" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.drop1(x)</span>
<span id="cb29-103"><a href="#cb29-103" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb29-104"><a href="#cb29-104" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb29-105"><a href="#cb29-105" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.drop2(x)</span>
<span id="cb29-106"><a href="#cb29-106" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb29-107"><a href="#cb29-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-108"><a href="#cb29-108" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb29-109"><a href="#cb29-109" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb29-110"><a href="#cb29-110" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb29-111"><a href="#cb29-111" aria-hidden="true" tabindex="-1"></a>            dim: <span class="bu">int</span>,</span>
<span id="cb29-112"><a href="#cb29-112" aria-hidden="true" tabindex="-1"></a>            num_heads: <span class="bu">int</span>,</span>
<span id="cb29-113"><a href="#cb29-113" aria-hidden="true" tabindex="-1"></a>            mlp_ratio: <span class="bu">float</span> <span class="op">=</span> <span class="fl">4.</span>,</span>
<span id="cb29-114"><a href="#cb29-114" aria-hidden="true" tabindex="-1"></a>            qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb29-115"><a href="#cb29-115" aria-hidden="true" tabindex="-1"></a>            proj_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb29-116"><a href="#cb29-116" aria-hidden="true" tabindex="-1"></a>            attn_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb29-117"><a href="#cb29-117" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb29-118"><a href="#cb29-118" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-119"><a href="#cb29-119" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-120"><a href="#cb29-120" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb29-121"><a href="#cb29-121" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb29-122"><a href="#cb29-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-123"><a href="#cb29-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> Mlp(</span>
<span id="cb29-124"><a href="#cb29-124" aria-hidden="true" tabindex="-1"></a>            in_features<span class="op">=</span>dim,</span>
<span id="cb29-125"><a href="#cb29-125" aria-hidden="true" tabindex="-1"></a>            hidden_features<span class="op">=</span><span class="bu">int</span>(dim <span class="op">*</span> mlp_ratio),</span>
<span id="cb29-126"><a href="#cb29-126" aria-hidden="true" tabindex="-1"></a>            drop<span class="op">=</span>proj_drop,</span>
<span id="cb29-127"><a href="#cb29-127" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-128"><a href="#cb29-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-129"><a href="#cb29-129" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> MHSA(</span>
<span id="cb29-130"><a href="#cb29-130" aria-hidden="true" tabindex="-1"></a>            dim<span class="op">=</span>dim,</span>
<span id="cb29-131"><a href="#cb29-131" aria-hidden="true" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb29-132"><a href="#cb29-132" aria-hidden="true" tabindex="-1"></a>            qkv_bias<span class="op">=</span>qkv_bias,</span>
<span id="cb29-133"><a href="#cb29-133" aria-hidden="true" tabindex="-1"></a>            attn_drop<span class="op">=</span>attn_drop,</span>
<span id="cb29-134"><a href="#cb29-134" aria-hidden="true" tabindex="-1"></a>            proj_drop<span class="op">=</span>proj_drop</span>
<span id="cb29-135"><a href="#cb29-135" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-136"><a href="#cb29-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-137"><a href="#cb29-137" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb29-138"><a href="#cb29-138" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.norm1(x))</span>
<span id="cb29-139"><a href="#cb29-139" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.norm2(x))</span>
<span id="cb29-140"><a href="#cb29-140" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-30" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>            img_size: Union[<span class="bu">int</span>, Tuple[<span class="bu">int</span>, <span class="bu">int</span>]] <span class="op">=</span> <span class="dv">224</span>,</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>            patch_size: Union[<span class="bu">int</span>, Tuple[<span class="bu">int</span>, <span class="bu">int</span>]] <span class="op">=</span> <span class="dv">16</span>,</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>            in_chans: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>            num_classes: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span>,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>            embed_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span>,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>            depth: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span>,</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>            num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span>,</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>            mlp_ratio: <span class="bu">float</span> <span class="op">=</span> <span class="fl">4.</span>,</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>            drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>            pos_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            proj_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>            attn_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="co">            img_size: Input image size.</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a><span class="co">            patch_size: Patch size.</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a><span class="co">            in_chans: Number of image input channels.</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="co">            num_classes: Number of classes for classification heads</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="co">            embed_dim: Transformer embedding dimension.</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a><span class="co">            depth: Depth of transformer.</span></span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a><span class="co">            num_heads: Number of attention heads.</span></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a><span class="co">            mlp_ratio: Ratio of mlp hidden dim to embedding dim.</span></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a><span class="co">            qkv_bias: Enable bias for qkv projections if True.</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a><span class="co">            drop_rate: Head dropout rate.</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a><span class="co">            pos_drop_rate: Position embedding dropout rate.</span></span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a><span class="co">            attn_drop_rate: Attention dropout rate.</span></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_classes <span class="op">=</span> num_classes</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_features <span class="op">=</span> <span class="va">self</span>.head_hidden_size <span class="op">=</span> <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim  <span class="co"># for consistency with other models</span></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the Patch Embedding module - note this does not include the CLS token yet</span></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbed(</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>            img_size<span class="op">=</span>img_size,</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>            patch_size<span class="op">=</span>patch_size,</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>            in_chans<span class="op">=</span>in_chans,</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>            embed_dim<span class="op">=</span>embed_dim,</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>            bias<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>        num_patches <span class="op">=</span> <span class="va">self</span>.patch_embed.num_patches</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a>        embed_len <span class="op">=</span> num_patches <span class="op">+</span> <span class="dv">1</span> <span class="co"># don't forget we need to incorporate the CLS token</span></span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the CLS token, the Positional Encodings/Embeddings, and a Dropout for the Positional information</span></span>
<span id="cb30-51"><a href="#cb30-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))</span>
<span id="cb30-52"><a href="#cb30-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, embed_len, embed_dim) <span class="op">*</span> <span class="fl">.02</span>)</span>
<span id="cb30-53"><a href="#cb30-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span>pos_drop_rate)</span>
<span id="cb30-54"><a href="#cb30-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-55"><a href="#cb30-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define LayerNorms for before and after the Encoder block processing</span></span>
<span id="cb30-56"><a href="#cb30-56" aria-hidden="true" tabindex="-1"></a>        norm_layer <span class="op">=</span> partial(nn.LayerNorm, eps<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb30-57"><a href="#cb30-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm_pre <span class="op">=</span> norm_layer(embed_dim)</span>
<span id="cb30-58"><a href="#cb30-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> norm_layer(embed_dim)</span>
<span id="cb30-59"><a href="#cb30-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-60"><a href="#cb30-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the blocks</span></span>
<span id="cb30-61"><a href="#cb30-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[</span>
<span id="cb30-62"><a href="#cb30-62" aria-hidden="true" tabindex="-1"></a>            Block(</span>
<span id="cb30-63"><a href="#cb30-63" aria-hidden="true" tabindex="-1"></a>                dim<span class="op">=</span>embed_dim,</span>
<span id="cb30-64"><a href="#cb30-64" aria-hidden="true" tabindex="-1"></a>                num_heads<span class="op">=</span>num_heads,</span>
<span id="cb30-65"><a href="#cb30-65" aria-hidden="true" tabindex="-1"></a>                mlp_ratio<span class="op">=</span>mlp_ratio,</span>
<span id="cb30-66"><a href="#cb30-66" aria-hidden="true" tabindex="-1"></a>                qkv_bias<span class="op">=</span>qkv_bias,</span>
<span id="cb30-67"><a href="#cb30-67" aria-hidden="true" tabindex="-1"></a>                proj_drop<span class="op">=</span>proj_drop_rate,</span>
<span id="cb30-68"><a href="#cb30-68" aria-hidden="true" tabindex="-1"></a>                attn_drop<span class="op">=</span>attn_drop_rate</span>
<span id="cb30-69"><a href="#cb30-69" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb30-70"><a href="#cb30-70" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(depth)])</span>
<span id="cb30-71"><a href="#cb30-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-72"><a href="#cb30-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_info <span class="op">=</span> [</span>
<span id="cb30-73"><a href="#cb30-73" aria-hidden="true" tabindex="-1"></a>            <span class="bu">dict</span>(module<span class="op">=</span><span class="ss">f'blocks.</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>, num_chs<span class="op">=</span>embed_dim) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb30-74"><a href="#cb30-74" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb30-75"><a href="#cb30-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-76"><a href="#cb30-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classifier Head</span></span>
<span id="cb30-77"><a href="#cb30-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_drop <span class="op">=</span> nn.Dropout(drop_rate)</span>
<span id="cb30-78"><a href="#cb30-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(<span class="va">self</span>.embed_dim, num_classes)</span>
<span id="cb30-79"><a href="#cb30-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-80"><a href="#cb30-80" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_features(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb30-81"><a href="#cb30-81" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-82"><a href="#cb30-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the Patch Embeddings</span></span>
<span id="cb30-83"><a href="#cb30-83" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)</span>
<span id="cb30-84"><a href="#cb30-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-85"><a href="#cb30-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prepend the CLS token, THEN add in the positional information</span></span>
<span id="cb30-86"><a href="#cb30-86" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat([</span>
<span id="cb30-87"><a href="#cb30-87" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), <span class="co"># have to expand out the batch axis</span></span>
<span id="cb30-88"><a href="#cb30-88" aria-hidden="true" tabindex="-1"></a>            x,</span>
<span id="cb30-89"><a href="#cb30-89" aria-hidden="true" tabindex="-1"></a>        ], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb30-90"><a href="#cb30-90" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed</span>
<span id="cb30-91"><a href="#cb30-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-92"><a href="#cb30-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass through the LayerNorms, the Encoder Blocks, then the final Norm</span></span>
<span id="cb30-93"><a href="#cb30-93" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm_pre(x)</span>
<span id="cb30-94"><a href="#cb30-94" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)</span>
<span id="cb30-95"><a href="#cb30-95" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb30-96"><a href="#cb30-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-97"><a href="#cb30-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb30-98"><a href="#cb30-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-99"><a href="#cb30-99" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_head(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb30-100"><a href="#cb30-100" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-101"><a href="#cb30-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the CLS token (also called "pooling" our logits)</span></span>
<span id="cb30-102"><a href="#cb30-102" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x[:, <span class="dv">0</span>]</span>
<span id="cb30-103"><a href="#cb30-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-104"><a href="#cb30-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass through the Dropout then the classification head</span></span>
<span id="cb30-105"><a href="#cb30-105" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head_drop(x)</span>
<span id="cb30-106"><a href="#cb30-106" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb30-107"><a href="#cb30-107" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-108"><a href="#cb30-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb30-109"><a href="#cb30-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-110"><a href="#cb30-110" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb30-111"><a href="#cb30-111" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.forward_features(x)</span>
<span id="cb30-112"><a href="#cb30-112" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.forward_head(x)</span>
<span id="cb30-113"><a href="#cb30-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb30-114"><a href="#cb30-114" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-115"><a href="#cb30-115" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VisionTransformer()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-31" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dl))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model(x)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(out.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([16, 1000])</code></pre>
</div>
</div>
</section>
<section id="loading-in-pretrained-weights" class="level2">
<h2 class="anchored" data-anchor-id="loading-in-pretrained-weights">Loading in Pretrained Weights</h2>
<p>Now that we’ve built our simplistic version of a Vision Transformer, let’s load in the parameters from a pretrained checkpoint, specifically from the Base variant of the <code>timm</code> library.</p>
<p>The procedure is simple: load in the <code>state_dict</code> from the pretrained model, and match with the corresponding parameters in your implementation. Since we were careful to stick with a naming convention similar to the implementation already in <code>timm</code>, we’ve made our job simpler already.</p>
<p>We will do this in a slightly unconventional way - we will define methods for loading in the parameters in <strong>each module</strong>. This makes our code arguably more readable, but requires us to rewrite the module again for the third time in this article (last time, promise!).</p>
<p>This form of matching and copying has been inspired from <a href="https://github.com/rasbt/LLMs-from-scratch">Sebastian Raschka’s book on “LLMs from Scratch”</a>, specifically <a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb">this notebook</a>.</p>
<div id="cell-33" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> assign_check(left: torch.Tensor, right: torch.Tensor):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Utility for checking and creating parameters to be used in loading a pretrained checkpoint'''</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> left.shape <span class="op">!=</span> right.shape:</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Shape mismatch: Left: </span><span class="sc">{</span>left<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, Right: </span><span class="sc">{</span>right<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.nn.Parameter(right.clone().detach())</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PatchEmbed(nn.Module):</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>                 img_size: <span class="bu">int</span>,</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>                 patch_size: <span class="bu">int</span>,</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>                 in_chans: <span class="bu">int</span>,</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>                 embed_dim: <span class="bu">int</span>,</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>                 bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>                 norm_layer: Optional[Callable] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grid_size <span class="op">=</span> (<span class="va">self</span>.img_size <span class="op">//</span> <span class="va">self</span>.patch_size, ) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> <span class="va">self</span>.grid_size[<span class="dv">0</span>] <span class="op">*</span> <span class="va">self</span>.grid_size[<span class="dv">1</span>]</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Conv2d(in_chans, </span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>                              embed_dim, </span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>                              kernel_size<span class="op">=</span>patch_size, </span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>                              stride<span class="op">=</span>patch_size, </span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>                              bias<span class="op">=</span>bias, </span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> norm_layer(embed_dim) <span class="cf">if</span> norm_layer <span class="cf">else</span> nn.Identity()</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.proj(x)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>).transpose(<span class="dv">1</span>,<span class="dv">2</span>) <span class="co"># flatten and transpose: BCHW -&gt; BLC</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MHSA(nn.Module):</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>            dim: <span class="bu">int</span>,</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>            num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>,</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>            qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>            attn_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>            proj_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> dim <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">'dim should be divisible by num_heads'</span></span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dim <span class="op">=</span> dim</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_drop <span class="op">=</span> nn.Dropout(attn_drop)</span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj_drop <span class="op">=</span> nn.Dropout(proj_drop)</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>        B, P, C <span class="op">=</span> x.shape</span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.q(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.k(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.v(x).view(B, P, H, <span class="op">-</span><span class="dv">1</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, H, P, head_size)</span></span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb33-72"><a href="#cb33-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-73"><a href="#cb33-73" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb33-74"><a href="#cb33-74" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb33-75"><a href="#cb33-75" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> <span class="va">self</span>.attn_drop(attn)</span>
<span id="cb33-76"><a href="#cb33-76" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> attn <span class="op">@</span> v</span>
<span id="cb33-77"><a href="#cb33-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-78"><a href="#cb33-78" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, P, C)</span>
<span id="cb33-79"><a href="#cb33-79" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.proj(x)</span>
<span id="cb33-80"><a href="#cb33-80" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.proj_drop(x)</span>
<span id="cb33-81"><a href="#cb33-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-82"><a href="#cb33-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-83"><a href="#cb33-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> att_weight_conversion(<span class="va">self</span>, qkv_params):</span>
<span id="cb33-84"><a href="#cb33-84" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb33-85"><a href="#cb33-85" aria-hidden="true" tabindex="-1"></a><span class="co">        Split and convert the QKV parameters from ViT checkpoints for the GQA implementation</span></span>
<span id="cb33-86"><a href="#cb33-86" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb33-87"><a href="#cb33-87" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> torch.split(qkv_params, qkv_params.shape[<span class="dv">0</span>] <span class="op">//</span> <span class="dv">3</span>, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb33-88"><a href="#cb33-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-89"><a href="#cb33-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb33-90"><a href="#cb33-90" aria-hidden="true" tabindex="-1"></a>            <span class="st">"q"</span>: q,</span>
<span id="cb33-91"><a href="#cb33-91" aria-hidden="true" tabindex="-1"></a>            <span class="st">"k"</span>: k,</span>
<span id="cb33-92"><a href="#cb33-92" aria-hidden="true" tabindex="-1"></a>            <span class="st">"v"</span>: v</span>
<span id="cb33-93"><a href="#cb33-93" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb33-94"><a href="#cb33-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-95"><a href="#cb33-95" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> load_pretrained_weights(<span class="va">self</span>, state_dict, block_idx):</span>
<span id="cb33-96"><a href="#cb33-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-97"><a href="#cb33-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load in parameters for the Query Key Value layers</span></span>
<span id="cb33-98"><a href="#cb33-98" aria-hidden="true" tabindex="-1"></a>        qkv_weight <span class="op">=</span> state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.attn.qkv.weight'</span>]</span>
<span id="cb33-99"><a href="#cb33-99" aria-hidden="true" tabindex="-1"></a>        qkv_bias <span class="op">=</span> state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.attn.qkv.bias'</span>]</span>
<span id="cb33-100"><a href="#cb33-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-101"><a href="#cb33-101" aria-hidden="true" tabindex="-1"></a>        wdict <span class="op">=</span> <span class="va">self</span>.att_weight_conversion(qkv_weight)</span>
<span id="cb33-102"><a href="#cb33-102" aria-hidden="true" tabindex="-1"></a>        bdict <span class="op">=</span> <span class="va">self</span>.att_weight_conversion(qkv_bias)</span>
<span id="cb33-103"><a href="#cb33-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-104"><a href="#cb33-104" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q.weight <span class="op">=</span> assign_check(<span class="va">self</span>.q.weight, wdict[<span class="st">'q'</span>])</span>
<span id="cb33-105"><a href="#cb33-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q.bias <span class="op">=</span> assign_check(<span class="va">self</span>.q.bias, bdict[<span class="st">'q'</span>])</span>
<span id="cb33-106"><a href="#cb33-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-107"><a href="#cb33-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k.weight <span class="op">=</span> assign_check(<span class="va">self</span>.k.weight, wdict[<span class="st">'k'</span>])</span>
<span id="cb33-108"><a href="#cb33-108" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k.bias <span class="op">=</span> assign_check(<span class="va">self</span>.k.bias, bdict[<span class="st">'k'</span>])</span>
<span id="cb33-109"><a href="#cb33-109" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-110"><a href="#cb33-110" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v.weight <span class="op">=</span> assign_check(<span class="va">self</span>.v.weight, wdict[<span class="st">'v'</span>])</span>
<span id="cb33-111"><a href="#cb33-111" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v.bias <span class="op">=</span> assign_check(<span class="va">self</span>.v.bias, bdict[<span class="st">'v'</span>])</span>
<span id="cb33-112"><a href="#cb33-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-113"><a href="#cb33-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load in parameters for the output projection</span></span>
<span id="cb33-114"><a href="#cb33-114" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj.weight <span class="op">=</span> assign_check(<span class="va">self</span>.proj.weight, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.attn.proj.weight'</span>])</span>
<span id="cb33-115"><a href="#cb33-115" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proj.bias <span class="op">=</span> assign_check(<span class="va">self</span>.proj.bias, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.attn.proj.bias'</span>])</span>
<span id="cb33-116"><a href="#cb33-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-117"><a href="#cb33-117" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Mlp(nn.Module):</span>
<span id="cb33-118"><a href="#cb33-118" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb33-119"><a href="#cb33-119" aria-hidden="true" tabindex="-1"></a>                 in_features: <span class="bu">int</span>,</span>
<span id="cb33-120"><a href="#cb33-120" aria-hidden="true" tabindex="-1"></a>                 hidden_features: <span class="bu">int</span>,</span>
<span id="cb33-121"><a href="#cb33-121" aria-hidden="true" tabindex="-1"></a>                 drop: <span class="bu">float</span>,</span>
<span id="cb33-122"><a href="#cb33-122" aria-hidden="true" tabindex="-1"></a>                 norm_layer: nn.Module <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb33-123"><a href="#cb33-123" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-124"><a href="#cb33-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-125"><a href="#cb33-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># There are two Linear layers in the conventional implementation</span></span>
<span id="cb33-126"><a href="#cb33-126" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(in_features, hidden_features)</span>
<span id="cb33-127"><a href="#cb33-127" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_features, in_features)</span>
<span id="cb33-128"><a href="#cb33-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-129"><a href="#cb33-129" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout is used twice, once after each linear layer</span></span>
<span id="cb33-130"><a href="#cb33-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop1 <span class="op">=</span> nn.Dropout(drop)</span>
<span id="cb33-131"><a href="#cb33-131" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop2 <span class="op">=</span> nn.Dropout(drop)</span>
<span id="cb33-132"><a href="#cb33-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-133"><a href="#cb33-133" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The paper uses the GeLU activation function</span></span>
<span id="cb33-134"><a href="#cb33-134" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act <span class="op">=</span> nn.GELU()</span>
<span id="cb33-135"><a href="#cb33-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-136"><a href="#cb33-136" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optional normalization layer (following timm's convention)</span></span>
<span id="cb33-137"><a href="#cb33-137" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> norm_layer(hidden_features) <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> nn.Identity()</span>
<span id="cb33-138"><a href="#cb33-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-139"><a href="#cb33-139" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb33-140"><a href="#cb33-140" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb33-141"><a href="#cb33-141" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.act(x)</span>
<span id="cb33-142"><a href="#cb33-142" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.drop1(x)</span>
<span id="cb33-143"><a href="#cb33-143" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb33-144"><a href="#cb33-144" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb33-145"><a href="#cb33-145" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.drop2(x)</span>
<span id="cb33-146"><a href="#cb33-146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-147"><a href="#cb33-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-148"><a href="#cb33-148" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb33-149"><a href="#cb33-149" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb33-150"><a href="#cb33-150" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb33-151"><a href="#cb33-151" aria-hidden="true" tabindex="-1"></a>            dim: <span class="bu">int</span>,</span>
<span id="cb33-152"><a href="#cb33-152" aria-hidden="true" tabindex="-1"></a>            num_heads: <span class="bu">int</span>,</span>
<span id="cb33-153"><a href="#cb33-153" aria-hidden="true" tabindex="-1"></a>            mlp_ratio: <span class="bu">float</span> <span class="op">=</span> <span class="fl">4.</span>,</span>
<span id="cb33-154"><a href="#cb33-154" aria-hidden="true" tabindex="-1"></a>            qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb33-155"><a href="#cb33-155" aria-hidden="true" tabindex="-1"></a>            proj_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb33-156"><a href="#cb33-156" aria-hidden="true" tabindex="-1"></a>            attn_drop: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb33-157"><a href="#cb33-157" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb33-158"><a href="#cb33-158" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-159"><a href="#cb33-159" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-160"><a href="#cb33-160" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb33-161"><a href="#cb33-161" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb33-162"><a href="#cb33-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-163"><a href="#cb33-163" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> Mlp(</span>
<span id="cb33-164"><a href="#cb33-164" aria-hidden="true" tabindex="-1"></a>            in_features<span class="op">=</span>dim,</span>
<span id="cb33-165"><a href="#cb33-165" aria-hidden="true" tabindex="-1"></a>            hidden_features<span class="op">=</span><span class="bu">int</span>(dim <span class="op">*</span> mlp_ratio),</span>
<span id="cb33-166"><a href="#cb33-166" aria-hidden="true" tabindex="-1"></a>            drop<span class="op">=</span>proj_drop,</span>
<span id="cb33-167"><a href="#cb33-167" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb33-168"><a href="#cb33-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-169"><a href="#cb33-169" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> MHSA(</span>
<span id="cb33-170"><a href="#cb33-170" aria-hidden="true" tabindex="-1"></a>            dim<span class="op">=</span>dim,</span>
<span id="cb33-171"><a href="#cb33-171" aria-hidden="true" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb33-172"><a href="#cb33-172" aria-hidden="true" tabindex="-1"></a>            qkv_bias<span class="op">=</span>qkv_bias,</span>
<span id="cb33-173"><a href="#cb33-173" aria-hidden="true" tabindex="-1"></a>            attn_drop<span class="op">=</span>attn_drop,</span>
<span id="cb33-174"><a href="#cb33-174" aria-hidden="true" tabindex="-1"></a>            proj_drop<span class="op">=</span>proj_drop</span>
<span id="cb33-175"><a href="#cb33-175" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb33-176"><a href="#cb33-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-177"><a href="#cb33-177" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb33-178"><a href="#cb33-178" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.norm1(x))</span>
<span id="cb33-179"><a href="#cb33-179" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.norm2(x))</span>
<span id="cb33-180"><a href="#cb33-180" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-181"><a href="#cb33-181" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-182"><a href="#cb33-182" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> load_pretrained_weights(<span class="va">self</span>, state_dict, block_idx):</span>
<span id="cb33-183"><a href="#cb33-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-184"><a href="#cb33-184" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn.load_pretrained_weights(state_dict, block_idx)</span>
<span id="cb33-185"><a href="#cb33-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-186"><a href="#cb33-186" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1.weight <span class="op">=</span> assign_check(<span class="va">self</span>.norm1.weight, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.norm1.weight'</span>])</span>
<span id="cb33-187"><a href="#cb33-187" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1.bias <span class="op">=</span> assign_check(<span class="va">self</span>.norm1.bias, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.norm1.bias'</span>])</span>
<span id="cb33-188"><a href="#cb33-188" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-189"><a href="#cb33-189" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2.weight <span class="op">=</span> assign_check(<span class="va">self</span>.norm2.weight, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.norm2.weight'</span>])</span>
<span id="cb33-190"><a href="#cb33-190" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2.bias <span class="op">=</span> assign_check(<span class="va">self</span>.norm2.bias, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.norm2.bias'</span>])</span>
<span id="cb33-191"><a href="#cb33-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-192"><a href="#cb33-192" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp.fc1.weight <span class="op">=</span> assign_check(<span class="va">self</span>.mlp.fc1.weight, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.mlp.fc1.weight'</span>])</span>
<span id="cb33-193"><a href="#cb33-193" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp.fc1.bias <span class="op">=</span> assign_check(<span class="va">self</span>.mlp.fc1.bias, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.mlp.fc1.bias'</span>])</span>
<span id="cb33-194"><a href="#cb33-194" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp.fc2.weight <span class="op">=</span> assign_check(<span class="va">self</span>.mlp.fc2.weight, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.mlp.fc2.weight'</span>])</span>
<span id="cb33-195"><a href="#cb33-195" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp.fc2.bias <span class="op">=</span> assign_check(<span class="va">self</span>.mlp.fc2.bias, state_dict[<span class="ss">f'blocks.</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">.mlp.fc2.bias'</span>])</span>
<span id="cb33-196"><a href="#cb33-196" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-197"><a href="#cb33-197" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb33-198"><a href="#cb33-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-199"><a href="#cb33-199" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb33-200"><a href="#cb33-200" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb33-201"><a href="#cb33-201" aria-hidden="true" tabindex="-1"></a>            img_size: Union[<span class="bu">int</span>, Tuple[<span class="bu">int</span>, <span class="bu">int</span>]] <span class="op">=</span> <span class="dv">224</span>,</span>
<span id="cb33-202"><a href="#cb33-202" aria-hidden="true" tabindex="-1"></a>            patch_size: Union[<span class="bu">int</span>, Tuple[<span class="bu">int</span>, <span class="bu">int</span>]] <span class="op">=</span> <span class="dv">16</span>,</span>
<span id="cb33-203"><a href="#cb33-203" aria-hidden="true" tabindex="-1"></a>            in_chans: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb33-204"><a href="#cb33-204" aria-hidden="true" tabindex="-1"></a>            num_classes: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span>,</span>
<span id="cb33-205"><a href="#cb33-205" aria-hidden="true" tabindex="-1"></a>            embed_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span>,</span>
<span id="cb33-206"><a href="#cb33-206" aria-hidden="true" tabindex="-1"></a>            depth: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span>,</span>
<span id="cb33-207"><a href="#cb33-207" aria-hidden="true" tabindex="-1"></a>            num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span>,</span>
<span id="cb33-208"><a href="#cb33-208" aria-hidden="true" tabindex="-1"></a>            mlp_ratio: <span class="bu">float</span> <span class="op">=</span> <span class="fl">4.</span>,</span>
<span id="cb33-209"><a href="#cb33-209" aria-hidden="true" tabindex="-1"></a>            qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb33-210"><a href="#cb33-210" aria-hidden="true" tabindex="-1"></a>            drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb33-211"><a href="#cb33-211" aria-hidden="true" tabindex="-1"></a>            pos_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb33-212"><a href="#cb33-212" aria-hidden="true" tabindex="-1"></a>            proj_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb33-213"><a href="#cb33-213" aria-hidden="true" tabindex="-1"></a>            attn_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span>,</span>
<span id="cb33-214"><a href="#cb33-214" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb33-215"><a href="#cb33-215" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb33-216"><a href="#cb33-216" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb33-217"><a href="#cb33-217" aria-hidden="true" tabindex="-1"></a><span class="co">            img_size: Input image size.</span></span>
<span id="cb33-218"><a href="#cb33-218" aria-hidden="true" tabindex="-1"></a><span class="co">            patch_size: Patch size.</span></span>
<span id="cb33-219"><a href="#cb33-219" aria-hidden="true" tabindex="-1"></a><span class="co">            in_chans: Number of image input channels.</span></span>
<span id="cb33-220"><a href="#cb33-220" aria-hidden="true" tabindex="-1"></a><span class="co">            num_classes: Number of classes for classification heads</span></span>
<span id="cb33-221"><a href="#cb33-221" aria-hidden="true" tabindex="-1"></a><span class="co">            embed_dim: Transformer embedding dimension.</span></span>
<span id="cb33-222"><a href="#cb33-222" aria-hidden="true" tabindex="-1"></a><span class="co">            depth: Depth of transformer.</span></span>
<span id="cb33-223"><a href="#cb33-223" aria-hidden="true" tabindex="-1"></a><span class="co">            num_heads: Number of attention heads.</span></span>
<span id="cb33-224"><a href="#cb33-224" aria-hidden="true" tabindex="-1"></a><span class="co">            mlp_ratio: Ratio of mlp hidden dim to embedding dim.</span></span>
<span id="cb33-225"><a href="#cb33-225" aria-hidden="true" tabindex="-1"></a><span class="co">            qkv_bias: Enable bias for qkv projections if True.</span></span>
<span id="cb33-226"><a href="#cb33-226" aria-hidden="true" tabindex="-1"></a><span class="co">            drop_rate: Head dropout rate.</span></span>
<span id="cb33-227"><a href="#cb33-227" aria-hidden="true" tabindex="-1"></a><span class="co">            pos_drop_rate: Position embedding dropout rate.</span></span>
<span id="cb33-228"><a href="#cb33-228" aria-hidden="true" tabindex="-1"></a><span class="co">            attn_drop_rate: Attention dropout rate.</span></span>
<span id="cb33-229"><a href="#cb33-229" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb33-230"><a href="#cb33-230" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-231"><a href="#cb33-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-232"><a href="#cb33-232" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_classes <span class="op">=</span> num_classes</span>
<span id="cb33-233"><a href="#cb33-233" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_features <span class="op">=</span> <span class="va">self</span>.head_hidden_size <span class="op">=</span> <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim  <span class="co"># for consistency with other models</span></span>
<span id="cb33-234"><a href="#cb33-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-235"><a href="#cb33-235" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the Patch Embedding module - note this does not include the CLS token yet</span></span>
<span id="cb33-236"><a href="#cb33-236" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbed(</span>
<span id="cb33-237"><a href="#cb33-237" aria-hidden="true" tabindex="-1"></a>            img_size<span class="op">=</span>img_size,</span>
<span id="cb33-238"><a href="#cb33-238" aria-hidden="true" tabindex="-1"></a>            patch_size<span class="op">=</span>patch_size,</span>
<span id="cb33-239"><a href="#cb33-239" aria-hidden="true" tabindex="-1"></a>            in_chans<span class="op">=</span>in_chans,</span>
<span id="cb33-240"><a href="#cb33-240" aria-hidden="true" tabindex="-1"></a>            embed_dim<span class="op">=</span>embed_dim,</span>
<span id="cb33-241"><a href="#cb33-241" aria-hidden="true" tabindex="-1"></a>            bias<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb33-242"><a href="#cb33-242" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb33-243"><a href="#cb33-243" aria-hidden="true" tabindex="-1"></a>        num_patches <span class="op">=</span> <span class="va">self</span>.patch_embed.num_patches</span>
<span id="cb33-244"><a href="#cb33-244" aria-hidden="true" tabindex="-1"></a>        embed_len <span class="op">=</span> num_patches <span class="op">+</span> <span class="dv">1</span> <span class="co"># don't forget we need to incorporate the CLS token</span></span>
<span id="cb33-245"><a href="#cb33-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-246"><a href="#cb33-246" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the CLS token, the Positional Encodings/Embeddings, and a Dropout for the Positional information</span></span>
<span id="cb33-247"><a href="#cb33-247" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))</span>
<span id="cb33-248"><a href="#cb33-248" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, embed_len, embed_dim) <span class="op">*</span> <span class="fl">.02</span>)</span>
<span id="cb33-249"><a href="#cb33-249" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span>pos_drop_rate)</span>
<span id="cb33-250"><a href="#cb33-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-251"><a href="#cb33-251" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define LayerNorms for before and after the Encoder block processing</span></span>
<span id="cb33-252"><a href="#cb33-252" aria-hidden="true" tabindex="-1"></a>        norm_layer <span class="op">=</span> partial(nn.LayerNorm, eps<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb33-253"><a href="#cb33-253" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm_pre <span class="op">=</span> norm_layer(embed_dim)</span>
<span id="cb33-254"><a href="#cb33-254" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> norm_layer(embed_dim)</span>
<span id="cb33-255"><a href="#cb33-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-256"><a href="#cb33-256" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the blocks</span></span>
<span id="cb33-257"><a href="#cb33-257" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[</span>
<span id="cb33-258"><a href="#cb33-258" aria-hidden="true" tabindex="-1"></a>            Block(</span>
<span id="cb33-259"><a href="#cb33-259" aria-hidden="true" tabindex="-1"></a>                dim<span class="op">=</span>embed_dim,</span>
<span id="cb33-260"><a href="#cb33-260" aria-hidden="true" tabindex="-1"></a>                num_heads<span class="op">=</span>num_heads,</span>
<span id="cb33-261"><a href="#cb33-261" aria-hidden="true" tabindex="-1"></a>                mlp_ratio<span class="op">=</span>mlp_ratio,</span>
<span id="cb33-262"><a href="#cb33-262" aria-hidden="true" tabindex="-1"></a>                qkv_bias<span class="op">=</span>qkv_bias,</span>
<span id="cb33-263"><a href="#cb33-263" aria-hidden="true" tabindex="-1"></a>                proj_drop<span class="op">=</span>proj_drop_rate,</span>
<span id="cb33-264"><a href="#cb33-264" aria-hidden="true" tabindex="-1"></a>                attn_drop<span class="op">=</span>attn_drop_rate</span>
<span id="cb33-265"><a href="#cb33-265" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb33-266"><a href="#cb33-266" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(depth)])</span>
<span id="cb33-267"><a href="#cb33-267" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-268"><a href="#cb33-268" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_info <span class="op">=</span> [</span>
<span id="cb33-269"><a href="#cb33-269" aria-hidden="true" tabindex="-1"></a>            <span class="bu">dict</span>(module<span class="op">=</span><span class="ss">f'blocks.</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>, num_chs<span class="op">=</span>embed_dim) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb33-270"><a href="#cb33-270" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb33-271"><a href="#cb33-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-272"><a href="#cb33-272" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classifier Head</span></span>
<span id="cb33-273"><a href="#cb33-273" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_drop <span class="op">=</span> nn.Dropout(drop_rate)</span>
<span id="cb33-274"><a href="#cb33-274" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(<span class="va">self</span>.embed_dim, num_classes)</span>
<span id="cb33-275"><a href="#cb33-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-276"><a href="#cb33-276" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_features(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb33-277"><a href="#cb33-277" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-278"><a href="#cb33-278" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the Patch Embeddings</span></span>
<span id="cb33-279"><a href="#cb33-279" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)</span>
<span id="cb33-280"><a href="#cb33-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-281"><a href="#cb33-281" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prepend the CLS token, THEN add in the positional information</span></span>
<span id="cb33-282"><a href="#cb33-282" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat([</span>
<span id="cb33-283"><a href="#cb33-283" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), <span class="co"># have to expand out the batch axis</span></span>
<span id="cb33-284"><a href="#cb33-284" aria-hidden="true" tabindex="-1"></a>            x,</span>
<span id="cb33-285"><a href="#cb33-285" aria-hidden="true" tabindex="-1"></a>        ], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb33-286"><a href="#cb33-286" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed</span>
<span id="cb33-287"><a href="#cb33-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-288"><a href="#cb33-288" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass through the LayerNorms, the Encoder Blocks, then the final Norm</span></span>
<span id="cb33-289"><a href="#cb33-289" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm_pre(x)</span>
<span id="cb33-290"><a href="#cb33-290" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)</span>
<span id="cb33-291"><a href="#cb33-291" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb33-292"><a href="#cb33-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-293"><a href="#cb33-293" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-294"><a href="#cb33-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-295"><a href="#cb33-295" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_head(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb33-296"><a href="#cb33-296" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-297"><a href="#cb33-297" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the CLS token (also called "pooling" our logits)</span></span>
<span id="cb33-298"><a href="#cb33-298" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x[:, <span class="dv">0</span>]</span>
<span id="cb33-299"><a href="#cb33-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-300"><a href="#cb33-300" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass through the Dropout then the classification head</span></span>
<span id="cb33-301"><a href="#cb33-301" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head_drop(x)</span>
<span id="cb33-302"><a href="#cb33-302" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb33-303"><a href="#cb33-303" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-304"><a href="#cb33-304" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-305"><a href="#cb33-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-306"><a href="#cb33-306" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb33-307"><a href="#cb33-307" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.forward_features(x)</span>
<span id="cb33-308"><a href="#cb33-308" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.forward_head(x)</span>
<span id="cb33-309"><a href="#cb33-309" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-310"><a href="#cb33-310" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-311"><a href="#cb33-311" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> load_pretrained_weights(<span class="va">self</span>, state_dict):</span>
<span id="cb33-312"><a href="#cb33-312" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-313"><a href="#cb33-313" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Loading in weights..."</span>)</span>
<span id="cb33-314"><a href="#cb33-314" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-315"><a href="#cb33-315" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> b, block <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.blocks):</span>
<span id="cb33-316"><a href="#cb33-316" aria-hidden="true" tabindex="-1"></a>            block.load_pretrained_weights(state_dict, b)</span>
<span id="cb33-317"><a href="#cb33-317" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Finished with </span><span class="sc">{</span>b<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> blocks..."</span>)</span>
<span id="cb33-318"><a href="#cb33-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-319"><a href="#cb33-319" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed.proj.weight <span class="op">=</span> assign_check(<span class="va">self</span>.patch_embed.proj.weight, state_dict[<span class="st">'patch_embed.proj.weight'</span>])</span>
<span id="cb33-320"><a href="#cb33-320" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed.proj.bias <span class="op">=</span> assign_check(<span class="va">self</span>.patch_embed.proj.bias, state_dict[<span class="st">'patch_embed.proj.bias'</span>])</span>
<span id="cb33-321"><a href="#cb33-321" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> assign_check(<span class="va">self</span>.cls_token, state_dict[<span class="st">'cls_token'</span>])</span>
<span id="cb33-322"><a href="#cb33-322" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> assign_check(<span class="va">self</span>.pos_embed, state_dict[<span class="st">'pos_embed'</span>])</span>
<span id="cb33-323"><a href="#cb33-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-324"><a href="#cb33-324" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Success!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-34" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vit_small_patch16_224(</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>        num_classes: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        pretrained: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        in_chans: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="dv">0</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        pos_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="dv">0</span>,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        attn_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="dv">0</span>,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        proj_drop_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        ):</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> VisionTransformer(</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        img_size<span class="op">=</span><span class="dv">224</span>,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        patch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        in_chans<span class="op">=</span>in_chans,</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        num_classes<span class="op">=</span>num_classes,</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        embed_dim<span class="op">=</span><span class="dv">384</span>,</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        num_heads<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>        depth<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        drop_rate<span class="op">=</span>drop_rate,</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        pos_drop_rate<span class="op">=</span>pos_drop_rate,</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        attn_drop_rate<span class="op">=</span>attn_drop_rate,</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        proj_drop_rate<span class="op">=</span>proj_drop_rate</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pretrained:</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        ckpt <span class="op">=</span> <span class="st">'vit_small_patch16_224'</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> in_chans <span class="op">!=</span> <span class="dv">3</span>:</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Cannot load in checkpoint with </span><span class="sc">{</span>in_chans<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Using checkpoint </span><span class="sc">{</span>ckpt<span class="sc">}</span><span class="ss">...'</span>)</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        hf_model <span class="op">=</span> timm.create_model(ckpt, pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>        model.load_pretrained_weights(hf_model.state_dict())</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-35" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load in our model</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> vit_small_patch16_224(num_classes<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>                             pretrained<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using checkpoint vit_small_patch16_224...
Loading in weights...
Finished with 12 blocks...
Success!</code></pre>
</div>
</div>
</section>
<section id="finetuning-our-vision-transformer" class="level2">
<h2 class="anchored" data-anchor-id="finetuning-our-vision-transformer">Finetuning our Vision Transformer</h2>
<p>Now with everything in place, we can finally move on to actually finetuning our model.</p>
<p>We will use the CIFAR100 dataset since it’s (arguably) rather difficult to perform well on: there’s only 600 images for each of its 100 classes, so training a model from scratch would be very difficult to get to perform well on this dataset. This serves as a nice pressure test for our model, provided we did our loading of the pretrained checkpoint properly.</p>
<p>To spice things up, we’ll add in random augmentations during the training process.</p>
<div id="cell-37" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare CIFAR100</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">224</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>TRAIN_TFMS <span class="op">=</span> transforms.Compose([</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    transforms.RandAugment(),</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">256</span>, <span class="dv">256</span>)),</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize([<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>TEST_TFMS <span class="op">=</span> transforms.Compose([</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">256</span>, <span class="dv">256</span>)),</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize([<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataloader(dataset: torch.utils.data.Dataset,</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>                   batch_size: <span class="bu">int</span>,</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>                   is_train: <span class="bu">bool</span>,</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>                   num_workers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>    loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>batch_size,</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>                        shuffle<span class="op">=</span>is_train, num_workers<span class="op">=</span>num_workers)</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loader</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_cifar100_dataset(root: <span class="bu">str</span>):</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>    trainset <span class="op">=</span> torchvision.datasets.CIFAR100(</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a>        root, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>TRAIN_TFMS</span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>    testset <span class="op">=</span> torchvision.datasets.CIFAR100(</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>        root, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>TEST_TFMS</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trainset, testset</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>train, val <span class="op">=</span> get_cifar100_dataset(<span class="st">"./data"</span>)</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> get_dataloader(train, <span class="dv">32</span>, <span class="va">True</span>)</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>val_dl <span class="op">=</span> get_dataloader(val, <span class="dv">32</span>, <span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified</code></pre>
</div>
</div>
<p>Now we’ll define our training and validation functions.</p>
<p>It’s always good to make your code modular.</p>
<div id="cell-39" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(model, dataloader, criterion, optimizer, device):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Train for one epoch'''</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step, (X, y) <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(dataloader), desc<span class="op">=</span><span class="st">"Training"</span>, leave<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(X)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(logits, y)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> torch.argmax(logits.detach(), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">+=</span> ((y_pred <span class="op">==</span> y).<span class="bu">sum</span>().item() <span class="op">/</span> <span class="bu">len</span>(y))</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> train_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">=</span> train_acc <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loss, train_acc</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.inference_mode</span>()</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_step(model, dataloader, criterion, device):</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>    eval_loss <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>    eval_acc <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (X, y) <span class="kw">in</span> tqdm(dataloader, desc<span class="op">=</span><span class="st">"Evaluating"</span>, leave<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(X)</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(logits, y)</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>        eval_loss <span class="op">+=</span> loss.item()</span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> torch.argmax(logits.detach(), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>        eval_acc <span class="op">+=</span> ((y_pred <span class="op">==</span> y).<span class="bu">sum</span>().item() <span class="op">/</span> <span class="bu">len</span>(y))</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>    eval_loss <span class="op">=</span> eval_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>    eval_acc <span class="op">=</span> eval_acc <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> eval_loss, eval_acc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now to actually kick off training.</p>
<p>We’ll keep things simple: - Finetune for 3 epochs - Use a learning rate of 0.0001 - Use the <code>AdamW</code> optimizer since this usually works well for something as annoying to train as a Vision Transformer</p>
<p>To speed up the training process, we’ll also use <code>torch.compile</code> since our implementation is very straightforward and will not incur any nasty graph breaks or such during the compilation process.</p>
<div id="cell-41" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.<span class="bu">compile</span>(model)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> {k: [] <span class="cf">for</span> k <span class="kw">in</span> (<span class="st">"train_loss"</span>, <span class="st">"train_acc"</span>, <span class="st">"val_loss"</span>, <span class="st">"val_acc"</span>)}</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs), desc<span class="op">=</span><span class="st">"Epochs"</span>):</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    train_loss, train_acc <span class="op">=</span> train_step(model, train_dl, criterion, optimizer, device)</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    val_loss, val_acc <span class="op">=</span> eval_step(model, val_dl, criterion, device)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">"train_loss"</span>].append(train_loss)</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">"train_acc"</span>].append(train_acc)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">"val_loss"</span>].append(val_loss)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">"val_acc"</span>].append(val_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Epochs: 100%|██████████| 3/3 [04:21&lt;00:00, 87.07s/it] </code></pre>
</div>
</div>
<div id="cell-42" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the loss curves</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history[<span class="st">"train_loss"</span>], label<span class="op">=</span><span class="st">"Train Loss"</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>plt.plot(history[<span class="st">"val_loss"</span>], label<span class="op">=</span><span class="st">"Validation Loss"</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Loss Curves"</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Cross Entropy Loss"</span>)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-43" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the loss curves</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history[<span class="st">"train_acc"</span>], label<span class="op">=</span><span class="st">"Train Accuracy"</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>plt.plot(history[<span class="st">"val_acc"</span>], label<span class="op">=</span><span class="st">"Validation Accuracy"</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Accuracy"</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Accuracy"</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Hopefully you can appreciate what we’ve built together from start to end.</p>
<p>You can use the same type of recipe to make a BERT clone, the only real differences there would be (1) the data munging, (2) the positional encodings, and (3) the token embeddings in place of the patch embeddings. Otherwise, the same general principles would apply here and big chunks of the same code can be copied verbatim.</p>
</section>
<section id="fin." class="level1">
<h1>Fin.</h1>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>