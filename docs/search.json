[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zohaib Khan",
    "section": "",
    "text": "About Me\nI’m currently working as a Research Associate, collaborating on projects at the Lahore University of Management Science and under the Fatima Fellowship.\nMy goal is to democratize Machine Learning systems, making them more efficient, robust, and safe\nBroadly, I’m interested in Reasoning, Safety Alignment, and Efficient ML Systems.\nYou can information about my experiences here.\n\n\nExperience\nResearch Collaborator - Stealth (August 2024 – Present)\nResearch Collaborator - Fatima Fellowship (August 2024 – Present)\nResearch Associate - CITY at LUMS (August 2024 – Present)\nResearch Intern/Associate - CSaLT at LUMS (Spring 2022 – Present)\nMachine Learning Engineer - ISSM.ai (Summer 2023)\nDeep Learning Intern - CodeSlash (Summer 2022 – Summer 2023)\n\n\nTeaching\nCS6304 Advanced Topics in Machine Learning (LUMS Fall 2024) - Head TA\nCS5302 Speech and Language Processing (LUMS Spring 2024) - Head TA\nCS535 Machine Learning (LUMS Fall 2023) - Head TA\nCS437 Deep Learning (LUMS Spring 2023) - TA\n\n\nMiscellaneous\nI drink an unhealthy amount of black coffee.\nMy favorite movie is Good Will Hunting.\nI can do a double backflip but only when no one is looking."
  },
  {
    "objectID": "blog/posts/2024-08-03-pruning-pt1/index.html",
    "href": "blog/posts/2024-08-03-pruning-pt1/index.html",
    "title": "An Introduction to Pruning (Part 1)",
    "section": "",
    "text": "Neural network compression refers to a general suite of techniques used to reduce the size and computational complexity of deep learning models. As models grow increasingly large and complex, especially with the advent of deep architectures like ResNets, Vision Transformers, and Large Language Models (LLMs) like GPT, the demand for resources—both in terms of memory and computation—also escalates. This often limits the deployment of such models to environments with abundant resources, like data centers with specialized hardware. However, in many real-world applications, especially on edge devices, there is a critical need to run these models efficiently without sacrificing too much accuracy. Neural network compression addresses this need by reducing model size, which in turn lowers memory usage, speeds up inference times, and makes models more feasible to deploy in resource-constrained environments.\nPruning is a specific technique within the broader field of neural network compression, originating from LeCun, Denker, and Solla (1989). It involves systematically removing parts of the neural network that are deemed unnecessary or less important, thereby simplifying the model. The underlying idea is that not all parts of a neural network contribute equally to its final output. By identifying and removing less important components—such as weights, neurons, or even entire layers—pruning can maintain the model’s performance while significantly reducing its size and complexity.\nPruning mechanisms typically involve evaluating the importance of different parts of a neural network based on certain criteria, such as the magnitude of weights or the activation levels. One common approach is to use the norms of these weights or activations—like the L1 or L2 norm—as a measure of their importance. Weights or activations with lower norms are often considered less critical and are candidates for pruning. By setting a threshold, parts of the network that fall below this threshold can be pruned, while the more significant components are retained. This selective pruning allows the model to remain functional, albeit in a more compact form.\n\n\n\nAs can be seen in the image above (source), pruning can be applied at various levels of granularity, ranging from fine-grained to more coarse-grained methods. Fine-grained pruning involves removing individual weights or neurons, offering the most precise level of control. This approach can lead to highly compact models but may require more sophisticated techniques to maintain performance. On the other end of the spectrum, coarse-grained pruning might involve removing entire channels or even layers from a model. For instance, in convolutional neural networks (CNNs), pruning might target entire filters or channels within a layer, leading to a more regular and structured reduction in model size. This type of pruning is often easier to implement and can lead to more predictable improvements in efficiency.\nPruning is not just limited to CNNs; it also has significant applicability to Large Language Models (LLMs), which are primarily composed of linear layers. Given the vast number of parameters in these models, pruning can be an effective way to reduce their size and make them more suitable for deployment in environments with limited resources. In LLMs, pruning might involve reducing the number of units in fully connected layers or simplifying the attention mechanisms. This can result in models that maintain their ability to understand and generate language while being much more efficient to run.\nIn this blog, we will focus on applying pruning techniques to Convolutional Neural Networks (CNNs). CNNs are particularly interesting for pruning because of their inherently structured nature, which makes them amenable to different granularities of pruning. This will be similar to the methodology followed in Han et al. (2015).\nMuch of the code has been inspired from the labs of the EfficientML.ai course by MIT, which delves into efficient machine learning techniques. By examining CNNs through the lens of pruning, we aim to demonstrate how these techniques can be applied in practice to create more efficient models without compromising too much on performance.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision.models import vgg16, VGG16_Weights\nfrom torchvision import datasets\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n\n\n\nWe’ll start by downloading a pretrained VGG-16 model from torchvision.models. This allows us to utilize the pretrained model’s learned features, providing a strong starting point for our pruning efforts.\nWe’ll then evaluate its size and performance on the CIFAR-10 dataset after a little fine-tuning. This initial evaluation will help us establish a baseline, allowing us to understand the model’s current capabilities in terms of both accuracy and resource usage. By doing so, we can better assess the impact of pruning techniques when we later modify the model to be more efficient. This benchmark will serve as a crucial reference as we explore different pruning strategies, ensuring that any changes we make can be directly compared to the original model’s performance.\n\nweights = VGG16_Weights.DEFAULT\nmodel = vgg16(weights=weights)\n\n\nbatch_size = 64\n\nroot = \"./data\"\ntrain_ds = datasets.CIFAR10(root=root,\n                            train=True,\n                            download=True,\n                            transform=weights.transforms())\ntest_ds = datasets.CIFAR10(root=root,\n                           train=False,\n                           download=True,\n                           transform=weights.transforms())\ntrain_dl = DataLoader(train_ds, \n                      batch_size=batch_size,\n                      shuffle=True)\ntest_dl = DataLoader(test_ds, \n                      batch_size=batch_size,\n                      shuffle=False)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\ndef train_step(model, dataloader, criterion, optimizer, device):\n\n    model.train()\n\n    train_loss = 0.\n    train_acc = 0.\n\n    for step, (X, y) in tqdm(enumerate(dataloader), desc=\"Training\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        train_acc += ((y_pred == y).sum().item() / len(y))\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\n@torch.inference_mode()\ndef eval_step(model, dataloader, criterion, device):\n    \n    model.eval()\n\n    eval_loss = 0.\n    eval_acc = 0.\n\n    for (X, y) in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        eval_loss += loss.item()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        eval_acc += ((y_pred == y).sum().item() / len(y))\n\n    eval_loss = eval_loss / len(dataloader)\n    eval_acc = eval_acc / len(dataloader)\n    return eval_loss, eval_acc\n\n\n# Finetune the model for 3 epochs\nlearning_rate = 1e-4\nepochs = 3\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    train_loss, train_acc = train_step(model, train_dl, criterion, optimizer, device)\n    val_loss, val_acc = eval_step(model, test_dl, criterion, device)\n    print(f\"Epoch {epoch+1}... Train Accuracy: {train_acc:.2f} | Validation Accuracy: {val_acc:.2f}\")\n\nEpochs:  33%|███▎      | 1/3 [03:09&lt;06:19, 189.96s/it]Epochs:  67%|██████▋   | 2/3 [06:18&lt;03:08, 188.85s/it]Epochs: 100%|██████████| 3/3 [09:27&lt;00:00, 189.08s/it]\n\n\nEpoch 1... Train Accuracy: 0.77 | Validation Accuracy: 0.86\nEpoch 2... Train Accuracy: 0.91 | Validation Accuracy: 0.89\nEpoch 3... Train Accuracy: 0.95 | Validation Accuracy: 0.90\n\n\n\nckpt_path = \"vgg16.pth\"\ntorch.save(model.state_dict(), ckpt_path)\n\nWe’ll define some utilities to get the number of parameters here and the size of the model in MB.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n\ndef get_num_parameters(model: nn.Module, count_nonzero_only=False) -&gt; int:\n    \"\"\"\n    calculate the total number of parameters of model\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    num_counted_elements = 0\n    for param in model.parameters():\n        if count_nonzero_only:\n            num_counted_elements += param.count_nonzero()\n        else:\n            num_counted_elements += param.numel()\n    return num_counted_elements\n\n\ndef get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -&gt; int:\n    \"\"\"\n    calculate the model size in bits\n    :param data_width: #bits per element\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    return get_num_parameters(model, count_nonzero_only) * data_width\n\n\n# Get original model size and benchmark accuracy\nMB = 8 * 1024**2\norig_model_size = get_model_size(model)\n_, orig_acc = eval_step(model, test_dl, criterion, device)\n\nprint(f\"Original model accuracy: {orig_acc:.4f}\")\nprint(f\"Original model size: {orig_model_size/(MB)}\")\n\n                                                             \n\n\nOriginal model accuracy: 0.8985\nOriginal model size: 527.7921447753906\n\n\n\n\n\nThe objective of pruning is to zero out the “unimportant” parts of the tensors within a neural network, effectively reducing the model’s complexity without significantly impacting its performance. To determine which parts of the tensor are “unimportant,” we rely on a heuristic that considers the absolute values of the tensor’s elements. The idea is that values closer to zero contribute less to the network’s overall function and can be pruned away with minimal impact. Mathematically, this heuristic can be expressed as \\(\\text{importance} = |W|\\), where $ W$ represents the weights of the network.\nWe begin with fine-grained pruning, where individual connections with the lowest importance are removed.\nThis method allows us to carefully trim down the network by eliminating the least important elements of the weight tensors. To implement this, we can define a target sparsity level that we aim to achieve in the tensors. Using our heuristic, we can establish a threshold corresponding to this target sparsity, which guides us in creating a mask that zeros out the appropriate parts of the weight tensor.\nLet’s explore this approach in code below.\n\ntarget_sparsity = 0.6\n\n# Grab the weights of a conv layer\nx = model.features[0].weight.data.detach().cpu()\nnum_el = x.numel()\n\nprint(f\"Number of elements in x: {num_el}\")\n\n# Visualize the distribution\nbins = 256\nplt.hist(x.view(-1), bins=bins, density=True)\nplt.show()\n\nNumber of elements in x: 1728\n\n\n\n\n\n\n\n\n\nThe distribution of the values in the tensor, as illustrated by the bell(-ish) curve in the figure, shows that most of the values cluster near the center, indicating that they have low magnitudes and are close to zero.\nThis distribution is beneficial for pruning because the majority of the tensor’s values are not crucial to the network’s performance. When we apply thresholding, we effectively cut out the values near the center of the curve, which correspond to the less important elements.\nBy focusing on retaining the heavier elements found at the tails of the distribution, which have higher magnitudes and are likely more significant to the model’s functionality, we can prune a substantial number of elements. This method allows us to reduce the size of the network considerably while preserving the most critical connections that contribute to the model’s predictive power.\n\nimportance = torch.abs(x)\n\nnum_zeros = int(target_sparsity * num_el)\nprint(f\"We want to have {num_zeros} zeros in our tensor of {num_el} elements.\")\n\n# The threshold is defined by the kth smallest magnitude of the (flattened) tensor\nthreshold = torch.kthvalue(\n    input=importance.view(-1),\n    k=num_zeros\n).values\n\nprint(f\"Threshold magnitude to prune under: {threshold:.3f}\")\n\nWe want to have 1036 zeros in our tensor of 1728 elements.\nThreshold magnitude to prune under: 0.173\n\n\nTo implement pruning, we start by creating a mask that identifies the elements to keep and the ones to zero out. The mask assigns a value of 1 to the elements that are above our threshold, indicating they are important and should be retained, while elements below the threshold are set to 0. This mask is then applied to the weight matrix, effectively zeroing out the elements deemed unimportant based on their low magnitudes.\nAfter applying the mask, a significant portion of the weight matrix—specifically, the amount corresponding to our target sparsity—will be pruned, resulting in many values being zeroed out. This reduction should be visible when we plot a histogram of the non-zero values in the weight matrix. The histogram will appear much more sparse, reflecting the substantial decrease in the number of active connections in the network. This visual change highlights the effectiveness of the pruning process in achieving our desired level of sparsity.\n\nmask = (importance &gt; threshold)\n\npruned_x = x.mul_(mask)\n\npruned_els = num_el - pruned_x.count_nonzero()\nprint(f\"Found {pruned_els} zeros post-pruning.\")\n\nto_plot = pruned_x.view(-1)\nto_plot = to_plot[to_plot != 0].view(-1)\nplt.hist(to_plot, bins=bins, density=True)\nplt.show()\n\nFound 1036 zeros post-pruning.\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we’ve explored the basics of pruning at the tensor level, we can start applying this technique to the entire VGG model. To streamline the pruning process across the model’s numerous layers, we’ll implement a utility class called FineGrainedPruner. This class will encapsulate the pruning procedure, allowing us to apply it systematically to every weight tensor in the model.\nThe FineGrainedPruner class will make it easy to perform pruning across the entire VGG model. Thanks to PyTorch’s flexible architecture, we can individually access the weights of each layer within the model, making it straightforward to apply the pruning mask to every relevant tensor. By iterating through the model’s layers, the FineGrainedPruner will ensure that all unimportant connections are zeroed out, reducing the model’s complexity while maintaining its overall performance.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n\n@torch.no_grad()\ndef fine_grained_prune(tensor: torch.Tensor, sparsity: float) -&gt; torch.Tensor:\n    \"\"\"\n    Perform magnitude-based pruning for a single tensor.\n\n    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n    :return: torch.(cuda.)Tensor, mask for zeros\n    \"\"\"\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        tensor.zero_()\n        return torch.zeros_like(tensor)\n    elif sparsity == 0.0:\n        return torch.ones_like(tensor)\n\n    num_elements = tensor.numel()\n\n    num_zeros = round(num_elements * sparsity)\n    importance = torch.abs(tensor)\n    threshold = torch.kthvalue(importance.flatten(), num_zeros)[0]\n    mask = importance &gt; threshold\n\n    tensor.mul_(mask)\n\n    return mask\n\nclass FineGrainedPruner:\n    def __init__(self, model, sparsity_dict):\n        \"\"\"\n        Initialize the FineGrainedPruner with a model and sparsity dictionary.\n\n        :param model: torch.nn.Module, the model to be pruned\n        :param sparsity_dict: dict, keys are parameter names, values are sparsity levels\n        \"\"\"\n        self.masks = self._prune(model, sparsity_dict)\n\n    @torch.no_grad()\n    def apply(self, model):\n        \"\"\"\n        Apply the computed masks to the model's parameters.\n\n        :param model: torch.nn.Module, the model to apply masks to\n        \"\"\"\n        for name, param in model.named_parameters():\n            if name in self.masks:\n                param *= self.masks[name]\n\n    @staticmethod\n    @torch.no_grad()\n    def _prune(model, sparsity_dict):\n        \"\"\"\n        Compute masks for pruning based on the sparsity dictionary.\n\n        :param model: torch.nn.Module, the model to be pruned\n        :param sparsity_dict: dict, keys are parameter names, values are sparsity levels\n        :return: dict, parameter names to their corresponding masks\n        \"\"\"\n        masks = dict()\n        for name, param in model.named_parameters():\n            if param.dim() &gt; 1:  # we only prune conv and fc weights\n                if name in sparsity_dict:\n                    masks[name] = fine_grained_prune(param, sparsity_dict[name])\n        return masks\n\nWhen applying pruning across a model, we still need to determine how much of each tensor should be pruned. However, using a uniform pruning ratio for all tensors might not be the most effective approach. Some tensors are larger or more sensitive than others, meaning that pruning them by the same amount could disproportionately impact the overall model performance. Furthermore, Linear layers have a much higher number of parameters generally in these models in comparison to the Convolution layers.\nTo better understand this, we will plot the weight distributions for all convolutional and linear layers, as well as the distribution of the number of parameters across these layers. This will give us a clearer picture of the variation in tensor sizes and importance.\nNext, we’ll conduct a Sensitivity Search, where we prune each layer independently and evaluate the model’s performance after each isolated change—without any fine-tuning or calibration. This approach will help us identify which layers are more sensitive to pruning and will provide valuable insights into how to determine the optimal pruning ratios for each layer. This way, we can apply non-uniform pruning ratios that are tailored to the characteristics of each layer, ensuring a more effective and balanced pruning strategy.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\ndef plot_weight_distribution(model, bins=256, count_nonzero_only=False):\n    fig, axes = plt.subplots(4, 4, figsize=(10, 6))\n    axes = axes.ravel()\n    plot_index = 0\n    \n    for name, module in model.named_modules():\n        if not isinstance(module, (nn.Conv2d, nn.Linear)):\n            continue\n        param = module.weight\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            if count_nonzero_only:\n                param_cpu = param.detach().view(-1).cpu()\n                param_cpu = param_cpu[param_cpu != 0].view(-1)\n                ax.hist(param_cpu, bins=bins, density=True,\n                        color = 'blue', alpha = 0.5)\n            else:\n                ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n                        color = 'blue', alpha = 0.5)\n            ax.set_xlabel(name)\n            ax.set_ylabel('density')\n            plot_index += 1\n\n    fig.suptitle('Histogram of Weights')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    plt.show()\n\nplot_weight_distribution(model)\n\n\n\n\n\n\n\n\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\ndef plot_num_parameters_distribution(model):\n    num_parameters = dict()\n    for name, module in model.named_modules():\n        if not isinstance(module, (nn.Conv2d, nn.Linear)):\n            continue\n        param = module.weight\n        if param.dim() &gt; 1:\n            num_parameters[name] = param.numel()\n    fig = plt.figure(figsize=(8, 6))\n    plt.grid(axis='y')\n    plt.bar(list(num_parameters.keys()), list(num_parameters.values()))\n    plt.title('#Parameter Distribution')\n    plt.ylabel('Number of Parameters')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n\nplot_num_parameters_distribution(model)\n\n\n\n\n\n\n\n\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n@torch.no_grad()\ndef sensitivity_scan(model, dataloader, scan_step=0.1, scan_start=0.4, scan_end=1.0, verbose=True):\n    sparsities = np.arange(start=scan_start, stop=scan_end, step=scan_step)\n    accuracies = []\n    named_conv_weights = [(name, param) for (name, param) \\\n                          in model.named_parameters() if param.dim() &gt; 1]\n    for i_layer, (name, param) in enumerate(named_conv_weights):\n        param_clone = param.detach().clone()\n        accuracy = []\n        for sparsity in tqdm(sparsities, desc=f'scanning {i_layer}/{len(named_conv_weights)} weight - {name}'):\n            fine_grained_prune(param.detach(), sparsity=sparsity)\n            _, acc = eval_step(model, dataloader, criterion, device)\n            # restore\n            param.copy_(param_clone)\n            accuracy.append(acc)\n        if verbose:\n            print(f'\\r    sparsity=[{\",\".join([\"{:.2f}\".format(x) for x in sparsities])}]: accuracy=[{\", \".join([\"{:.2f}\".format(x) for x in accuracy])}]', end='')\n        accuracies.append(accuracy)\n    return sparsities, accuracies\n\nsparsities, accuracies = sensitivity_scan(\n    model, test_dl, scan_step=0.1, scan_start=0.4, scan_end=1.0)\n\nscanning 0/16 weight - features.0.weight: 100%|██████████| 6/6 [01:29&lt;00:00, 14.94s/it]\nscanning 1/16 weight - features.2.weight: 100%|██████████| 6/6 [01:28&lt;00:00, 14.83s/it]\nscanning 2/16 weight - features.5.weight: 100%|██████████| 6/6 [01:29&lt;00:00, 14.91s/it]\nscanning 3/16 weight - features.7.weight: 100%|██████████| 6/6 [01:29&lt;00:00, 14.95s/it]\nscanning 4/16 weight - features.10.weight: 100%|██████████| 6/6 [01:29&lt;00:00, 14.84s/it]\nscanning 5/16 weight - features.12.weight: 100%|██████████| 6/6 [01:26&lt;00:00, 14.45s/it]\nscanning 6/16 weight - features.14.weight: 100%|██████████| 6/6 [01:31&lt;00:00, 15.20s/it]\nscanning 7/16 weight - features.17.weight: 100%|██████████| 6/6 [01:28&lt;00:00, 14.79s/it]\nscanning 8/16 weight - features.19.weight: 100%|██████████| 6/6 [01:27&lt;00:00, 14.52s/it]\nscanning 9/16 weight - features.21.weight: 100%|██████████| 6/6 [01:27&lt;00:00, 14.62s/it]\nscanning 10/16 weight - features.24.weight: 100%|██████████| 6/6 [01:27&lt;00:00, 14.52s/it]\nscanning 11/16 weight - features.26.weight: 100%|██████████| 6/6 [01:25&lt;00:00, 14.21s/it]\nscanning 12/16 weight - features.28.weight: 100%|██████████| 6/6 [01:30&lt;00:00, 15.10s/it]\nscanning 13/16 weight - classifier.0.weight: 100%|██████████| 6/6 [01:32&lt;00:00, 15.40s/it]\nscanning 14/16 weight - classifier.3.weight: 100%|██████████| 6/6 [01:31&lt;00:00, 15.28s/it]\nscanning 15/16 weight - classifier.6.weight: 100%|██████████| 6/6 [01:28&lt;00:00, 14.67s/it]\n\n\n    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.88, 0.86, 0.79, 0.64, 0.48, 0.26]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.85]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.89, 0.84]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.87, 0.76]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.90, 0.88, 0.83]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.84]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.89, 0.89, 0.85, 0.66]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.87, 0.75]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.80]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.82]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.90, 0.89, 0.84]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.85]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.82]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.90, 0.90, 0.90]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.90, 0.90, 0.90]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.89, 0.89, 0.89, 0.86]\n\n\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\ndef plot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy):\n    \"\"\"\n    Plot sensitivity curves for model pruning.\n\n    :param sparsities: List or array of pruning sparsities.\n    :param accuracies: List or array of accuracies corresponding to each sparsity.\n    :param dense_model_accuracy: Accuracy of the model without pruning.\n    \"\"\"\n    # Normalize dense model accuracy to the range of 0 to 1 if it's not already\n    dense_model_accuracy = min(max(dense_model_accuracy, 0.0), 1.0)\n    \n    # Calculate lower bound accuracy for plotting reference line\n    lower_bound_accuracy = 1 - (1 - dense_model_accuracy) * 1.5\n    \n    # Ensure lower_bound_accuracy is within [0, 1]\n    lower_bound_accuracy = min(max(lower_bound_accuracy, 0.0), 1.0)\n    \n    fig, axes = plt.subplots(4, 4, figsize=(15, 12))\n    axes = axes.ravel()\n    plot_index = 0\n    \n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            \n            # Plot accuracy vs sparsity curve\n            curve = ax.plot(sparsities, accuracies[plot_index], label='Accuracy after pruning')\n            \n            # Plot lower bound accuracy line\n            line = ax.plot(sparsities, [lower_bound_accuracy] * len(sparsities), '--', color='red', \n                           label=f'{lower_bound_accuracy / dense_model_accuracy * 100:.0f}% of dense model accuracy')\n            \n            # Set x and y axis labels and limits\n            ax.set_xticks(np.arange(start=0.4, stop=1.0, step=0.1))\n            ax.set_ylim(0, 1)  # Set y-axis limits to be between 0 and 1\n            \n            # Set title and labels\n            ax.set_title(name)\n            ax.set_xlabel('Pruning Ratio')\n            ax.set_ylabel('Accuracy')\n            \n            # Add legend and grid\n            ax.legend()\n            ax.grid(axis='x')\n            \n            plot_index += 1\n    \n    # Adjust layout and title\n    fig.suptitle('Sensitivity Curves: Validation Accuracy vs. Pruning Sparsity')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    \n    plt.show()\n\nplot_sensitivity_scan(sparsities, accuracies, val_acc)\n\n\n\n\n\n\n\n\nAfter plotting the sensitivity curves and analyzing the distribution of parameters, we can use this information to decide how much to prune each layer. The sensitivity plots show how the model’s performance changes as we increase the pruning ratio for each layer. Layers with more parameters and less sensitivity—where the performance curves show smaller dips as pruning increases—are typically more forgiving. This suggests that we can prune these layers more aggressively without significantly impacting the overall model performance.\nOur goal is to prune as much as possible while maintaining the model’s effectiveness. By focusing on layers with a high parameter count and low sensitivity, we can maximize the reduction in model complexity with minimal performance loss. This approach ensures that we optimize the pruning process, trimming away unnecessary components while preserving the key features that contribute to the model’s success.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\nsparsity_dict = {\n    'features.0.weight': 0, # very sensitive\n\n    'features.2.weight': 0.7, # not sensitive\n    'features.5.weight': 0.7,\n    'features.7.weight': 0.7,\n    'features.10.weight': 0.7,\n    'features.12.weight': 0.7,\n\n    'features.14.weight': 0.6, # little more sensitive\n    'features.17.weight': 0.6,\n    'features.19.weight': 0.6,\n    'features.21.weight': 0.6,\n    'features.24.weight': 0.6,\n    'features.26.weight': 0.6,\n    'features.28.weight': 0.6,\n\n    'classifier.0.weight': 0.8, # not sensitive and has TOO many params\n    \n    'classifier.3.weight': 0.6, # not sensitive and has a good chunk of params\n    'classifier.6.weight': 0.6\n}\n\nget_sparsity = lambda tensor: 1 - float(tensor.count_nonzero()) / tensor.numel()\npruner = FineGrainedPruner(model, sparsity_dict)\n\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model size: {sparse_model_size / (8 * 1024**2):.2f} MiB = {sparse_model_size / orig_model_size * 100:.2f}% of dense model size\")\n_, sparse_model_accuracy = eval_step(model, test_dl, criterion, device)\nprint(f\"Sparse model Accuracy: {sparse_model_accuracy:.2f} (before finetuning)\")\n\nplot_weight_distribution(model, count_nonzero_only=True)\n\nSparse model size: 132.32 MiB = 25.07% of dense model size\nSparse model Accuracy: 0.84 (before finetuning)\n\n\n                                                             \n\n\n\n\n\n\n\n\n\nAfter pruning, our results show that we successfully reduced the model size by nearly 75%, with only a 6% drop in accuracy. While this is an impressive reduction, we don’t have to stop here.\nIt’s a common practice to fine-tune the model post-pruning to recover some of the lost performance. This makes sense—suddenly forcing a smaller portion of the model to handle the full inference task can be overwhelming, so giving the model a chance to recalibrate itself is essential.\nWe will fine-tune the model for an additional three epochs, ensuring that pruning is reapplied after each step. This reinforcement is necessary because gradient descent during fine-tuning might adjust the previously pruned weights back to non-zero values. By consistently applying the pruning mask, we ensure that the model remains in its pruned state, allowing it to adapt to its new, more compact structure while regaining some of the lost performance.\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    train_loss, train_acc = train_step(model, train_dl, criterion, optimizer, device)\n    val_loss, val_acc = eval_step(model, test_dl, criterion, device)\n    print(f\"Epoch {epoch+1}... Train Accuracy: {train_acc:.2f} | Validation Accuracy: {val_acc:.2f}\")\n    pruner.apply(model)\n\nEpochs:  33%|███▎      | 1/3 [03:09&lt;06:19, 189.78s/it]Epochs:  67%|██████▋   | 2/3 [06:20&lt;03:10, 190.46s/it]Epochs: 100%|██████████| 3/3 [09:32&lt;00:00, 190.82s/it]\n\n\nEpoch 1... Train Accuracy: 0.94 | Validation Accuracy: 0.89\nEpoch 2... Train Accuracy: 0.95 | Validation Accuracy: 0.91\nEpoch 3... Train Accuracy: 0.96 | Validation Accuracy: 0.91\n\n\n\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model size: {sparse_model_size / (8 * 1024**2):.2f} MiB = {sparse_model_size / orig_model_size * 100:.2f}% of dense model size\")\n_, sparse_model_accuracy = eval_step(model, test_dl, criterion, device)\nprint(f\"Sparse model Accuracy: {sparse_model_accuracy:.2f} (after finetuning)\")\n\nSparse model size: 132.32 MiB = 25.07% of dense model size\nSparse model Accuracy: 0.93 (after finetuning)\n\n\n                                                             \n\n\nIn conclusion, fine-grained pruning has enabled us to significantly reduce the number of effective parameters in our model, cutting down the model size by a substantial amount. The subsequent fine-tuning not only recovered much of the lost performance but actually improved the model’s accuracy beyond its initial level. This results in a more efficient model that occupies only a quarter of the original storage cost.\nDespite these gains, it’s important to consider whether the time spent on fine-tuning, which remained the same as before, justifies the pruning. While fine-grained pruning can be highly effective, it also comes with its challenges. Zeroing out individual connections in the network is primarily beneficial with specialized hardware designed to handle sparse matrices efficiently. Without such hardware, the computational and memory overhead of working with sparsely populated tensors can offset some of the benefits of pruning.\nIn contrast, channel-wise pruning offers a more regular approach by removing entire channels or layers from the network. This method involves explicitly slicing out chunks of the parameter tensors, resulting in a model where the pruned parts are completely removed rather than simply zeroed out. This approach is often easier to implement and can be more compatible with standard hardware, making it a practical alternative to fine-grained pruning.\nWe will explore this in a future blog."
  },
  {
    "objectID": "blog/posts/2024-08-03-pruning-pt1/index.html#readying-our-model-and-data",
    "href": "blog/posts/2024-08-03-pruning-pt1/index.html#readying-our-model-and-data",
    "title": "An Introduction to Pruning (Part 1)",
    "section": "",
    "text": "We’ll start by downloading a pretrained VGG-16 model from torchvision.models. This allows us to utilize the pretrained model’s learned features, providing a strong starting point for our pruning efforts.\nWe’ll then evaluate its size and performance on the CIFAR-10 dataset after a little fine-tuning. This initial evaluation will help us establish a baseline, allowing us to understand the model’s current capabilities in terms of both accuracy and resource usage. By doing so, we can better assess the impact of pruning techniques when we later modify the model to be more efficient. This benchmark will serve as a crucial reference as we explore different pruning strategies, ensuring that any changes we make can be directly compared to the original model’s performance.\n\nweights = VGG16_Weights.DEFAULT\nmodel = vgg16(weights=weights)\n\n\nbatch_size = 64\n\nroot = \"./data\"\ntrain_ds = datasets.CIFAR10(root=root,\n                            train=True,\n                            download=True,\n                            transform=weights.transforms())\ntest_ds = datasets.CIFAR10(root=root,\n                           train=False,\n                           download=True,\n                           transform=weights.transforms())\ntrain_dl = DataLoader(train_ds, \n                      batch_size=batch_size,\n                      shuffle=True)\ntest_dl = DataLoader(test_ds, \n                      batch_size=batch_size,\n                      shuffle=False)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\ndef train_step(model, dataloader, criterion, optimizer, device):\n\n    model.train()\n\n    train_loss = 0.\n    train_acc = 0.\n\n    for step, (X, y) in tqdm(enumerate(dataloader), desc=\"Training\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        train_acc += ((y_pred == y).sum().item() / len(y))\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\n@torch.inference_mode()\ndef eval_step(model, dataloader, criterion, device):\n    \n    model.eval()\n\n    eval_loss = 0.\n    eval_acc = 0.\n\n    for (X, y) in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        eval_loss += loss.item()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        eval_acc += ((y_pred == y).sum().item() / len(y))\n\n    eval_loss = eval_loss / len(dataloader)\n    eval_acc = eval_acc / len(dataloader)\n    return eval_loss, eval_acc\n\n\n# Finetune the model for 3 epochs\nlearning_rate = 1e-4\nepochs = 3\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    train_loss, train_acc = train_step(model, train_dl, criterion, optimizer, device)\n    val_loss, val_acc = eval_step(model, test_dl, criterion, device)\n    print(f\"Epoch {epoch+1}... Train Accuracy: {train_acc:.2f} | Validation Accuracy: {val_acc:.2f}\")\n\nEpochs:  33%|███▎      | 1/3 [03:09&lt;06:19, 189.96s/it]Epochs:  67%|██████▋   | 2/3 [06:18&lt;03:08, 188.85s/it]Epochs: 100%|██████████| 3/3 [09:27&lt;00:00, 189.08s/it]\n\n\nEpoch 1... Train Accuracy: 0.77 | Validation Accuracy: 0.86\nEpoch 2... Train Accuracy: 0.91 | Validation Accuracy: 0.89\nEpoch 3... Train Accuracy: 0.95 | Validation Accuracy: 0.90\n\n\n\nckpt_path = \"vgg16.pth\"\ntorch.save(model.state_dict(), ckpt_path)\n\nWe’ll define some utilities to get the number of parameters here and the size of the model in MB.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n\ndef get_num_parameters(model: nn.Module, count_nonzero_only=False) -&gt; int:\n    \"\"\"\n    calculate the total number of parameters of model\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    num_counted_elements = 0\n    for param in model.parameters():\n        if count_nonzero_only:\n            num_counted_elements += param.count_nonzero()\n        else:\n            num_counted_elements += param.numel()\n    return num_counted_elements\n\n\ndef get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -&gt; int:\n    \"\"\"\n    calculate the model size in bits\n    :param data_width: #bits per element\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    return get_num_parameters(model, count_nonzero_only) * data_width\n\n\n# Get original model size and benchmark accuracy\nMB = 8 * 1024**2\norig_model_size = get_model_size(model)\n_, orig_acc = eval_step(model, test_dl, criterion, device)\n\nprint(f\"Original model accuracy: {orig_acc:.4f}\")\nprint(f\"Original model size: {orig_model_size/(MB)}\")\n\n                                                             \n\n\nOriginal model accuracy: 0.8985\nOriginal model size: 527.7921447753906"
  },
  {
    "objectID": "blog/posts/2024-08-03-pruning-pt1/index.html#starting-with-pruning",
    "href": "blog/posts/2024-08-03-pruning-pt1/index.html#starting-with-pruning",
    "title": "An Introduction to Pruning (Part 1)",
    "section": "",
    "text": "The objective of pruning is to zero out the “unimportant” parts of the tensors within a neural network, effectively reducing the model’s complexity without significantly impacting its performance. To determine which parts of the tensor are “unimportant,” we rely on a heuristic that considers the absolute values of the tensor’s elements. The idea is that values closer to zero contribute less to the network’s overall function and can be pruned away with minimal impact. Mathematically, this heuristic can be expressed as \\(\\text{importance} = |W|\\), where $ W$ represents the weights of the network.\nWe begin with fine-grained pruning, where individual connections with the lowest importance are removed.\nThis method allows us to carefully trim down the network by eliminating the least important elements of the weight tensors. To implement this, we can define a target sparsity level that we aim to achieve in the tensors. Using our heuristic, we can establish a threshold corresponding to this target sparsity, which guides us in creating a mask that zeros out the appropriate parts of the weight tensor.\nLet’s explore this approach in code below.\n\ntarget_sparsity = 0.6\n\n# Grab the weights of a conv layer\nx = model.features[0].weight.data.detach().cpu()\nnum_el = x.numel()\n\nprint(f\"Number of elements in x: {num_el}\")\n\n# Visualize the distribution\nbins = 256\nplt.hist(x.view(-1), bins=bins, density=True)\nplt.show()\n\nNumber of elements in x: 1728\n\n\n\n\n\n\n\n\n\nThe distribution of the values in the tensor, as illustrated by the bell(-ish) curve in the figure, shows that most of the values cluster near the center, indicating that they have low magnitudes and are close to zero.\nThis distribution is beneficial for pruning because the majority of the tensor’s values are not crucial to the network’s performance. When we apply thresholding, we effectively cut out the values near the center of the curve, which correspond to the less important elements.\nBy focusing on retaining the heavier elements found at the tails of the distribution, which have higher magnitudes and are likely more significant to the model’s functionality, we can prune a substantial number of elements. This method allows us to reduce the size of the network considerably while preserving the most critical connections that contribute to the model’s predictive power.\n\nimportance = torch.abs(x)\n\nnum_zeros = int(target_sparsity * num_el)\nprint(f\"We want to have {num_zeros} zeros in our tensor of {num_el} elements.\")\n\n# The threshold is defined by the kth smallest magnitude of the (flattened) tensor\nthreshold = torch.kthvalue(\n    input=importance.view(-1),\n    k=num_zeros\n).values\n\nprint(f\"Threshold magnitude to prune under: {threshold:.3f}\")\n\nWe want to have 1036 zeros in our tensor of 1728 elements.\nThreshold magnitude to prune under: 0.173\n\n\nTo implement pruning, we start by creating a mask that identifies the elements to keep and the ones to zero out. The mask assigns a value of 1 to the elements that are above our threshold, indicating they are important and should be retained, while elements below the threshold are set to 0. This mask is then applied to the weight matrix, effectively zeroing out the elements deemed unimportant based on their low magnitudes.\nAfter applying the mask, a significant portion of the weight matrix—specifically, the amount corresponding to our target sparsity—will be pruned, resulting in many values being zeroed out. This reduction should be visible when we plot a histogram of the non-zero values in the weight matrix. The histogram will appear much more sparse, reflecting the substantial decrease in the number of active connections in the network. This visual change highlights the effectiveness of the pruning process in achieving our desired level of sparsity.\n\nmask = (importance &gt; threshold)\n\npruned_x = x.mul_(mask)\n\npruned_els = num_el - pruned_x.count_nonzero()\nprint(f\"Found {pruned_els} zeros post-pruning.\")\n\nto_plot = pruned_x.view(-1)\nto_plot = to_plot[to_plot != 0].view(-1)\nplt.hist(to_plot, bins=bins, density=True)\nplt.show()\n\nFound 1036 zeros post-pruning."
  },
  {
    "objectID": "blog/posts/2024-08-03-pruning-pt1/index.html#pruning-our-model",
    "href": "blog/posts/2024-08-03-pruning-pt1/index.html#pruning-our-model",
    "title": "An Introduction to Pruning (Part 1)",
    "section": "",
    "text": "Now that we’ve explored the basics of pruning at the tensor level, we can start applying this technique to the entire VGG model. To streamline the pruning process across the model’s numerous layers, we’ll implement a utility class called FineGrainedPruner. This class will encapsulate the pruning procedure, allowing us to apply it systematically to every weight tensor in the model.\nThe FineGrainedPruner class will make it easy to perform pruning across the entire VGG model. Thanks to PyTorch’s flexible architecture, we can individually access the weights of each layer within the model, making it straightforward to apply the pruning mask to every relevant tensor. By iterating through the model’s layers, the FineGrainedPruner will ensure that all unimportant connections are zeroed out, reducing the model’s complexity while maintaining its overall performance.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n\n@torch.no_grad()\ndef fine_grained_prune(tensor: torch.Tensor, sparsity: float) -&gt; torch.Tensor:\n    \"\"\"\n    Perform magnitude-based pruning for a single tensor.\n\n    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n    :return: torch.(cuda.)Tensor, mask for zeros\n    \"\"\"\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        tensor.zero_()\n        return torch.zeros_like(tensor)\n    elif sparsity == 0.0:\n        return torch.ones_like(tensor)\n\n    num_elements = tensor.numel()\n\n    num_zeros = round(num_elements * sparsity)\n    importance = torch.abs(tensor)\n    threshold = torch.kthvalue(importance.flatten(), num_zeros)[0]\n    mask = importance &gt; threshold\n\n    tensor.mul_(mask)\n\n    return mask\n\nclass FineGrainedPruner:\n    def __init__(self, model, sparsity_dict):\n        \"\"\"\n        Initialize the FineGrainedPruner with a model and sparsity dictionary.\n\n        :param model: torch.nn.Module, the model to be pruned\n        :param sparsity_dict: dict, keys are parameter names, values are sparsity levels\n        \"\"\"\n        self.masks = self._prune(model, sparsity_dict)\n\n    @torch.no_grad()\n    def apply(self, model):\n        \"\"\"\n        Apply the computed masks to the model's parameters.\n\n        :param model: torch.nn.Module, the model to apply masks to\n        \"\"\"\n        for name, param in model.named_parameters():\n            if name in self.masks:\n                param *= self.masks[name]\n\n    @staticmethod\n    @torch.no_grad()\n    def _prune(model, sparsity_dict):\n        \"\"\"\n        Compute masks for pruning based on the sparsity dictionary.\n\n        :param model: torch.nn.Module, the model to be pruned\n        :param sparsity_dict: dict, keys are parameter names, values are sparsity levels\n        :return: dict, parameter names to their corresponding masks\n        \"\"\"\n        masks = dict()\n        for name, param in model.named_parameters():\n            if param.dim() &gt; 1:  # we only prune conv and fc weights\n                if name in sparsity_dict:\n                    masks[name] = fine_grained_prune(param, sparsity_dict[name])\n        return masks\n\nWhen applying pruning across a model, we still need to determine how much of each tensor should be pruned. However, using a uniform pruning ratio for all tensors might not be the most effective approach. Some tensors are larger or more sensitive than others, meaning that pruning them by the same amount could disproportionately impact the overall model performance. Furthermore, Linear layers have a much higher number of parameters generally in these models in comparison to the Convolution layers.\nTo better understand this, we will plot the weight distributions for all convolutional and linear layers, as well as the distribution of the number of parameters across these layers. This will give us a clearer picture of the variation in tensor sizes and importance.\nNext, we’ll conduct a Sensitivity Search, where we prune each layer independently and evaluate the model’s performance after each isolated change—without any fine-tuning or calibration. This approach will help us identify which layers are more sensitive to pruning and will provide valuable insights into how to determine the optimal pruning ratios for each layer. This way, we can apply non-uniform pruning ratios that are tailored to the characteristics of each layer, ensuring a more effective and balanced pruning strategy.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\ndef plot_weight_distribution(model, bins=256, count_nonzero_only=False):\n    fig, axes = plt.subplots(4, 4, figsize=(10, 6))\n    axes = axes.ravel()\n    plot_index = 0\n    \n    for name, module in model.named_modules():\n        if not isinstance(module, (nn.Conv2d, nn.Linear)):\n            continue\n        param = module.weight\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            if count_nonzero_only:\n                param_cpu = param.detach().view(-1).cpu()\n                param_cpu = param_cpu[param_cpu != 0].view(-1)\n                ax.hist(param_cpu, bins=bins, density=True,\n                        color = 'blue', alpha = 0.5)\n            else:\n                ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n                        color = 'blue', alpha = 0.5)\n            ax.set_xlabel(name)\n            ax.set_ylabel('density')\n            plot_index += 1\n\n    fig.suptitle('Histogram of Weights')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    plt.show()\n\nplot_weight_distribution(model)\n\n\n\n\n\n\n\n\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\ndef plot_num_parameters_distribution(model):\n    num_parameters = dict()\n    for name, module in model.named_modules():\n        if not isinstance(module, (nn.Conv2d, nn.Linear)):\n            continue\n        param = module.weight\n        if param.dim() &gt; 1:\n            num_parameters[name] = param.numel()\n    fig = plt.figure(figsize=(8, 6))\n    plt.grid(axis='y')\n    plt.bar(list(num_parameters.keys()), list(num_parameters.values()))\n    plt.title('#Parameter Distribution')\n    plt.ylabel('Number of Parameters')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n\nplot_num_parameters_distribution(model)\n\n\n\n\n\n\n\n\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n@torch.no_grad()\ndef sensitivity_scan(model, dataloader, scan_step=0.1, scan_start=0.4, scan_end=1.0, verbose=True):\n    sparsities = np.arange(start=scan_start, stop=scan_end, step=scan_step)\n    accuracies = []\n    named_conv_weights = [(name, param) for (name, param) \\\n                          in model.named_parameters() if param.dim() &gt; 1]\n    for i_layer, (name, param) in enumerate(named_conv_weights):\n        param_clone = param.detach().clone()\n        accuracy = []\n        for sparsity in tqdm(sparsities, desc=f'scanning {i_layer}/{len(named_conv_weights)} weight - {name}'):\n            fine_grained_prune(param.detach(), sparsity=sparsity)\n            _, acc = eval_step(model, dataloader, criterion, device)\n            # restore\n            param.copy_(param_clone)\n            accuracy.append(acc)\n        if verbose:\n            print(f'\\r    sparsity=[{\",\".join([\"{:.2f}\".format(x) for x in sparsities])}]: accuracy=[{\", \".join([\"{:.2f}\".format(x) for x in accuracy])}]', end='')\n        accuracies.append(accuracy)\n    return sparsities, accuracies\n\nsparsities, accuracies = sensitivity_scan(\n    model, test_dl, scan_step=0.1, scan_start=0.4, scan_end=1.0)\n\nscanning 0/16 weight - features.0.weight: 100%|██████████| 6/6 [01:29&lt;00:00, 14.94s/it]\nscanning 1/16 weight - features.2.weight: 100%|██████████| 6/6 [01:28&lt;00:00, 14.83s/it]\nscanning 2/16 weight - features.5.weight: 100%|██████████| 6/6 [01:29&lt;00:00, 14.91s/it]\nscanning 3/16 weight - features.7.weight: 100%|██████████| 6/6 [01:29&lt;00:00, 14.95s/it]\nscanning 4/16 weight - features.10.weight: 100%|██████████| 6/6 [01:29&lt;00:00, 14.84s/it]\nscanning 5/16 weight - features.12.weight: 100%|██████████| 6/6 [01:26&lt;00:00, 14.45s/it]\nscanning 6/16 weight - features.14.weight: 100%|██████████| 6/6 [01:31&lt;00:00, 15.20s/it]\nscanning 7/16 weight - features.17.weight: 100%|██████████| 6/6 [01:28&lt;00:00, 14.79s/it]\nscanning 8/16 weight - features.19.weight: 100%|██████████| 6/6 [01:27&lt;00:00, 14.52s/it]\nscanning 9/16 weight - features.21.weight: 100%|██████████| 6/6 [01:27&lt;00:00, 14.62s/it]\nscanning 10/16 weight - features.24.weight: 100%|██████████| 6/6 [01:27&lt;00:00, 14.52s/it]\nscanning 11/16 weight - features.26.weight: 100%|██████████| 6/6 [01:25&lt;00:00, 14.21s/it]\nscanning 12/16 weight - features.28.weight: 100%|██████████| 6/6 [01:30&lt;00:00, 15.10s/it]\nscanning 13/16 weight - classifier.0.weight: 100%|██████████| 6/6 [01:32&lt;00:00, 15.40s/it]\nscanning 14/16 weight - classifier.3.weight: 100%|██████████| 6/6 [01:31&lt;00:00, 15.28s/it]\nscanning 15/16 weight - classifier.6.weight: 100%|██████████| 6/6 [01:28&lt;00:00, 14.67s/it]\n\n\n    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.88, 0.86, 0.79, 0.64, 0.48, 0.26]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.85]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.89, 0.84]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.87, 0.76]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.90, 0.88, 0.83]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.84]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.89, 0.89, 0.85, 0.66]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.87, 0.75]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.80]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.82]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.90, 0.89, 0.84]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.85]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.89, 0.88, 0.82]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.90, 0.90, 0.90]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.90, 0.90, 0.90, 0.90]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[0.90, 0.90, 0.89, 0.89, 0.89, 0.86]\n\n\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\ndef plot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy):\n    \"\"\"\n    Plot sensitivity curves for model pruning.\n\n    :param sparsities: List or array of pruning sparsities.\n    :param accuracies: List or array of accuracies corresponding to each sparsity.\n    :param dense_model_accuracy: Accuracy of the model without pruning.\n    \"\"\"\n    # Normalize dense model accuracy to the range of 0 to 1 if it's not already\n    dense_model_accuracy = min(max(dense_model_accuracy, 0.0), 1.0)\n    \n    # Calculate lower bound accuracy for plotting reference line\n    lower_bound_accuracy = 1 - (1 - dense_model_accuracy) * 1.5\n    \n    # Ensure lower_bound_accuracy is within [0, 1]\n    lower_bound_accuracy = min(max(lower_bound_accuracy, 0.0), 1.0)\n    \n    fig, axes = plt.subplots(4, 4, figsize=(15, 12))\n    axes = axes.ravel()\n    plot_index = 0\n    \n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            \n            # Plot accuracy vs sparsity curve\n            curve = ax.plot(sparsities, accuracies[plot_index], label='Accuracy after pruning')\n            \n            # Plot lower bound accuracy line\n            line = ax.plot(sparsities, [lower_bound_accuracy] * len(sparsities), '--', color='red', \n                           label=f'{lower_bound_accuracy / dense_model_accuracy * 100:.0f}% of dense model accuracy')\n            \n            # Set x and y axis labels and limits\n            ax.set_xticks(np.arange(start=0.4, stop=1.0, step=0.1))\n            ax.set_ylim(0, 1)  # Set y-axis limits to be between 0 and 1\n            \n            # Set title and labels\n            ax.set_title(name)\n            ax.set_xlabel('Pruning Ratio')\n            ax.set_ylabel('Accuracy')\n            \n            # Add legend and grid\n            ax.legend()\n            ax.grid(axis='x')\n            \n            plot_index += 1\n    \n    # Adjust layout and title\n    fig.suptitle('Sensitivity Curves: Validation Accuracy vs. Pruning Sparsity')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    \n    plt.show()\n\nplot_sensitivity_scan(sparsities, accuracies, val_acc)\n\n\n\n\n\n\n\n\nAfter plotting the sensitivity curves and analyzing the distribution of parameters, we can use this information to decide how much to prune each layer. The sensitivity plots show how the model’s performance changes as we increase the pruning ratio for each layer. Layers with more parameters and less sensitivity—where the performance curves show smaller dips as pruning increases—are typically more forgiving. This suggests that we can prune these layers more aggressively without significantly impacting the overall model performance.\nOur goal is to prune as much as possible while maintaining the model’s effectiveness. By focusing on layers with a high parameter count and low sensitivity, we can maximize the reduction in model complexity with minimal performance loss. This approach ensures that we optimize the pruning process, trimming away unnecessary components while preserving the key features that contribute to the model’s success.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\nsparsity_dict = {\n    'features.0.weight': 0, # very sensitive\n\n    'features.2.weight': 0.7, # not sensitive\n    'features.5.weight': 0.7,\n    'features.7.weight': 0.7,\n    'features.10.weight': 0.7,\n    'features.12.weight': 0.7,\n\n    'features.14.weight': 0.6, # little more sensitive\n    'features.17.weight': 0.6,\n    'features.19.weight': 0.6,\n    'features.21.weight': 0.6,\n    'features.24.weight': 0.6,\n    'features.26.weight': 0.6,\n    'features.28.weight': 0.6,\n\n    'classifier.0.weight': 0.8, # not sensitive and has TOO many params\n    \n    'classifier.3.weight': 0.6, # not sensitive and has a good chunk of params\n    'classifier.6.weight': 0.6\n}\n\nget_sparsity = lambda tensor: 1 - float(tensor.count_nonzero()) / tensor.numel()\npruner = FineGrainedPruner(model, sparsity_dict)\n\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model size: {sparse_model_size / (8 * 1024**2):.2f} MiB = {sparse_model_size / orig_model_size * 100:.2f}% of dense model size\")\n_, sparse_model_accuracy = eval_step(model, test_dl, criterion, device)\nprint(f\"Sparse model Accuracy: {sparse_model_accuracy:.2f} (before finetuning)\")\n\nplot_weight_distribution(model, count_nonzero_only=True)\n\nSparse model size: 132.32 MiB = 25.07% of dense model size\nSparse model Accuracy: 0.84 (before finetuning)\n\n\n                                                             \n\n\n\n\n\n\n\n\n\nAfter pruning, our results show that we successfully reduced the model size by nearly 75%, with only a 6% drop in accuracy. While this is an impressive reduction, we don’t have to stop here.\nIt’s a common practice to fine-tune the model post-pruning to recover some of the lost performance. This makes sense—suddenly forcing a smaller portion of the model to handle the full inference task can be overwhelming, so giving the model a chance to recalibrate itself is essential.\nWe will fine-tune the model for an additional three epochs, ensuring that pruning is reapplied after each step. This reinforcement is necessary because gradient descent during fine-tuning might adjust the previously pruned weights back to non-zero values. By consistently applying the pruning mask, we ensure that the model remains in its pruned state, allowing it to adapt to its new, more compact structure while regaining some of the lost performance.\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    train_loss, train_acc = train_step(model, train_dl, criterion, optimizer, device)\n    val_loss, val_acc = eval_step(model, test_dl, criterion, device)\n    print(f\"Epoch {epoch+1}... Train Accuracy: {train_acc:.2f} | Validation Accuracy: {val_acc:.2f}\")\n    pruner.apply(model)\n\nEpochs:  33%|███▎      | 1/3 [03:09&lt;06:19, 189.78s/it]Epochs:  67%|██████▋   | 2/3 [06:20&lt;03:10, 190.46s/it]Epochs: 100%|██████████| 3/3 [09:32&lt;00:00, 190.82s/it]\n\n\nEpoch 1... Train Accuracy: 0.94 | Validation Accuracy: 0.89\nEpoch 2... Train Accuracy: 0.95 | Validation Accuracy: 0.91\nEpoch 3... Train Accuracy: 0.96 | Validation Accuracy: 0.91\n\n\n\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model size: {sparse_model_size / (8 * 1024**2):.2f} MiB = {sparse_model_size / orig_model_size * 100:.2f}% of dense model size\")\n_, sparse_model_accuracy = eval_step(model, test_dl, criterion, device)\nprint(f\"Sparse model Accuracy: {sparse_model_accuracy:.2f} (after finetuning)\")\n\nSparse model size: 132.32 MiB = 25.07% of dense model size\nSparse model Accuracy: 0.93 (after finetuning)\n\n\n                                                             \n\n\nIn conclusion, fine-grained pruning has enabled us to significantly reduce the number of effective parameters in our model, cutting down the model size by a substantial amount. The subsequent fine-tuning not only recovered much of the lost performance but actually improved the model’s accuracy beyond its initial level. This results in a more efficient model that occupies only a quarter of the original storage cost.\nDespite these gains, it’s important to consider whether the time spent on fine-tuning, which remained the same as before, justifies the pruning. While fine-grained pruning can be highly effective, it also comes with its challenges. Zeroing out individual connections in the network is primarily beneficial with specialized hardware designed to handle sparse matrices efficiently. Without such hardware, the computational and memory overhead of working with sparsely populated tensors can offset some of the benefits of pruning.\nIn contrast, channel-wise pruning offers a more regular approach by removing entire channels or layers from the network. This method involves explicitly slicing out chunks of the parameter tensors, resulting in a model where the pruned parts are completely removed rather than simply zeroed out. This approach is often easier to implement and can be more compatible with standard hardware, making it a practical alternative to fine-grained pruning.\nWe will explore this in a future blog."
  },
  {
    "objectID": "blog/posts/2024-08-15-pruning-pt2/index.html",
    "href": "blog/posts/2024-08-15-pruning-pt2/index.html",
    "title": "An Introduction to Pruning (Part 2)",
    "section": "",
    "text": "In the previous part of this blog series, we covered the basics of pruning, specifically fine-grained pruning. If you haven’t had a chance to read it yet, I encourage you to check out that notebook to familiarize yourself with the foundational concepts and understanding the motivation behind what we will do now.\nIn this section, we will review the different types of pruning methods, focusing particularly on channel-wise pruning. Pruning can range from fine-grained approaches, where individual connections are zeroed out based on their importance, to more structured forms like channel-wise pruning. Channel-wise pruning is a more regular method that involves removing entire channels or layers from the network.\nSimilar to fine-grained pruning, we will use magnitudes as a heuristic measure of importance to guide our pruning decisions. However, the key advantage of channel-wise pruning over fine-grained approaches is its structure. By removing entire channels, we achieve a more organized pruning process, which doesn’t rely heavily on specialized hardware for efficient computation. This is because channel-wise pruning involves explicitly slicing out chunks of the parameter tensors, making it more straightforward and potentially more compatible with standard hardware.\nThis article has been inspired from the labs of the EfficientML.ai course by MIT. In the following sections, we will delve into the specifics of channel-wise pruning and how it can be effectively implemented to achieve significant model compression while maintaining computational efficiency. This will be similar to the methodology followed in Han et al. (2015).\n\nfrom typing import List, Union\nimport copy\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision.models import vgg16, VGG16_Weights\nfrom torchvision import transforms, datasets\nfrom tqdm import tqdm\n\n\n\nWe will be using the same setup as the previous blog: a VGG-16 architecture that we will prune and evaluate on the CIFAR-10 dataset.\n\n\nCode\nweights = VGG16_Weights.DEFAULT\nmodel = vgg16(weights=weights)\n\nbatch_size = 64\n\nroot = \"./data\"\ntrain_ds = datasets.CIFAR10(root=root,\n                            train=True,\n                            download=True,\n                            transform=weights.transforms())\ntest_ds = datasets.CIFAR10(root=root,\n                           train=False,\n                           download=True,\n                           transform=weights.transforms())\ntrain_dl = DataLoader(train_ds, \n                      batch_size=batch_size,\n                      shuffle=True)\ntest_dl = DataLoader(test_ds, \n                      batch_size=batch_size,\n                      shuffle=False)\n\ndef get_num_parameters(model: nn.Module, count_nonzero_only=False) -&gt; int:\n    \"\"\"\n    calculate the total number of parameters of model\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    num_counted_elements = 0\n    for param in model.parameters():\n        if count_nonzero_only:\n            num_counted_elements += param.count_nonzero()\n        else:\n            num_counted_elements += param.numel()\n    return num_counted_elements\n\n\ndef get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -&gt; int:\n    \"\"\"\n    calculate the model size in bits\n    \n    :param data_width: #bits per element\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    return get_num_parameters(model, count_nonzero_only) * data_width\n\ndef train_step(model, dataloader, criterion, optimizer, device):\n\n    model.train()\n\n    train_loss = 0.\n    train_acc = 0.\n\n    for step, (X, y) in tqdm(enumerate(dataloader), desc=\"Training\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        train_acc += ((y_pred == y).sum().item() / len(y))\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\n@torch.inference_mode()\ndef eval_step(model, dataloader, criterion, device):\n    \n    model.eval()\n\n    eval_loss = 0.\n    eval_acc = 0.\n\n    for (X, y) in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        eval_loss += loss.item()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        eval_acc += ((y_pred == y).sum().item() / len(y))\n\n    eval_loss = eval_loss / len(dataloader)\n    eval_acc = eval_acc / len(dataloader)\n    return eval_loss, eval_acc\n\ncriterion = nn.CrossEntropyLoss()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)\n\nckpt_path = \"vgg16.pth\"\nmodel.load_state_dict(torch.load(ckpt_path))\n\n\n\n# Get original model size and benchmark accuracy\nval_loss, orig_acc = eval_step(model, test_dl, criterion, device)\nMB = 8 * 1024**2\norig_model_size_mb = get_model_size(model) / MB\n\nprint(f\"Original model accuracy: {orig_acc:.2f}\")\nprint(f\"Original model size: {orig_model_size_mb:.2f} MB\")\n\n                                                             \n\n\nOriginal model accuracy: 0.90\nOriginal model size: 527.79 MB\n\n\n\n\n\nTo begin with channel-wise pruning, it’s essential to understand the structure of the weight tensor in a convolutional block. A convolutional layer typically has a weight tensor with dimensions corresponding to the number of input channels, output channels, and the kernel size.\nWhen considering channel-wise pruning, we need to determine which axis of this tensor we will be pruning along. Specifically, our goal is to identify and remove the less important channels from either the input or output of the convolutional layer. We will see later what considerations we have to keep in mind.\n\n# Examine the structure of a Convolution layer\nin_chans = 32\nout_chans = 64\nkernel_size = 3\nconv_layer = nn.Conv2d(in_channels=in_chans,\n                       out_channels=out_chans,\n                       kernel_size=kernel_size)\n\nconv_weight = conv_layer.weight.data\n\nprint(f\"In Channels: {in_chans}\")\nprint(f\"Out Channels: {out_chans}\")\nprint(f\"Kernel Size: {kernel_size}\")\nprint(f\"Shape of Conv Layer weight tensor: {conv_weight.shape}\")\n\nIn Channels: 32\nOut Channels: 64\nKernel Size: 3\nShape of Conv Layer weight tensor: torch.Size([64, 32, 3, 3])\n\n\nTo delve deeper into channel-wise pruning, it’s crucial to examine the weight tensor’s structure more closely. For a convolutional layer, the weight tensor typically has the shape \\((c_{out}, c_{in}, k_h, k_w)\\), where \\(c_{out}\\) and \\(c_{in}\\) denote the number of output and input channels respectively, and \\(k_h\\) and \\(k_w\\) represent the height and width of the kernels.\nWhen considering channel-wise pruning, our primary focus is on the second axis of the weight tensor — the \\(c_{in}\\) dimension, which represents the input channels. Pruning channels involves slicing out certain portions along this axis. However, it’s important to recognize that removing channels from this dimension will alter the shape of the activation maps that pass through the convolutional layer.\nTo effectively implement channel-wise pruning, we need to understand how the changes to one layer’s weight tensor will impact subsequent layers in the network. Specifically, if we prune input channels, the number of input channels to subsequent layers will be affected. This alteration requires us to adjust the subsequent layers accordingly to ensure that they can still process the modified activations.\nExamining the VGG architecture, we can develop a strategy for pruning that takes these dependencies into account. For example, VGG networks consist of a series of convolutional layers followed by fully connected layers. If we prune channels in one convolutional layer, we must update the subsequent layers to match the new input dimensions and ensure that the network remains functional.\nIn summary, while pruning channels from the second axis of the weight tensor is straightforward, the resulting changes in activation shapes necessitate careful consideration of the network’s overall architecture. By understanding and adjusting for these changes, we can effectively apply channel-wise pruning while maintaining the integrity of the model.\n\n# Examine model architecture\nmodel\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\nThe VGG architecture features alternating blocks of convolutional layers, ReLUs, and MaxPooling operations. To apply channel-wise pruning effectively, we should focus on each pair of adjacent convolutional blocks.\nFor each pair, we adjust the output channels of the preceding convolutional block and the input channels of the subsequent convolutional block. This approach ensures that the changes made by pruning are consistent throughout the network, maintaining the correct flow of data and preserving model functionality.\n\n# Dummy example\nclass ConvBlock(nn.Module):\n    def __init__(self, in_chans, hidden_size, out_chans):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_chans, out_channels=hidden_size, kernel_size=3)\n        self.mp = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=out_chans, kernel_size=3)\n    \n    def forward(self, x):\n        return self.conv2(self.mp(self.conv1(x)))\n\nin_chans = 16\nhidden_size = 32\nout_chans = 64\nimg_size = 28\n\nx = torch.randn(in_chans, img_size, img_size)\nblock = ConvBlock(in_chans, hidden_size, out_chans)\n\nout = block(x)\nblock_orig_numparams = get_num_parameters(block)\nprint(f\"Number of parameters in block: {block_orig_numparams}\")\nprint(f\"Output shape: {out.shape}\")\n\nNumber of parameters in block: 23136\nOutput shape: torch.Size([64, 11, 11])\n\n\n\ndef get_num_channels_to_keep(channels: int, prune_ratio: float) -&gt; int:\n    \"\"\"\n    A function to calculate the number of layers to PRESERVE after pruning\n    \"\"\"\n    return int(round((1-prune_ratio)*channels))\n\n# Get the weights from the block\nconv1 = block.conv1\nmp = block.mp\nconv2 = block.conv2\n\n# Start pruning the channels\nprune_ratio = 0.6\noriginal_channels = conv1.out_channels\nn_keep = get_num_channels_to_keep(original_channels, prune_ratio)\n\n# 1. Prune the output channels of the first convolution layer\nwith torch.no_grad():\n    conv1.weight = nn.Parameter(\n        conv1.weight.detach()[:n_keep, ...]\n    )\n    # Adjust the bias as well, if it exists\n    if conv1.bias is not None:\n        conv1.bias = nn.Parameter(\n            conv1.bias.detach()[:n_keep]\n        )\n\n# 2. Prune the affected input channels of the next convolution\nwith torch.no_grad():\n    conv2.weight = nn.Parameter(\n        conv2.weight.detach()[:, :n_keep, ...]\n    )\n    # Bias does not need to be adjusted for this layer\n\n\nprint(f\"Number of parameters in block (post-pruning): {get_num_parameters(block)}\")\nprint(f\"Output shape: {out.shape}\")\n\nNumber of parameters in block (post-pruning): 9437\nOutput shape: torch.Size([64, 11, 11])\n\n\nAfter experimenting with a dummy example of pruning input and output channels within a convolutional block, we’ve observed a reduction in the number of parameters while maintaining the same output shape. However, it’s important to note that the number of parameters remaining is not precisely the product of the prune ratio and the total number of parameters, as we’re only modifying the channels and not the entire network.\nTo streamline this process, we’ll define a function that automates channel-wise pruning for the entire model. This function will handle adjusting the number of input and output channels for each convolutional layer, ensuring that the pruned network remains consistent and functional. This automated approach will simplify the pruning procedure and facilitate efficient model compression.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n\n@torch.no_grad()\ndef channel_prune(model: nn.Module,\n                  prune_ratio: Union[List, float]) -&gt; nn.Module:\n    \"\"\"\n    Apply channel pruning to each convolutional layer in the backbone.\n\n    :param model: The model to be pruned\n    :param prune_ratio: Either a single float for uniform pruning across \n                        layers or a list of floats specifying per-layer pruning rates.\n\n    :return pruned_model: The model with pruned channels\n    \"\"\"\n    assert isinstance(prune_ratio, (float, list))\n\n    conv_layers = [m for m in model.features if isinstance(m, nn.Conv2d)]\n    n_conv = len(conv_layers)\n    \n    # The ratio affects the first conv's input channels, and the next one's out channels \n    if isinstance(prune_ratio, float):\n        prune_ratio = [prune_ratio] * (n_conv - 1)\n    else:\n        assert len(prune_ratio) == n_conv - 1, \"prune_ratio list length must be one less than the number of Conv2d layers.\"\n\n    # Create a deepcopy so we don't modify the original\n    pruned_model = copy.deepcopy(model)\n    conv_layers = [m for m in pruned_model.features if isinstance(m, nn.Conv2d)]\n\n    # Apply channel pruning to each pair of consecutive convolutional layers\n    for i, ratio in enumerate(prune_ratio):\n        prev_conv = conv_layers[i]\n        next_conv = conv_layers[i + 1]\n        prev_channels = prev_conv.out_channels\n        n_keep = get_num_channels_to_keep(prev_channels, ratio)\n\n        with torch.no_grad():\n            # Prune the output channels of the previous convolution\n            prev_conv.weight = nn.Parameter(prev_conv.weight.detach()[:n_keep, ...])\n            if prev_conv.bias is not None:\n                prev_conv.bias = nn.Parameter(prev_conv.bias.detach()[:n_keep])\n\n            # Prune the input channels of the next convolution\n            next_conv.weight = nn.Parameter(next_conv.weight.detach()[:, :n_keep, ...])\n\n    print(\"Channel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\")\n    return pruned_model\n\n\n# Prune the model without considering the channel importances\npruned_model_naive = channel_prune(model, 0.4)\npruned_model_size_mb = get_model_size(pruned_model_naive) / MB\n\nprint(f\"Original model size: {orig_model_size_mb:.2f} MB\")\nprint(f\"Pruned model size: {pruned_model_size_mb:.2f} MB\")\n\nChannel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\nOriginal model size: 527.79 MB\nPruned model size: 494.03 MB\n\n\n\n# Evaluate the model after this crude pruning\n_, acc = eval_step(pruned_model_naive, test_dl, criterion, device)\n\nprint(f\"Accuracy after naive pruning: {acc:.2f}\")\n\n                                                             \n\n\nAccuracy after naive pruning: 0.10\n\n\n\n\n\nWhen considering channel-wise pruning, it’s crucial to avoid arbitrary removal of channels, as this could lead to significant performance degradation by discarding “important” channels, as was seen above. To address this, we need to prioritize channels based on their importance.\nA practical approach to determining channel importance is to use the norms of the weight tensors as a measure. The idea is that channels with larger norms are generally more critical for the network’s performance. By calculating these norms, we can rank the channels and selectively prune the less important ones.\nThe process involves defining the importance of each channel, sorting them accordingly, and then retaining only the most important channels. We can integrate this approach into our existing pruning function, which previously kept the first n_keep channels. By incorporating channel importance into this function, we can ensure that the pruning is more strategic, preserving the channels that contribute most significantly to the network’s performance.\n\n# Grab a random convolution weight tensor to demonstrate computing channel importances\nrand_weight = model.features[2].weight\nprint(f\"Shape: {rand_weight.shape}\")\n\ndef get_input_channel_importance(weight):\n    in_channels = weight.shape[1]\n    importances = []\n    \n    # Compute the importance for each input channel\n    for i_c in range(in_channels):\n        channel_weight = weight.detach()[:, i_c] # (c_out, k, k)\n        importance = torch.norm(channel_weight) # take the Frobenius norm\n        importances.append(importance.view(1))\n    return torch.cat(importances)\n\nprint(f\"Importances of the 64 input channels:\\n{get_input_channel_importance(rand_weight)}\")\n\nShape: torch.Size([64, 64, 3, 3])\nImportances of the 64 input channels:\ntensor([1.6980, 1.7297, 2.1539, 0.9993, 1.8365, 0.8037, 1.6834, 0.9603, 1.1872,\n        1.3582, 1.1134, 1.0917, 1.4087, 0.9050, 1.7213, 0.7823, 1.1833, 1.1185,\n        1.1565, 2.2874, 1.3824, 1.4115, 1.0174, 1.3120, 1.1977, 0.9108, 0.7976,\n        0.9291, 1.1520, 1.1238, 0.9578, 0.7938, 1.4062, 1.4817, 2.5130, 1.0180,\n        1.3782, 0.9571, 0.9826, 1.3465, 1.0445, 0.8921, 1.5498, 0.7251, 1.1079,\n        0.9550, 1.3082, 1.2728, 0.9647, 0.8078, 0.7796, 2.1746, 1.1919, 2.0185,\n        0.7407, 1.4707, 1.0315, 1.8911, 2.1096, 2.2035, 0.9893, 0.7218, 0.7914,\n        1.1358], device='cuda:0')\n\n\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n\n@torch.no_grad()\ndef apply_channel_sorting(model):\n    '''\n    Sorts the channels in decreasing order of importance for the given model\n\n    :param model: Model to apply the channel sorting to\n    '''\n    # Create a deep copy of the model to avoid modifying the original\n    sorted_model = copy.deepcopy(model)\n\n    # Fetch all the convolutional layers from the backbone\n    conv_layers = [m for m in sorted_model.features if isinstance(m, nn.Conv2d)]\n\n    # Iterate through the convolutional layers and sort channels by importance\n    for i in range(len(conv_layers) - 1):\n        prev_conv = conv_layers[i]\n        next_conv = conv_layers[i + 1]\n\n        # Compute the importance of input channels for the next convolutional layer\n        importance = get_input_channel_importance(next_conv.weight)\n        sort_idx = torch.argsort(importance, descending=True)\n\n        # Sort the output channels of the previous convolutional layer\n        prev_conv.weight = nn.Parameter(torch.index_select(prev_conv.weight.detach(), 0, sort_idx))\n        if prev_conv.bias is not None:\n            prev_conv.bias = nn.Parameter(torch.index_select(prev_conv.bias.detach(), 0, sort_idx))\n\n        # Sort the input channels of the next convolutional layer\n        next_conv.weight = nn.Parameter(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n\n    return sorted_model\n\nWe directly manipulate the slices of the corresponding channels within the weight and bias tensors. By sorting channels based on their importance in decreasing order, we can identify and retain the most critical channels as per our desired pruning ratio.\nAfter sorting, we keep the first n_keep slices, which ensures that the most important channels are preserved. This method leverages our previous function for pruning, allowing us to efficiently apply the revised approach. The rearrangement of tensor slices according to channel importance ensures that the pruned network maintains its effectiveness while reducing its size.\n\nchannel_pruning_ratio = 0.4  # pruned-out ratio\n\nprint(\"Without sorting channels by importance...\")\npruned_model = channel_prune(model, channel_pruning_ratio)\n_, acc = eval_step(pruned_model, test_dl, criterion, device)\nprint(f\"Pruned model accuracy: {acc:.2f}\")\n\nprint('-'*25)\n\nprint(\"With sorting channels by importance...\")\nsorted_model = apply_channel_sorting(model)\npruned_model = channel_prune(sorted_model, channel_pruning_ratio)\n_, acc = eval_step(pruned_model, test_dl, criterion, device)\nprint(f\"Pruned model accuracy: {acc:.2f}\")\n\nWithout sorting channels by importance...\nChannel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\nPruned model accuracy: 0.10\n-------------------------\nWith sorting channels by importance...\nChannel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\nPruned model accuracy: 0.15\n\n\n                                                                                                                          \n\n\n\n\n\nWith the sorting, we observe a somewhat smaller performance decrease compared to the initial approach, though the drop is still significant. This reduction in performance is typical with channel-wise pruning because entire chunks of the model are removed, which can affect its ability to generalize.\nTo mitigate the performance loss, we again employ fine-tuning. This step is essential to allow the model to adjust to the new, pruned structure and recover some of its original performance. Fine-tuning helps recalibrate the remaining parameters and optimize the network for its reduced size.\n\n# Finetune to recover performance\nlearning_rate = 1e-4\nepochs = 3\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(pruned_model.parameters(), lr=learning_rate)\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    train_loss, train_acc = train_step(pruned_model, train_dl, criterion, optimizer, device)\n    val_loss, val_acc = eval_step(pruned_model, test_dl, criterion, device)\n    print(f\"Epoch {epoch+1}... Train Accuracy: {train_acc:.2f} | Validation Accuracy: {val_acc:.2f}\")\n\nEpochs:  33%|███▎      | 1/3 [02:14&lt;04:29, 134.74s/it]Epochs:  67%|██████▋   | 2/3 [04:32&lt;02:16, 136.25s/it]Epochs: 100%|██████████| 3/3 [06:47&lt;00:00, 135.89s/it]\n\n\nEpoch 1... Train Accuracy: 0.87 | Validation Accuracy: 0.85\nEpoch 2... Train Accuracy: 0.94 | Validation Accuracy: 0.90\nEpoch 3... Train Accuracy: 0.97 | Validation Accuracy: 0.90\n\n\n\n_, acc = eval_step(pruned_model, test_dl, criterion, device)\n\nprint(f\"Validation accuracy of original (dense) model: {orig_acc:.3f}\")\nprint(f\"Final validation accuracy of pruned model: {acc:.3f}\")\n\npruned_model_size_mb = get_model_size(pruned_model) / MB\n\nprint(f\"Original model size: {orig_model_size_mb:.2f} MB\")\nprint(f\"Pruned model size: {pruned_model_size_mb:.2f} MB\")\n\n                                                             \n\n\nValidation accuracy of original (dense) model: 0.898\nFinal validation accuracy of pruned model: 0.899\nOriginal model size: 527.79 MB\nPruned model size: 494.03 MB\n\n\n\n# Test the latency of the original and pruned models\nx = next(iter(train_dl))[0].cuda()\n\n%timeit model(x)\n%timeit pruned_model(x)\n\n53.7 ms ± 85.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n35.2 ms ± 75.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nWe can see from the outputs that the pruned model not only (virtually) matches the performance of the original model on the dataset, but is also actually smaller and takes lesser time for inference.\nOne of the notable advantages of channel-wise pruning is that it does not require special hardware to handle the zeros introduced during fine-grained pruning. Since channel-wise pruning involves removing entire channels rather than zeroing out individual connections, the pruned model becomes inherently more efficient. This structured approach results in a more compact model that requires less storage and computation, making it easier to deploy and operate without the need for hardware designed to optimize sparse matrices. Thus, channel-wise pruning not only helps reduce the model size but also simplifies the computational requirements for inference, leading to overall efficiency gains."
  },
  {
    "objectID": "blog/posts/2024-08-15-pruning-pt2/index.html#setup",
    "href": "blog/posts/2024-08-15-pruning-pt2/index.html#setup",
    "title": "An Introduction to Pruning (Part 2)",
    "section": "",
    "text": "We will be using the same setup as the previous blog: a VGG-16 architecture that we will prune and evaluate on the CIFAR-10 dataset.\n\n\nCode\nweights = VGG16_Weights.DEFAULT\nmodel = vgg16(weights=weights)\n\nbatch_size = 64\n\nroot = \"./data\"\ntrain_ds = datasets.CIFAR10(root=root,\n                            train=True,\n                            download=True,\n                            transform=weights.transforms())\ntest_ds = datasets.CIFAR10(root=root,\n                           train=False,\n                           download=True,\n                           transform=weights.transforms())\ntrain_dl = DataLoader(train_ds, \n                      batch_size=batch_size,\n                      shuffle=True)\ntest_dl = DataLoader(test_ds, \n                      batch_size=batch_size,\n                      shuffle=False)\n\ndef get_num_parameters(model: nn.Module, count_nonzero_only=False) -&gt; int:\n    \"\"\"\n    calculate the total number of parameters of model\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    num_counted_elements = 0\n    for param in model.parameters():\n        if count_nonzero_only:\n            num_counted_elements += param.count_nonzero()\n        else:\n            num_counted_elements += param.numel()\n    return num_counted_elements\n\n\ndef get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -&gt; int:\n    \"\"\"\n    calculate the model size in bits\n    \n    :param data_width: #bits per element\n    :param count_nonzero_only: only count nonzero weights\n    \"\"\"\n    return get_num_parameters(model, count_nonzero_only) * data_width\n\ndef train_step(model, dataloader, criterion, optimizer, device):\n\n    model.train()\n\n    train_loss = 0.\n    train_acc = 0.\n\n    for step, (X, y) in tqdm(enumerate(dataloader), desc=\"Training\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        train_acc += ((y_pred == y).sum().item() / len(y))\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\n@torch.inference_mode()\ndef eval_step(model, dataloader, criterion, device):\n    \n    model.eval()\n\n    eval_loss = 0.\n    eval_acc = 0.\n\n    for (X, y) in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        eval_loss += loss.item()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        eval_acc += ((y_pred == y).sum().item() / len(y))\n\n    eval_loss = eval_loss / len(dataloader)\n    eval_acc = eval_acc / len(dataloader)\n    return eval_loss, eval_acc\n\ncriterion = nn.CrossEntropyLoss()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)\n\nckpt_path = \"vgg16.pth\"\nmodel.load_state_dict(torch.load(ckpt_path))\n\n\n\n# Get original model size and benchmark accuracy\nval_loss, orig_acc = eval_step(model, test_dl, criterion, device)\nMB = 8 * 1024**2\norig_model_size_mb = get_model_size(model) / MB\n\nprint(f\"Original model accuracy: {orig_acc:.2f}\")\nprint(f\"Original model size: {orig_model_size_mb:.2f} MB\")\n\n                                                             \n\n\nOriginal model accuracy: 0.90\nOriginal model size: 527.79 MB"
  },
  {
    "objectID": "blog/posts/2024-08-15-pruning-pt2/index.html#channel-wise-pruning",
    "href": "blog/posts/2024-08-15-pruning-pt2/index.html#channel-wise-pruning",
    "title": "An Introduction to Pruning (Part 2)",
    "section": "",
    "text": "To begin with channel-wise pruning, it’s essential to understand the structure of the weight tensor in a convolutional block. A convolutional layer typically has a weight tensor with dimensions corresponding to the number of input channels, output channels, and the kernel size.\nWhen considering channel-wise pruning, we need to determine which axis of this tensor we will be pruning along. Specifically, our goal is to identify and remove the less important channels from either the input or output of the convolutional layer. We will see later what considerations we have to keep in mind.\n\n# Examine the structure of a Convolution layer\nin_chans = 32\nout_chans = 64\nkernel_size = 3\nconv_layer = nn.Conv2d(in_channels=in_chans,\n                       out_channels=out_chans,\n                       kernel_size=kernel_size)\n\nconv_weight = conv_layer.weight.data\n\nprint(f\"In Channels: {in_chans}\")\nprint(f\"Out Channels: {out_chans}\")\nprint(f\"Kernel Size: {kernel_size}\")\nprint(f\"Shape of Conv Layer weight tensor: {conv_weight.shape}\")\n\nIn Channels: 32\nOut Channels: 64\nKernel Size: 3\nShape of Conv Layer weight tensor: torch.Size([64, 32, 3, 3])\n\n\nTo delve deeper into channel-wise pruning, it’s crucial to examine the weight tensor’s structure more closely. For a convolutional layer, the weight tensor typically has the shape \\((c_{out}, c_{in}, k_h, k_w)\\), where \\(c_{out}\\) and \\(c_{in}\\) denote the number of output and input channels respectively, and \\(k_h\\) and \\(k_w\\) represent the height and width of the kernels.\nWhen considering channel-wise pruning, our primary focus is on the second axis of the weight tensor — the \\(c_{in}\\) dimension, which represents the input channels. Pruning channels involves slicing out certain portions along this axis. However, it’s important to recognize that removing channels from this dimension will alter the shape of the activation maps that pass through the convolutional layer.\nTo effectively implement channel-wise pruning, we need to understand how the changes to one layer’s weight tensor will impact subsequent layers in the network. Specifically, if we prune input channels, the number of input channels to subsequent layers will be affected. This alteration requires us to adjust the subsequent layers accordingly to ensure that they can still process the modified activations.\nExamining the VGG architecture, we can develop a strategy for pruning that takes these dependencies into account. For example, VGG networks consist of a series of convolutional layers followed by fully connected layers. If we prune channels in one convolutional layer, we must update the subsequent layers to match the new input dimensions and ensure that the network remains functional.\nIn summary, while pruning channels from the second axis of the weight tensor is straightforward, the resulting changes in activation shapes necessitate careful consideration of the network’s overall architecture. By understanding and adjusting for these changes, we can effectively apply channel-wise pruning while maintaining the integrity of the model.\n\n# Examine model architecture\nmodel\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\nThe VGG architecture features alternating blocks of convolutional layers, ReLUs, and MaxPooling operations. To apply channel-wise pruning effectively, we should focus on each pair of adjacent convolutional blocks.\nFor each pair, we adjust the output channels of the preceding convolutional block and the input channels of the subsequent convolutional block. This approach ensures that the changes made by pruning are consistent throughout the network, maintaining the correct flow of data and preserving model functionality.\n\n# Dummy example\nclass ConvBlock(nn.Module):\n    def __init__(self, in_chans, hidden_size, out_chans):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_chans, out_channels=hidden_size, kernel_size=3)\n        self.mp = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=out_chans, kernel_size=3)\n    \n    def forward(self, x):\n        return self.conv2(self.mp(self.conv1(x)))\n\nin_chans = 16\nhidden_size = 32\nout_chans = 64\nimg_size = 28\n\nx = torch.randn(in_chans, img_size, img_size)\nblock = ConvBlock(in_chans, hidden_size, out_chans)\n\nout = block(x)\nblock_orig_numparams = get_num_parameters(block)\nprint(f\"Number of parameters in block: {block_orig_numparams}\")\nprint(f\"Output shape: {out.shape}\")\n\nNumber of parameters in block: 23136\nOutput shape: torch.Size([64, 11, 11])\n\n\n\ndef get_num_channels_to_keep(channels: int, prune_ratio: float) -&gt; int:\n    \"\"\"\n    A function to calculate the number of layers to PRESERVE after pruning\n    \"\"\"\n    return int(round((1-prune_ratio)*channels))\n\n# Get the weights from the block\nconv1 = block.conv1\nmp = block.mp\nconv2 = block.conv2\n\n# Start pruning the channels\nprune_ratio = 0.6\noriginal_channels = conv1.out_channels\nn_keep = get_num_channels_to_keep(original_channels, prune_ratio)\n\n# 1. Prune the output channels of the first convolution layer\nwith torch.no_grad():\n    conv1.weight = nn.Parameter(\n        conv1.weight.detach()[:n_keep, ...]\n    )\n    # Adjust the bias as well, if it exists\n    if conv1.bias is not None:\n        conv1.bias = nn.Parameter(\n            conv1.bias.detach()[:n_keep]\n        )\n\n# 2. Prune the affected input channels of the next convolution\nwith torch.no_grad():\n    conv2.weight = nn.Parameter(\n        conv2.weight.detach()[:, :n_keep, ...]\n    )\n    # Bias does not need to be adjusted for this layer\n\n\nprint(f\"Number of parameters in block (post-pruning): {get_num_parameters(block)}\")\nprint(f\"Output shape: {out.shape}\")\n\nNumber of parameters in block (post-pruning): 9437\nOutput shape: torch.Size([64, 11, 11])\n\n\nAfter experimenting with a dummy example of pruning input and output channels within a convolutional block, we’ve observed a reduction in the number of parameters while maintaining the same output shape. However, it’s important to note that the number of parameters remaining is not precisely the product of the prune ratio and the total number of parameters, as we’re only modifying the channels and not the entire network.\nTo streamline this process, we’ll define a function that automates channel-wise pruning for the entire model. This function will handle adjusting the number of input and output channels for each convolutional layer, ensuring that the pruned network remains consistent and functional. This automated approach will simplify the pruning procedure and facilitate efficient model compression.\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n\n@torch.no_grad()\ndef channel_prune(model: nn.Module,\n                  prune_ratio: Union[List, float]) -&gt; nn.Module:\n    \"\"\"\n    Apply channel pruning to each convolutional layer in the backbone.\n\n    :param model: The model to be pruned\n    :param prune_ratio: Either a single float for uniform pruning across \n                        layers or a list of floats specifying per-layer pruning rates.\n\n    :return pruned_model: The model with pruned channels\n    \"\"\"\n    assert isinstance(prune_ratio, (float, list))\n\n    conv_layers = [m for m in model.features if isinstance(m, nn.Conv2d)]\n    n_conv = len(conv_layers)\n    \n    # The ratio affects the first conv's input channels, and the next one's out channels \n    if isinstance(prune_ratio, float):\n        prune_ratio = [prune_ratio] * (n_conv - 1)\n    else:\n        assert len(prune_ratio) == n_conv - 1, \"prune_ratio list length must be one less than the number of Conv2d layers.\"\n\n    # Create a deepcopy so we don't modify the original\n    pruned_model = copy.deepcopy(model)\n    conv_layers = [m for m in pruned_model.features if isinstance(m, nn.Conv2d)]\n\n    # Apply channel pruning to each pair of consecutive convolutional layers\n    for i, ratio in enumerate(prune_ratio):\n        prev_conv = conv_layers[i]\n        next_conv = conv_layers[i + 1]\n        prev_channels = prev_conv.out_channels\n        n_keep = get_num_channels_to_keep(prev_channels, ratio)\n\n        with torch.no_grad():\n            # Prune the output channels of the previous convolution\n            prev_conv.weight = nn.Parameter(prev_conv.weight.detach()[:n_keep, ...])\n            if prev_conv.bias is not None:\n                prev_conv.bias = nn.Parameter(prev_conv.bias.detach()[:n_keep])\n\n            # Prune the input channels of the next convolution\n            next_conv.weight = nn.Parameter(next_conv.weight.detach()[:, :n_keep, ...])\n\n    print(\"Channel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\")\n    return pruned_model\n\n\n# Prune the model without considering the channel importances\npruned_model_naive = channel_prune(model, 0.4)\npruned_model_size_mb = get_model_size(pruned_model_naive) / MB\n\nprint(f\"Original model size: {orig_model_size_mb:.2f} MB\")\nprint(f\"Pruned model size: {pruned_model_size_mb:.2f} MB\")\n\nChannel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\nOriginal model size: 527.79 MB\nPruned model size: 494.03 MB\n\n\n\n# Evaluate the model after this crude pruning\n_, acc = eval_step(pruned_model_naive, test_dl, criterion, device)\n\nprint(f\"Accuracy after naive pruning: {acc:.2f}\")\n\n                                                             \n\n\nAccuracy after naive pruning: 0.10"
  },
  {
    "objectID": "blog/posts/2024-08-15-pruning-pt2/index.html#pruning-less-important-channels",
    "href": "blog/posts/2024-08-15-pruning-pt2/index.html#pruning-less-important-channels",
    "title": "An Introduction to Pruning (Part 2)",
    "section": "",
    "text": "When considering channel-wise pruning, it’s crucial to avoid arbitrary removal of channels, as this could lead to significant performance degradation by discarding “important” channels, as was seen above. To address this, we need to prioritize channels based on their importance.\nA practical approach to determining channel importance is to use the norms of the weight tensors as a measure. The idea is that channels with larger norms are generally more critical for the network’s performance. By calculating these norms, we can rank the channels and selectively prune the less important ones.\nThe process involves defining the importance of each channel, sorting them accordingly, and then retaining only the most important channels. We can integrate this approach into our existing pruning function, which previously kept the first n_keep channels. By incorporating channel importance into this function, we can ensure that the pruning is more strategic, preserving the channels that contribute most significantly to the network’s performance.\n\n# Grab a random convolution weight tensor to demonstrate computing channel importances\nrand_weight = model.features[2].weight\nprint(f\"Shape: {rand_weight.shape}\")\n\ndef get_input_channel_importance(weight):\n    in_channels = weight.shape[1]\n    importances = []\n    \n    # Compute the importance for each input channel\n    for i_c in range(in_channels):\n        channel_weight = weight.detach()[:, i_c] # (c_out, k, k)\n        importance = torch.norm(channel_weight) # take the Frobenius norm\n        importances.append(importance.view(1))\n    return torch.cat(importances)\n\nprint(f\"Importances of the 64 input channels:\\n{get_input_channel_importance(rand_weight)}\")\n\nShape: torch.Size([64, 64, 3, 3])\nImportances of the 64 input channels:\ntensor([1.6980, 1.7297, 2.1539, 0.9993, 1.8365, 0.8037, 1.6834, 0.9603, 1.1872,\n        1.3582, 1.1134, 1.0917, 1.4087, 0.9050, 1.7213, 0.7823, 1.1833, 1.1185,\n        1.1565, 2.2874, 1.3824, 1.4115, 1.0174, 1.3120, 1.1977, 0.9108, 0.7976,\n        0.9291, 1.1520, 1.1238, 0.9578, 0.7938, 1.4062, 1.4817, 2.5130, 1.0180,\n        1.3782, 0.9571, 0.9826, 1.3465, 1.0445, 0.8921, 1.5498, 0.7251, 1.1079,\n        0.9550, 1.3082, 1.2728, 0.9647, 0.8078, 0.7796, 2.1746, 1.1919, 2.0185,\n        0.7407, 1.4707, 1.0315, 1.8911, 2.1096, 2.2035, 0.9893, 0.7218, 0.7914,\n        1.1358], device='cuda:0')\n\n\n\n# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n\n@torch.no_grad()\ndef apply_channel_sorting(model):\n    '''\n    Sorts the channels in decreasing order of importance for the given model\n\n    :param model: Model to apply the channel sorting to\n    '''\n    # Create a deep copy of the model to avoid modifying the original\n    sorted_model = copy.deepcopy(model)\n\n    # Fetch all the convolutional layers from the backbone\n    conv_layers = [m for m in sorted_model.features if isinstance(m, nn.Conv2d)]\n\n    # Iterate through the convolutional layers and sort channels by importance\n    for i in range(len(conv_layers) - 1):\n        prev_conv = conv_layers[i]\n        next_conv = conv_layers[i + 1]\n\n        # Compute the importance of input channels for the next convolutional layer\n        importance = get_input_channel_importance(next_conv.weight)\n        sort_idx = torch.argsort(importance, descending=True)\n\n        # Sort the output channels of the previous convolutional layer\n        prev_conv.weight = nn.Parameter(torch.index_select(prev_conv.weight.detach(), 0, sort_idx))\n        if prev_conv.bias is not None:\n            prev_conv.bias = nn.Parameter(torch.index_select(prev_conv.bias.detach(), 0, sort_idx))\n\n        # Sort the input channels of the next convolutional layer\n        next_conv.weight = nn.Parameter(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n\n    return sorted_model\n\nWe directly manipulate the slices of the corresponding channels within the weight and bias tensors. By sorting channels based on their importance in decreasing order, we can identify and retain the most critical channels as per our desired pruning ratio.\nAfter sorting, we keep the first n_keep slices, which ensures that the most important channels are preserved. This method leverages our previous function for pruning, allowing us to efficiently apply the revised approach. The rearrangement of tensor slices according to channel importance ensures that the pruned network maintains its effectiveness while reducing its size.\n\nchannel_pruning_ratio = 0.4  # pruned-out ratio\n\nprint(\"Without sorting channels by importance...\")\npruned_model = channel_prune(model, channel_pruning_ratio)\n_, acc = eval_step(pruned_model, test_dl, criterion, device)\nprint(f\"Pruned model accuracy: {acc:.2f}\")\n\nprint('-'*25)\n\nprint(\"With sorting channels by importance...\")\nsorted_model = apply_channel_sorting(model)\npruned_model = channel_prune(sorted_model, channel_pruning_ratio)\n_, acc = eval_step(pruned_model, test_dl, criterion, device)\nprint(f\"Pruned model accuracy: {acc:.2f}\")\n\nWithout sorting channels by importance...\nChannel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\nPruned model accuracy: 0.10\n-------------------------\nWith sorting channels by importance...\nChannel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\nPruned model accuracy: 0.15"
  },
  {
    "objectID": "blog/posts/2024-08-15-pruning-pt2/index.html#recovering-performance-with-finetuning",
    "href": "blog/posts/2024-08-15-pruning-pt2/index.html#recovering-performance-with-finetuning",
    "title": "An Introduction to Pruning (Part 2)",
    "section": "",
    "text": "With the sorting, we observe a somewhat smaller performance decrease compared to the initial approach, though the drop is still significant. This reduction in performance is typical with channel-wise pruning because entire chunks of the model are removed, which can affect its ability to generalize.\nTo mitigate the performance loss, we again employ fine-tuning. This step is essential to allow the model to adjust to the new, pruned structure and recover some of its original performance. Fine-tuning helps recalibrate the remaining parameters and optimize the network for its reduced size.\n\n# Finetune to recover performance\nlearning_rate = 1e-4\nepochs = 3\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(pruned_model.parameters(), lr=learning_rate)\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    train_loss, train_acc = train_step(pruned_model, train_dl, criterion, optimizer, device)\n    val_loss, val_acc = eval_step(pruned_model, test_dl, criterion, device)\n    print(f\"Epoch {epoch+1}... Train Accuracy: {train_acc:.2f} | Validation Accuracy: {val_acc:.2f}\")\n\nEpochs:  33%|███▎      | 1/3 [02:14&lt;04:29, 134.74s/it]Epochs:  67%|██████▋   | 2/3 [04:32&lt;02:16, 136.25s/it]Epochs: 100%|██████████| 3/3 [06:47&lt;00:00, 135.89s/it]\n\n\nEpoch 1... Train Accuracy: 0.87 | Validation Accuracy: 0.85\nEpoch 2... Train Accuracy: 0.94 | Validation Accuracy: 0.90\nEpoch 3... Train Accuracy: 0.97 | Validation Accuracy: 0.90\n\n\n\n_, acc = eval_step(pruned_model, test_dl, criterion, device)\n\nprint(f\"Validation accuracy of original (dense) model: {orig_acc:.3f}\")\nprint(f\"Final validation accuracy of pruned model: {acc:.3f}\")\n\npruned_model_size_mb = get_model_size(pruned_model) / MB\n\nprint(f\"Original model size: {orig_model_size_mb:.2f} MB\")\nprint(f\"Pruned model size: {pruned_model_size_mb:.2f} MB\")\n\n                                                             \n\n\nValidation accuracy of original (dense) model: 0.898\nFinal validation accuracy of pruned model: 0.899\nOriginal model size: 527.79 MB\nPruned model size: 494.03 MB\n\n\n\n# Test the latency of the original and pruned models\nx = next(iter(train_dl))[0].cuda()\n\n%timeit model(x)\n%timeit pruned_model(x)\n\n53.7 ms ± 85.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n35.2 ms ± 75.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nWe can see from the outputs that the pruned model not only (virtually) matches the performance of the original model on the dataset, but is also actually smaller and takes lesser time for inference.\nOne of the notable advantages of channel-wise pruning is that it does not require special hardware to handle the zeros introduced during fine-grained pruning. Since channel-wise pruning involves removing entire channels rather than zeroing out individual connections, the pruned model becomes inherently more efficient. This structured approach results in a more compact model that requires less storage and computation, making it easier to deploy and operate without the need for hardware designed to optimize sparse matrices. Thus, channel-wise pruning not only helps reduce the model size but also simplifies the computational requirements for inference, leading to overall efficiency gains."
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "",
    "text": "Vision Transformers are a rather recent innovation in the field of Computer Vision (though in the fast-paced world of Machine Learning, they are rather old) introduced in the paper Dosovitskiy et al. (2021).\nWhat’s so special about this model is how it departs from traditional Convolutional Neural Network (CNN) type processing, and leverages ideas from Natural Language Processing - the Transformer architecture introduced in 2017 by Vaswani et al. (2023) to perform Machine Translation (an inherently language-based task).\nAt the heart of the Vision Transformer is the concept of self-attention. This mechanism allows the model to weigh the importance of different elements in the input, considering the global context of the data. In the case of NLP, self-attention helps the model understand the relationships between words in a sentence, regardless of their position. Similarly, in the Vision Transformer, self-attention enables the model to capture dependencies between different patches of an image, allowing it to learn rich and complex features that are crucial for tasks like image classification and object detection.\nThe coolest part of Transformers generally is its ubiquitous nature: with very minor changes (specifically just the Positional and Patch Embedding modules), we can adapt the original architecture to the world of Computer Vision. This means that the same lessons we can learn from this model can be applied just as easily to understanding models like GPT-2 and members of the LLaMA family.\nIn this article, we will implement a Vision Transformer from the ground up, load in weights from a pretrained checkpoint, and adapt it to a simple task of Image Classification.\nThis implementation has been inspired a lot from the timm implementation(Wightman 2019) - many of the snippets will be similar but this simplified implementation is intended to give more flexibility in terms of tinkering with the model and bringing in your own tweaks.\n\nfrom typing import Optional, Tuple, Callable, Optional, Type, Union\nfrom functools import partial\nfrom IPython.display import clear_output\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\nimport timm\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#introduction-to-vision-transformers",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#introduction-to-vision-transformers",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "",
    "text": "Vision Transformers are a rather recent innovation in the field of Computer Vision (though in the fast-paced world of Machine Learning, they are rather old) introduced in the paper Dosovitskiy et al. (2021).\nWhat’s so special about this model is how it departs from traditional Convolutional Neural Network (CNN) type processing, and leverages ideas from Natural Language Processing - the Transformer architecture introduced in 2017 by Vaswani et al. (2023) to perform Machine Translation (an inherently language-based task).\nAt the heart of the Vision Transformer is the concept of self-attention. This mechanism allows the model to weigh the importance of different elements in the input, considering the global context of the data. In the case of NLP, self-attention helps the model understand the relationships between words in a sentence, regardless of their position. Similarly, in the Vision Transformer, self-attention enables the model to capture dependencies between different patches of an image, allowing it to learn rich and complex features that are crucial for tasks like image classification and object detection.\nThe coolest part of Transformers generally is its ubiquitous nature: with very minor changes (specifically just the Positional and Patch Embedding modules), we can adapt the original architecture to the world of Computer Vision. This means that the same lessons we can learn from this model can be applied just as easily to understanding models like GPT-2 and members of the LLaMA family.\nIn this article, we will implement a Vision Transformer from the ground up, load in weights from a pretrained checkpoint, and adapt it to a simple task of Image Classification.\nThis implementation has been inspired a lot from the timm implementation(Wightman 2019) - many of the snippets will be similar but this simplified implementation is intended to give more flexibility in terms of tinkering with the model and bringing in your own tweaks.\n\nfrom typing import Optional, Tuple, Callable, Optional, Type, Union\nfrom functools import partial\nfrom IPython.display import clear_output\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\nimport timm\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#getting-our-data",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#getting-our-data",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Getting our Data",
    "text": "Getting our Data\nLet’s start off by bringing in our dataset that we’ll be using going forward.\nWe could just implement the components willy-nilly but it helps to perform the forward passes on actual tensors as we go, and understand the shapes.\nLet’s keep it simple and use the CIFAR-100 dataset; torchvision makes it very easy to use.\n\n# Define some variables pertaining to our dataset\nIMAGE_SIZE = 224\nTRAIN_TFMS = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\nTEST_TFMS = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ndef get_cifar100_dataset(root: str):\n    \n    trainset = datasets.CIFAR100(\n        root, train=True, download=True, transform=TRAIN_TFMS\n    )\n    valset = datasets.CIFAR100(\n        root, train=False, download=True, transform=TEST_TFMS\n    )\n    return trainset, valset\n\n# Create the Dataset\ntrain_ds, val_ds = get_cifar100_dataset(\"./data\")\n\n# Turn these into DataLoaders\nBATCH_SIZE = 16\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\nclear_output()\n\n# Let's visualize our data for fun\ngrid_img = torchvision.utils.make_grid(\n    tensor=next(iter(train_dl))[0], # grab a batch of images from the DataLoader\n    nrow=4\n)\nplt.imshow(grid_img.permute(1,2,0)) # move the channel axis to the end\nplt.axis(False)\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n\n\n\n\n\n\n\n\n\n\n# Get an instance from the dataset - returns a tuple of (input, ground truth)\nx, y = train_ds[0]\nprint(x.shape)\n\ntorch.Size([3, 224, 224])\n\n\nSome remarks about the above cells:\n\nThe IMAGE_SIZE being \\(224\\) specifically is because we will later load in a Vision Transformer checkpoint that was pretrained on images that were \\((3, 224, 224)\\) in shape.\nThe Resize and CenterCrop combination here is a common technique for processing images into a specific size.\nThe Normalize transform’s values for the mean and standard deviation looks rather strange, but it again follows what the checkpoint we will load in later was trained on. This is rather irritating since the statistics from the ImageNet dataset are very different, so this feels a bit of an anomaly in some ways."
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#embedding-an-input-image",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#embedding-an-input-image",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Embedding an Input Image",
    "text": "Embedding an Input Image\nTo leverage the transformer architecture for images, we first need to convert an image into a format that the model can process, similar to how words are tokenized and embedded in NLP tasks. This process involves dividing the image into smaller, non-overlapping patches and then embedding these patches into vectors—a step analogous to generating token embeddings from words.\nImagine an image as a grid of pixels, where each pixel carries information about color and intensity. Instead of processing the entire image at once, the Vision Transformer splits the image into fixed-size patches, treating each patch as a “token” in the sequence. For instance, an image of size \\((224,224)\\) pixels could be divided into patches of size \\((16,16)\\), resulting in a grid of \\((14 \\times 14)\\) patches if each patch is also \\((16,16)\\) pixels. This transformation effectively turns a 2D image into a 1D sequence of patches, where each patch contains a small portion of the image’s data.\nOnce the image is divided into patches, the next step is to embed each patch into a vector space that the transformer can work with. This is where convolutional layers come into play. By applying a convolutional layer with an appropriate kernel size and stride, we can extract features from each patch and represent these features as embedding vectors. Each embedding vector corresponds to a specific patch and captures its local information—such as edges, textures, and colors—while also compressing the data into a more manageable form for the transformer.\nThis patch embedding process is akin to the embedding of words in NLP. Just as words are embedded into vectors that encapsulate their semantic meaning, image patches are embedded into vectors that represent visual features. These embeddings are then fed into the transformer model, which applies self-attention to learn relationships and dependencies between different parts of the image. This enables the Vision Transformer to understand the global structure of the image, recognizing patterns and objects in a manner similar to how it understands sentences in language tasks.\nTLDR: - The Vision Transformer can treat an image as a sequence of fixed-size patches.\n\nEach patch is converted into a feature vector, and this is easily done using convolutional layers.\n\n\nPATCH_SIZE = 16\n\n# Let's explore how we end up with 14x14 patches with the hyperparameters defined so far\nnumber_of_patches = int((IMAGE_SIZE**2) / PATCH_SIZE**2)\nprint(f\"Using {PATCH_SIZE=}, we get {number_of_patches=} for each channel.\")\nprint(f\"This is what we expected as {14*14=}.\")\n\nUsing PATCH_SIZE=16, we get number_of_patches=196 for each channel.\nThis is what we expected as 14*14=196.\n\n\n\n# Now if we consider the output as a long sequence of patches, we can compute the expected output shape\nprint(f\"Original input shape: {x.shape}\")\n\npatchified = x.contiguous().view(number_of_patches, -1)\nprint(f\"Transformed input shape: {patchified.shape}\")\n\nOriginal input shape: torch.Size([3, 224, 224])\nTransformed input shape: torch.Size([196, 768])\n\n\n\nUsing Convolutions to Generate Patch Embeddings\nRecall that to convert our image into a sequence of patch embeddings, we need to:\n\nGenerate fixed-size patches from the image.\nEmbed these patches into a vector space.\n\nWe can achieve both of these steps in one go using nn.Conv2d.\nIf the convolution operation consists of a kernel of size \\((k, k)\\) and a stride of \\(k\\), it effectively breaks the input image into non-overlapping patches of size \\(k \\times k\\). The kernel, also known as the filter, slides across the image, covering different sections, or patches, of the input. At each position, the kernel performs a dot product between the filter weights and the corresponding input pixels, followed by summing these products and applying an activation function. The output from each position becomes an element in the resulting feature map.\nIn this context, the filter’s role is twofold. First, it serves as a mechanism for slicing the image into smaller patches, with each patch being a subset of the image’s pixel data. Second, the convolution operation inherently compresses and transforms these patches into a set of feature maps, which can be interpreted as the patch embeddings. By adjusting the kernel size and stride, we control the size of the patches and the level of overlap between them.\nFor example, using a kernel size of \\(k = 16\\) and stride \\(s = 16\\) on an image of size \\(224 \\times 224\\) will produce a grid of \\(14 \\times 14\\) patches, each of size \\(16 \\times 16\\). The convolutional layer outputs a feature map for each filter used, where each feature map represents a different aspect of the image’s data, such as edges, textures, or colors. The depth of the feature map, determined by the number of filters, defines the dimension of the patch embedding vectors.\nThus, using nn.Conv2d, we efficiently transform the input image into a sequence of patch embeddings, with each embedding vector encapsulating the salient features of its corresponding image patch.\n\n# Create the patchifier\npatchifier = nn.Conv2d(in_channels=3,\n                       out_channels=768, # as we computed above\n                       kernel_size=PATCH_SIZE,\n                       stride=PATCH_SIZE,\n                       padding=0                       \n)\n\n# Transform a batch of inputs to see how it works\nx, _ = next(iter(train_dl))\nout = patchifier(x)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Patchified shape: {out.shape}\")\n\nInput shape: torch.Size([16, 3, 224, 224])\nPatchified shape: torch.Size([16, 768, 14, 14])\n\n\nQuick note about the shape:\n\\[(16, 768, 14, 14)\\] \\[\\text{(batch size, embedding dim, number of patches horizontally, number of patches vertically)}\\]\nSince we want to treat this as a sequence, i.e. losing the 2D structure, we can simply flatten this along the last two axes.\nWe will also transpose the tensor so that we have the number of channels/features at the end, this is just convention.\n\npatch_emb = out.flatten(start_dim=2).transpose(1, 2) # NCHW -&gt; NLC\nprint(f\"Final shape: {patch_emb.shape}\")\n\nFinal shape: torch.Size([16, 196, 768])\n\n\nBefore moving forward, another important point to note is how we will incorporate a “CLS token” or “classification token” later on for our task of image classification.\nThis is a technique borrowed from BERT (Devlin et al. 2019), where you have a learnable embedding meant to represent the entire input sequence. In the context of the Vision Transformer, the CLS token is a special token added to the sequence of patch embeddings. It serves as a summary or a representation of the entire image. The model learns this token’s embedding along with the patch embeddings during training.\nAt the end of the transformer layers, the CLS token’s final embedding is used as the input to a classification head, typically a fully connected layer, to predict the class label. This approach allows the model to consider the entire image’s context when making a classification decision, leveraging the self-attention mechanism to gather information from all patches into this single, informative vector.\n\n\n\n\n# Quick demonstration of prepending a learnable embedding to this activation (along the \"patch length\" axis)\ncls_token = nn.Parameter(\n    torch.zeros(1, 1, 768) # channels-last\n)\n\ntoks = torch.cat([\n    cls_token.expand(BATCH_SIZE, -1, -1), # have to expand out the batch axis\n    patch_emb,\n], dim=1)\n\nprint(f\"Final shape of embeddings with the CLS token: {toks.shape}\")\n\nFinal shape of embeddings with the CLS token: torch.Size([16, 197, 768])\n\n\n\n\nExplicitly adding in positional information\nSelf-attention, while powerful, is inherently permutation invariant. This means that it does not take into account the order of the patches, treating them as a set rather than a sequence. However, in vision tasks, the spatial arrangement of patches is crucial for understanding the structure and relationships within an image.\nTo address this, we introduce positional encodings or embeddings. These are additional vectors added to the patch embeddings to inject information about the relative or absolute position of each patch in the image. In our implementation, we’ll use a set of learnable weights for these positional encodings, allowing the model to learn the most effective way to incorporate positional information during training.\nBy combining these positional encodings with the patch embeddings, we create a richer input representation that not only captures the visual content of the patches but also their spatial arrangement. This enriched input is then fed into the next main component of the transformer, enabling the model to leverage both the content and the position of patches for more accurate understanding and processing of the image.\nIn the cell below, note how we use nn.Parameter to intialize the tensor and the shape following the patch embeddings. We rely on broadcasting to create copies over the batch axis since we do not want there to be different positional encodings for elements in different batches (that would make no sense).\n\n# Initialize a randomly initialized set of positional encodings\npos_embed = nn.Parameter(torch.randn(1, toks.shape[1], toks.shape[2]) * 0.02) # this factor is from the timm implementation\nx = toks + pos_embed\n\nprint(f\"Final shape of input: {x.shape}\")\n\nFinal shape of input: torch.Size([16, 197, 768])"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#multi-head-self-attention",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#multi-head-self-attention",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Multi-Head Self Attention",
    "text": "Multi-Head Self Attention\nSelf-attention is a fundamental mechanism in the Vision Transformer that enables patches, or tokens, to communicate with one another across the entire image. This mechanism allows each patch to consider the information from all other patches, effectively sharing and enriching the context of each patch’s representation. In the scene of computer vision, this means that the model can capture relationships and dependencies between different parts of the image, such as identifying that certain shapes or colors in one part of the image may relate to features in another part. This global interaction helps the model build a more comprehensive understanding of the image, crucial for tasks like image classification.\nThe Self-Attention mechanism can be expressed with the following expression from Vaswani et al. (2023): \\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\\]\nHere, the components are:\n\nQueries (\\(Q\\)): These represent the specific aspects or “questions” that each patch wants to know about other patches.\nKeys (\\(K\\)): These act like tags or “keywords” that help identify relevant information in the patches.\nValues (\\(V\\)): These are the actual data or “answers” that the patches contain.\n\nTo explain with an analogy, think of queries, keys, and values as parts of a search engine system:\n\nQueries are like the search terms you enter. They specify what you’re looking for.\nKeys are the tags or metadata associated with each document or webpage, helping to determine if a document matches a search query.\nValues are the actual content of the documents that are retrieved and displayed.\n\nIn the Vision Transformer, these components are extracted from the patch embeddings using learned linear transformations. The three matrices are computed as follows: \\[Q = W^Q X\\] \\[K = W^K X\\] \\[V = W^V X\\]\nwhere \\(W^Q, W^K, W^V\\) are learnable weight matrices, and \\(X\\) is the tensor corresponding to the input embeddings.\n\n# Define variables before moving further\nembed_dim = 768\nB, P, C = x.shape\n\n# We can use nn.Linear layers (without the bias) to perform these computations\nquery = nn.Linear(embed_dim, embed_dim, bias=False)\nkey = nn.Linear(embed_dim, embed_dim, bias=False)\nvalue = nn.Linear(embed_dim, embed_dim, bias=False)\n\n# Get the projections, these are the Q, K, V matrices in the equation above\nq = query(x)\nk = key(x)\nv = value(x)\n\n# Get the shapes\nq.shape, k.shape, v.shape\n\n(torch.Size([16, 197, 768]),\n torch.Size([16, 197, 768]),\n torch.Size([16, 197, 768]))\n\n\n\nThe Multiple “Head(s)”\nIn the context of self-attention, a “head” refers to an individual attention mechanism within the multi-head attention framework. Each head operates independently, learning different aspects of the relationships between patches or tokens. By having multiple heads, the model can capture a diverse range of interactions and dependencies, enhancing its ability to understand complex patterns in the data.\nEach head has its own set of learnable parameters for queries, keys, and values. The use of multiple heads allows the model to focus on different types of relationships or features in parallel. For instance, one head might learn to focus on spatial locality, while another might capture more global interactions.\nNote how the in_features and out_features are the same for this setup. This actually makes it much easier for us to work with multiple heads in one go: we can partition the projections from these matrices by introducing a “head axis”, this would then let us perform computations with H of these vectors of size C//H in parallel.\n\nH = 12 # hyperparam from timm's implementation\nhead_size = C // H\n\n# MHSA is usually implemented as such\nq = query(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\nk = key(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\nv = value(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\n\nq.shape, k.shape, v.shape\n\n(torch.Size([16, 12, 197, 64]),\n torch.Size([16, 12, 197, 64]),\n torch.Size([16, 12, 197, 64]))\n\n\nWith the projections ready, we can finally implement the meat of the component: the equation.\n\\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\\]\nHere’s a few things to note: 1. \\(Q\\) and \\(K\\) are 4D tensors so the notion of a “tranpose” sounds rather strange. Note however, that the output we want from this operation is an Attention Map - a map that lays out the affinities between each and every pair of patches in the input tensor. This means what we really want is a (B, H, P, P) tensor - this means all we have to do is swap the last two axes of \\(K\\) and follow the rules of Batched Matrix Multiplication.\n\nThe scale factor \\(\\sqrt{d_k}\\) is helpful for stable training. Without this, the activations would blow up exactly on the order of \\(d_k\\), and this can lead to unstable gradients - so we scale everything down by this amount to end up with activations of unit-variance.\nThe Softmax here is applied on a row axis - i.e. the rows of the 2D slice must contain values that sum up to 1. This is important to note since we consider the dot product of the row of the query matrix with the columns of the transpose of the key (so if you ask a specific question following the analogy above, you’d want the answers to be weighed according to the question, not all questions weighed according to a single answer).\n\n\n# Let's implement this all in one go\n\nH = 12\nhead_size = C // H\n\nprint(f\"{x.shape=}\")\n\n# MHSA is usually implemented as such\nq = query(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\nk = key(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\nv = value(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\n\nscale = q.size(-1) # the head size\n\nq = q*scale\n\n# Take the dot product\nattn = q @ k.transpose(-2, -1) # (B, H, P, head_size) @ (B, H, head_size, P) --&gt; (B, H, P, P)\nprint(f\"{attn.shape=}\")\n\n# Softmax along the final axis, i.e. rows\nattn = attn.softmax(dim=-1)\nout = attn @ v # (B, H, P, P) @ (B, H, P, head_size) --&gt; (B, H, P, head_size)\nprint(f\"{out.shape=}\")\n\n# Collapse the head dimension to get back \nsa_out = out.transpose(1, 2).contiguous().view(B, P, C)\nprint(f\"{sa_out.shape=}\")\n\nx.shape=torch.Size([16, 197, 768])\nattn.shape=torch.Size([16, 12, 197, 197])\nout.shape=torch.Size([16, 12, 197, 64])\nsa_out.shape=torch.Size([16, 197, 768])\n\n\nThe shapes can help give us a very interesting interpretation of what Self-Attention really does: if the input and output shapes are exactly the same (i.e. [16, 197, 768]) but with so much of processing with learnable parameters in the middle, then we can think of this module as simply refining the representations.\nTo expand on this, your input is provided as a sequence of embeddings (order matters here) - the Self-Attention operation allows the elements to communicate and share information with one another, enriching each other with context, so that the final consolidated output is simply an enriched version of the input. It also helps that having the same shape allows you to stack these components on top of one another very easily, as we will see later.\n\n\nBuilding MHSA\nLet’s put all of these operations, with a few other bells and whistles into a single nn.Module class.\nNote how we add two things here: 1. The proj component: this is a projection back into the same vector space, so it again acts like simply refining what you already have. Andrej Karpathy calls this as thinking on what you have just computed from the Self-Attention operation.\n\nThe proj_drop component: This is a form of Dropout which acts as a regularizing mechanism for the Vision Transformer. This becomes very important since they are prone to overfit on simplistic datasets, so this and attn_drop (functionally the same thing) can help mitigate this heinous activity.\n\n\nclass MHSA(nn.Module):\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int = 8,\n            qkv_bias: bool = False,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.\n    ) -&gt; None:\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, P, C = x.shape\n        H = self.num_heads\n        q = self.q(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        k = self.k(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        v = self.v(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        \n        q = q * self.scale\n\n        attn = q @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, P, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\n# Perform a forward pass on a dummy input\nmhsa = MHSA(dim=768,\n            num_heads=12,\n            qkv_bias=True,\n            )\n\n# Create sample input tensor\nB, P, C = 16, 196, 768\nx = torch.randn(B, P, C)\n\n# Pass the input through the MHSA layer to trigger the hook\noutput = mhsa(x)\nprint(f\"{x.shape=} --&gt; {output.shape=}\")\ndel mhsa\n\nx.shape=torch.Size([16, 196, 768]) --&gt; output.shape=torch.Size([16, 196, 768])"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#the-encoder-block",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#the-encoder-block",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "The “Encoder Block”",
    "text": "The “Encoder Block”\nLet’s go back to the main illustration in the original paper:\n\n\n\nThe grayed out block on the right side represents a single “Encoder Block”.\nThe Encoder Block is a fundamental component of the Vision Transformer, inheriting its architecture from the original Transformer model used in natural language processing. Each Encoder Block is designed to process and refine the input sequence—here, the sequence of patch embeddings enriched with positional encodings.\nThe Encoder Block consists of two main sub-layers:\nMulti-Head Self-Attention Mechanism: This layer allows the model to focus on different parts of the input sequence simultaneously, capturing various relationships and dependencies among the patches. As discussed earlier, multiple heads enable the model to learn different aspects of the data, providing a comprehensive representation.\nFeed-Forward Neural Network (FFN): Following the self-attention layer, a position-wise feed-forward neural network processes the output. This network typically consists of two linear transformations separated by a non-linear activation function, such as GeLU. It acts on each position independently and helps in further transforming and refining the representation learned from the self-attention layer.\nTo ensure the effective flow of gradients during training and to preserve the original input information, each sub-layer is equipped with Skip Connections (also known as Residual Connections). These connections add the input of each sub-layer to its output, forming a residual path that facilitates better gradient propagation and helps prevent the vanishing gradient problem. Mathematically, this can be expressed as:\n\\[\\text{Output} = \\text{Layernorm}(x + \\text{SubLayer}(x))\\]\nIn the equation above, \\(x\\) represents the input to the sub-layer, and the sum \\(x + \\text{SubLayer}(x)\\) forms the residual connection. This is a very old idea in Deep Learning, going back to 2015 with ResNets (He et al. 2015), and can perhaps be better understood with the following illustration:\n\n\n\nThe output is then normalized using Layer Normalization (LayerNorm), which stabilizes the training by normalizing the summed outputs across each patch, ensuring that the model’s activations are within a stable range. LayerNorm adjusts the output by scaling and shifting, allowing the model to maintain useful information while preventing excessive internal covariate shifts.\nThe Encoder Block’s design, with its combination of self-attention, feed-forward neural networks, skip connections, and layer normalization, enables the Vision Transformer to learn rich, hierarchical representations of the input data. This structure is repeated multiple times in the model, each block building upon the representations learned by the previous one, gradually refining the understanding of the input image.\nLet’s go ahead and implement this directly.\n\nclass Mlp(nn.Module):\n    def __init__(self, \n                 in_features: int,\n                 hidden_features: int,\n                 drop: float,\n                 norm_layer: nn.Module = None) -&gt; None:\n        super().__init__()\n\n        # There are two Linear layers in the conventional implementation\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, in_features)\n\n        # Dropout is used twice, once after each linear layer\n        self.drop1 = nn.Dropout(drop)\n        self.drop2 = nn.Dropout(drop)\n\n        # The paper uses the GeLU activation function\n        self.act = nn.GELU()\n\n        # Optional normalization layer (following timm's convention)\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = False,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n    ) -&gt; None:\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            drop=proj_drop,\n        )\n\n        self.attn = MHSA(\n            dim=dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n    \nblock = Block(dim=768,\n              num_heads=12)\nblock_out = block(x)\n\nprint(f\"{x.shape=} --&gt; {block_out.shape=}\")\ndel block\n\nx.shape=torch.Size([16, 196, 768]) --&gt; block_out.shape=torch.Size([16, 196, 768])\n\n\nA wonderful observation here is again that the input and output shapes are exactly the same!\nThis means we can again fall back on the interpretation that each Block (on top of each MHSA module) is simply refining and embedding rich context into whatever input is fed into it.\nPlus, having the same shape as the input allows us to stack these encoders nicely on top of each other without much thought or care."
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#putting-it-all-together",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#putting-it-all-together",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Putting it all together",
    "text": "Putting it all together\nNow with everything we’ve learned so far, let’s make one final class that aggregates everything.\nRecall what we had to do with\n\nThe Patch Embeddings to represent our image as a sequence and let Self-Attention link parts of it with one another,\nThe Positional Encodings/Embeddings to move past the permutation invariant nature of Self-Attention and embed information regarding the position of each patch into the mix,\nThe CLS Token to let the model have an overall representation of the entire image which would provide a means of performing classification,\nThe Multi-Head Self-Attention class (MHSA) to let the patches communicate and share information with one another in the hopes of enriching the representations,\nThe Block class (Block) to be able to string together the computations performed by the Self-Attention, Feedforward, and Layer Normalization modules.\n\nNow we put it all together.\n\nclass PatchEmbed(nn.Module):\n    def __init__(self,\n                 img_size: int,\n                 patch_size: int,\n                 in_chans: int,\n                 embed_dim: int,\n                 bias: bool = True,\n                 norm_layer: Optional[Callable] = None) -&gt; None:\n        super().__init__()\n        \n        self.img_size = img_size\n        self.patch_size = patch_size\n\n        # self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n        self.grid_size = (self.img_size // self.patch_size, ) * 2\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n        self.proj = nn.Conv2d(in_chans, \n                              embed_dim, \n                              kernel_size=patch_size, \n                              stride=patch_size, \n                              bias=bias, \n                              padding=0)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1,2) # flatten and transpose: BCHW -&gt; BLC\n        return x\n\n\nclass VisionTransformer(nn.Module):\n\n    def __init__(\n            self,\n            img_size: Union[int, Tuple[int, int]] = 224,\n            patch_size: Union[int, Tuple[int, int]] = 16,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            embed_dim: int = 768,\n            depth: int = 12,\n            num_heads: int = 12,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            drop_rate: float = 0.,\n            pos_drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of image input channels.\n            num_classes: Number of classes for classification heads\n            embed_dim: Transformer embedding dimension.\n            depth: Depth of transformer.\n            num_heads: Number of attention heads.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: Enable bias for qkv projections if True.\n            drop_rate: Head dropout rate.\n            pos_drop_rate: Position embedding dropout rate.\n            attn_drop_rate: Attention dropout rate.\n        \"\"\"\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_features = self.head_hidden_size = self.embed_dim = embed_dim  # for consistency with other models\n\n        # Define the Patch Embedding module - note this does not include the CLS token yet\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            bias=True,\n        )\n        num_patches = self.patch_embed.num_patches\n        embed_len = num_patches + 1 # don't forget we need to incorporate the CLS token\n\n        # Define the CLS token, the Positional Encodings/Embeddings, and a Dropout for the Positional information\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        # Define LayerNorms for before and after the Encoder block processing\n        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n        self.norm_pre = norm_layer(embed_dim)\n        self.norm = norm_layer(embed_dim)\n\n        # Initialize the blocks\n        self.blocks = nn.Sequential(*[\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate\n            )\n            for i in range(depth)])\n        \n        self.feature_info = [\n            dict(module=f'blocks.{i}', num_chs=embed_dim) for i in range(depth)\n        ]\n\n        # Classifier Head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes)\n\n    def forward_features(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \n        # Get the Patch Embeddings\n        x = self.patch_embed(x)\n\n        # Prepend the CLS token, THEN add in the positional information\n        x = torch.cat([\n            self.cls_token.expand(x.shape[0], -1, -1), # have to expand out the batch axis\n            x,\n        ], dim=1)\n        x = x + self.pos_embed\n\n        # Pass through the LayerNorms, the Encoder Blocks, then the final Norm\n        x = self.norm_pre(x)\n        x = self.blocks(x)\n        x = self.norm(x)\n\n        return x\n\n    def forward_head(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \n        # Get the CLS token (also called \"pooling\" our logits)\n        x = x[:, 0]\n\n        # Pass through the Dropout then the classification head\n        x = self.head_drop(x)\n        x = self.head(x)\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n    \nmodel = VisionTransformer()\n\n\nx, _ = next(iter(train_dl))\nout = model(x)\nprint(out.shape)\n\ntorch.Size([16, 1000])"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#loading-in-pretrained-weights",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#loading-in-pretrained-weights",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Loading in Pretrained Weights",
    "text": "Loading in Pretrained Weights\nNow that we’ve built our simplistic version of a Vision Transformer, let’s load in the parameters from a pretrained checkpoint, specifically from the Base variant of the timm library.\nThe procedure is simple: load in the state_dict from the pretrained model, and match with the corresponding parameters in your implementation. Since we were careful to stick with a naming convention similar to the implementation already in timm, we’ve made our job simpler already.\nWe will do this in a slightly unconventional way - we will define methods for loading in the parameters in each module. This makes our code arguably more readable, but requires us to rewrite the module again for the purposes of this article (last time, promise!).\nThis form of matching and copying has been inspired from Sebastian Raschka’s book on “LLMs from Scratch” (Raschka 2024), specifically this notebook.\n\ndef assign_check(left: torch.Tensor, right: torch.Tensor):\n    '''Utility for checking and creating parameters to be used in loading a pretrained checkpoint'''\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch: Left: {left.shape}, Right: {right.shape}\")\n    return torch.nn.Parameter(right.clone().detach())\n\nclass PatchEmbed(nn.Module):\n    def __init__(self,\n                 img_size: int,\n                 patch_size: int,\n                 in_chans: int,\n                 embed_dim: int,\n                 bias: bool = True,\n                 norm_layer: Optional[Callable] = None) -&gt; None:\n        super().__init__()\n        \n        self.img_size = img_size\n        self.patch_size = patch_size\n\n        # self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n        self.grid_size = (self.img_size // self.patch_size, ) * 2\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n        self.proj = nn.Conv2d(in_chans, \n                              embed_dim, \n                              kernel_size=patch_size, \n                              stride=patch_size, \n                              bias=bias, \n                              padding=0)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1,2) # flatten and transpose: BCHW -&gt; BLC\n        return x\n    \nclass MHSA(nn.Module):\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int = 8,\n            qkv_bias: bool = False,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.\n    ) -&gt; None:\n        super().__init__()\n        \n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, P, C = x.shape\n        H = self.num_heads\n        q = self.q(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        k = self.k(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        v = self.v(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        \n        q = q * self.scale\n\n        attn = q @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, P, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n    \n    def att_weight_conversion(self, qkv_params):\n        '''\n        Split and convert the QKV parameters from ViT checkpoints for the GQA implementation\n        '''\n        q, k, v = torch.split(qkv_params, qkv_params.shape[0] // 3, dim=0)\n        \n        return {\n            \"q\": q,\n            \"k\": k,\n            \"v\": v\n        }\n    \n    def load_pretrained_weights(self, state_dict, block_idx):\n\n        # Load in parameters for the Query Key Value layers\n        qkv_weight = state_dict[f'blocks.{block_idx}.attn.qkv.weight']\n        qkv_bias = state_dict[f'blocks.{block_idx}.attn.qkv.bias']\n\n        wdict = self.att_weight_conversion(qkv_weight)\n        bdict = self.att_weight_conversion(qkv_bias)\n\n        self.q.weight = assign_check(self.q.weight, wdict['q'])\n        self.q.bias = assign_check(self.q.bias, bdict['q'])\n\n        self.k.weight = assign_check(self.k.weight, wdict['k'])\n        self.k.bias = assign_check(self.k.bias, bdict['k'])\n        \n        self.v.weight = assign_check(self.v.weight, wdict['v'])\n        self.v.bias = assign_check(self.v.bias, bdict['v'])\n\n        # Load in parameters for the output projection\n        self.proj.weight = assign_check(self.proj.weight, state_dict[f'blocks.{block_idx}.attn.proj.weight'])\n        self.proj.bias = assign_check(self.proj.bias, state_dict[f'blocks.{block_idx}.attn.proj.bias'])\n\nclass Mlp(nn.Module):\n    def __init__(self, \n                 in_features: int,\n                 hidden_features: int,\n                 drop: float,\n                 norm_layer: nn.Module = None) -&gt; None:\n        super().__init__()\n\n        # There are two Linear layers in the conventional implementation\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, in_features)\n\n        # Dropout is used twice, once after each linear layer\n        self.drop1 = nn.Dropout(drop)\n        self.drop2 = nn.Dropout(drop)\n\n        # The paper uses the GeLU activation function\n        self.act = nn.GELU()\n\n        # Optional normalization layer (following timm's convention)\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = False,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n    ) -&gt; None:\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            drop=proj_drop,\n        )\n\n        self.attn = MHSA(\n            dim=dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n    \n    def load_pretrained_weights(self, state_dict, block_idx):\n\n        self.attn.load_pretrained_weights(state_dict, block_idx)\n\n        self.norm1.weight = assign_check(self.norm1.weight, state_dict[f'blocks.{block_idx}.norm1.weight'])\n        self.norm1.bias = assign_check(self.norm1.bias, state_dict[f'blocks.{block_idx}.norm1.bias'])\n        \n        self.norm2.weight = assign_check(self.norm2.weight, state_dict[f'blocks.{block_idx}.norm2.weight'])\n        self.norm2.bias = assign_check(self.norm2.bias, state_dict[f'blocks.{block_idx}.norm2.bias'])\n\n        self.mlp.fc1.weight = assign_check(self.mlp.fc1.weight, state_dict[f'blocks.{block_idx}.mlp.fc1.weight'])\n        self.mlp.fc1.bias = assign_check(self.mlp.fc1.bias, state_dict[f'blocks.{block_idx}.mlp.fc1.bias'])\n        self.mlp.fc2.weight = assign_check(self.mlp.fc2.weight, state_dict[f'blocks.{block_idx}.mlp.fc2.weight'])\n        self.mlp.fc2.bias = assign_check(self.mlp.fc2.bias, state_dict[f'blocks.{block_idx}.mlp.fc2.bias'])\n    \nclass VisionTransformer(nn.Module):\n\n    def __init__(\n            self,\n            img_size: Union[int, Tuple[int, int]] = 224,\n            patch_size: Union[int, Tuple[int, int]] = 16,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            embed_dim: int = 768,\n            depth: int = 12,\n            num_heads: int = 12,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            drop_rate: float = 0.,\n            pos_drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of image input channels.\n            num_classes: Number of classes for classification heads\n            embed_dim: Transformer embedding dimension.\n            depth: Depth of transformer.\n            num_heads: Number of attention heads.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: Enable bias for qkv projections if True.\n            drop_rate: Head dropout rate.\n            pos_drop_rate: Position embedding dropout rate.\n            attn_drop_rate: Attention dropout rate.\n        \"\"\"\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_features = self.head_hidden_size = self.embed_dim = embed_dim  # for consistency with other models\n\n        # Define the Patch Embedding module - note this does not include the CLS token yet\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            bias=True,\n        )\n        num_patches = self.patch_embed.num_patches\n        embed_len = num_patches + 1 # don't forget we need to incorporate the CLS token\n\n        # Define the CLS token, the Positional Encodings/Embeddings, and a Dropout for the Positional information\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        # Define LayerNorms for before and after the Encoder block processing\n        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n        self.norm_pre = norm_layer(embed_dim)\n        self.norm = norm_layer(embed_dim)\n\n        # Initialize the blocks\n        self.blocks = nn.Sequential(*[\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate\n            )\n            for i in range(depth)])\n        \n        self.feature_info = [\n            dict(module=f'blocks.{i}', num_chs=embed_dim) for i in range(depth)\n        ]\n\n        # Classifier Head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes)\n\n    def forward_features(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \n        # Get the Patch Embeddings\n        x = self.patch_embed(x)\n\n        # Prepend the CLS token, THEN add in the positional information\n        x = torch.cat([\n            self.cls_token.expand(x.shape[0], -1, -1), # have to expand out the batch axis\n            x,\n        ], dim=1)\n        x = x + self.pos_embed\n\n        # Pass through the LayerNorms, the Encoder Blocks, then the final Norm\n        x = self.norm_pre(x)\n        x = self.blocks(x)\n        x = self.norm(x)\n\n        return x\n\n    def forward_head(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \n        # Get the CLS token (also called \"pooling\" our logits)\n        x = x[:, 0]\n\n        # Pass through the Dropout then the classification head\n        x = self.head_drop(x)\n        x = self.head(x)\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n    \n    def load_pretrained_weights(self, state_dict):\n        \n        print(\"Loading in weights...\")\n        \n        for b, block in enumerate(self.blocks):\n            block.load_pretrained_weights(state_dict, b)\n        print(f\"Finished with {b+1} blocks...\")\n\n        self.patch_embed.proj.weight = assign_check(self.patch_embed.proj.weight, state_dict['patch_embed.proj.weight'])\n        self.patch_embed.proj.bias = assign_check(self.patch_embed.proj.bias, state_dict['patch_embed.proj.bias'])\n        self.cls_token = assign_check(self.cls_token, state_dict['cls_token'])\n        self.pos_embed = assign_check(self.pos_embed, state_dict['pos_embed'])\n\n        print(\"Success!\")\n\n\ndef vit_small_patch16_224(\n        num_classes: int = 10,\n        pretrained: bool = False,\n        in_chans: int = 3,\n        drop_rate: float = 0,\n        pos_drop_rate: float = 0,\n        attn_drop_rate: float = 0,\n        proj_drop_rate: float = 0.\n        ):\n    \n    model = VisionTransformer(\n        img_size=224,\n        patch_size=16,\n        in_chans=in_chans,\n        num_classes=num_classes,\n        embed_dim=384,\n        num_heads=6,\n        depth=12,\n        drop_rate=drop_rate,\n        pos_drop_rate=pos_drop_rate,\n        attn_drop_rate=attn_drop_rate,\n        proj_drop_rate=proj_drop_rate\n    )\n\n    if pretrained:\n        ckpt = 'vit_small_patch16_224'\n        if in_chans != 3:\n            raise ValueError(f\"Cannot load in checkpoint with {in_chans=}\")\n        print(f'Using checkpoint {ckpt}...')\n        hf_model = timm.create_model(ckpt, pretrained=True)\n        model.load_pretrained_weights(hf_model.state_dict())\n    \n    return model\n\n\n# Load in our model\nmodel = vit_small_patch16_224(num_classes=100,\n                             pretrained=True)\n\nUsing checkpoint vit_small_patch16_224...\nLoading in weights...\nFinished with 12 blocks...\nSuccess!"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#finetuning-our-vision-transformer",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#finetuning-our-vision-transformer",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Finetuning our Vision Transformer",
    "text": "Finetuning our Vision Transformer\nNow with everything in place, we can finally move on to actually finetuning our model.\nWe will use the CIFAR100 dataset since it’s (arguably) rather difficult to perform well on: there’s only 600 images for each of its 100 classes, so training a model from scratch would be very difficult to get to perform well on this dataset. This serves as a nice pressure test for our model, provided we did our loading of the pretrained checkpoint properly.\nTo spice things up, we’ll add in random augmentations during the training process.\n\n# Prepare CIFAR100\nIMAGE_SIZE = 224\nTRAIN_TFMS = transforms.Compose([\n    transforms.RandAugment(),\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\nTEST_TFMS = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ndef get_dataloader(dataset: torch.utils.data.Dataset,\n                   batch_size: int,\n                   is_train: bool,\n                   num_workers: int = 1):\n    \n    loader = DataLoader(dataset, batch_size=batch_size,\n                        shuffle=is_train, num_workers=num_workers)\n    return loader\n\ndef get_cifar100_dataset(root: str):\n\n    trainset = torchvision.datasets.CIFAR100(\n        root, train=True, download=True, transform=TRAIN_TFMS\n    )\n\n    testset = torchvision.datasets.CIFAR100(\n        root, train=False, download=True, transform=TEST_TFMS\n    )\n\n    return trainset, testset\n\ntrain, val = get_cifar100_dataset(\"./data\")\ntrain_dl = get_dataloader(train, 32, True)\nval_dl = get_dataloader(val, 32, False)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\nNow we’ll define our training and validation functions.\nIt’s always good to make your code modular.\n\ndef train_step(model, dataloader, criterion, optimizer, device):\n    '''Train for one epoch'''\n\n    model.train()\n\n    train_loss = 0.\n    train_acc = 0.\n\n    for step, (X, y) in tqdm(enumerate(dataloader), desc=\"Training\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        train_acc += ((y_pred == y).sum().item() / len(y))\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\n@torch.inference_mode()\ndef eval_step(model, dataloader, criterion, device):\n    \n    model.eval()\n\n    eval_loss = 0.\n    eval_acc = 0.\n\n    for (X, y) in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        eval_loss += loss.item()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        eval_acc += ((y_pred == y).sum().item() / len(y))\n\n    eval_loss = eval_loss / len(dataloader)\n    eval_acc = eval_acc / len(dataloader)\n    return eval_loss, eval_acc\n\nNow to actually kick off training.\nWe’ll keep things simple: - Finetune for 3 epochs - Use a learning rate of 0.0001 - Use the AdamW optimizer since this usually works well for something as annoying to train as a Vision Transformer\nTo speed up the training process, we’ll also use torch.compile since our implementation is very straightforward and will not incur any nasty graph breaks or such during the compilation process.\n\nlearning_rate = 1e-4\nepochs = 3\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)\nmodel = torch.compile(model)\n\nhistory = {k: [] for k in (\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\")}\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    train_loss, train_acc = train_step(model, train_dl, criterion, optimizer, device)\n    val_loss, val_acc = eval_step(model, val_dl, criterion, device)\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n\nEpochs: 100%|██████████| 3/3 [04:21&lt;00:00, 87.07s/it] \n\n\n\n# Plot the loss curves\nplt.figure(figsize=(6,6))\nplt.plot(history[\"train_loss\"], label=\"Train Loss\")\nplt.plot(history[\"val_loss\"], label=\"Validation Loss\")\nplt.title(\"Loss Curves\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the loss curves\nplt.figure(figsize=(6,6))\nplt.plot(history[\"train_acc\"], label=\"Train Accuracy\")\nplt.plot(history[\"val_acc\"], label=\"Validation Accuracy\")\nplt.title(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nHopefully you can appreciate what we’ve built together from start to end.\nYou can use the same type of recipe to make a BERT clone, the only real differences there would be (1) the data munging, (2) the positional encodings, and (3) the token embeddings in place of the patch embeddings. Otherwise, the same general principles would apply here and big chunks of the same code can be copied verbatim."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "An Introduction to Pruning (Part 2)\n\n\n\n\n\nExploring structured pruning with CNNs.\n\n\n\n\n\nAug 15, 2024\n\n\n7 min\n\n\n\n\n\n\n\nAn Introduction to Pruning (Part 1)\n\n\n\n\n\nIntroducing pruning, and implementing fine-grained pruning for CNNs.\n\n\n\n\n\nAug 3, 2024\n\n\n11 min\n\n\n\n\n\n\n\nImplementing a Vision Transformer from Scratch\n\n\n\n\n\nLet’s implement a Vision Transformer from scratch, load in pretrained weights, and fine-tune it.\n\n\n\n\n\nJun 13, 2024\n\n\n18 min\n\n\n\n\n\n\nNo matching items"
  }
]