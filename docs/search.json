[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Zohaib Khan",
    "section": "",
    "text": "About Me\nI’m a Research Assistant at the Lahore University of Management Sciences.\nMy goal is to be contribute towards democratizing and building efficient Machine Learning systems that empower underrepresented populations, and continuously improve upon existing technologies.\nSome of my interests include (but are not limited to)\n\nEfficient ML: Quantization, Pruning, Distillation, Neural Architecture Search, Parameter-Efficient Finetuning, TinyML\nMultimodal Representation Learning: Visual Language Models, Diffusion Models, Text-to-Speech/Speech-to-Text Models\nNatural Language Processing: LLMs for Code Understanding and Rewriting, LLM Agents, Multilingual NLP, NLP for underrepresented populations\nMiscellaneous: Mixture of Experts, Model Merging, Parallel Computing, Open-Source Software, Automatic Differentiation Engines, Optimization\n\nYou can find a lot more details about my projects, relevant experiences (industry, research, and teaching), and activities outside academia in my CV.\n\n\nExperience\n\nResearch Intern/Assistant - CSaLT at LUMS (Spring 2022 – Present)\nMachine Learning Engineer - ISSM.ai (Summer 2023)\nDeep Learning Intern - CodeSlash (Summer 2022 – Summer 2023)\n\n\n\nTeaching\n\nCS5302 Speech and Language Processing (LUMS Spring 2024) - Head TA\nCS535 Machine Learning (LUMS Fall 2023) - Head TA\nCS437 Deep Learning (LUMS Spring 2023) - TA\n\n\n\nMiscellaneous\n\nI enjoy Boxing and BJJ (I was Captain of the Boxing Team at LUMS for two years).\nI drink an unhealthy amount of black coffee.\nMy favorite movie is Good Will Hunting.\nI have five beautiful dogs.\nI know how to quit Vim."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Welcome to my Blog",
    "section": "",
    "text": "Implementing a Vision Transformer from Scratch\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "",
    "text": "Vision Transformers are a rather recent innovation in the field of Computer Vision (though in the fast-paced world of Machine Learning, they are rather old) introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\nWhat’s so special about this model is how it departs from traditional Convolutional Neural Network (CNN) type processing, and leverages ideas from Natural Language Processing - the Transformer architecture introduced in 2017 by Vaswani et al. to perform Machine Translation (an inherently language-based task).\nAt the heart of the Vision Transformer is the concept of self-attention. This mechanism allows the model to weigh the importance of different elements in the input, considering the global context of the data. In the case of NLP, self-attention helps the model understand the relationships between words in a sentence, regardless of their position. Similarly, in the Vision Transformer, self-attention enables the model to capture dependencies between different patches of an image, allowing it to learn rich and complex features that are crucial for tasks like image classification and object detection.\nThe coolest part of Transformers generally is its ubiquitous nature: with very minor changes (specifically just the Positional and Patch Embedding modules), we can adapt the original architecture to the world of Computer Vision. This means that the same lessons we can learn from this model can be applied just as easily to understanding models like GPT-2 and members of the LLaMA family.\nIn this article, we will implement a Vision Transformer from the ground up, load in weights from a pretrained checkpoint, and adapt it to a simple task of Image Classification.\nThis implementation has been inspired a lot from the timm implementation - many of the snippets will be similar but this simplified implementation is intended to give more flexibility in terms of tinkering with the model and bringing in your own tweaks.\n\nfrom typing import Optional, Tuple, Callable, Optional, Type, Union\nfrom functools import partial\nfrom IPython.display import clear_output\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\nimport timm\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#introduction-to-vision-transformers",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#introduction-to-vision-transformers",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "",
    "text": "Vision Transformers are a rather recent innovation in the field of Computer Vision (though in the fast-paced world of Machine Learning, they are rather old) introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\nWhat’s so special about this model is how it departs from traditional Convolutional Neural Network (CNN) type processing, and leverages ideas from Natural Language Processing - the Transformer architecture introduced in 2017 by Vaswani et al. to perform Machine Translation (an inherently language-based task).\nAt the heart of the Vision Transformer is the concept of self-attention. This mechanism allows the model to weigh the importance of different elements in the input, considering the global context of the data. In the case of NLP, self-attention helps the model understand the relationships between words in a sentence, regardless of their position. Similarly, in the Vision Transformer, self-attention enables the model to capture dependencies between different patches of an image, allowing it to learn rich and complex features that are crucial for tasks like image classification and object detection.\nThe coolest part of Transformers generally is its ubiquitous nature: with very minor changes (specifically just the Positional and Patch Embedding modules), we can adapt the original architecture to the world of Computer Vision. This means that the same lessons we can learn from this model can be applied just as easily to understanding models like GPT-2 and members of the LLaMA family.\nIn this article, we will implement a Vision Transformer from the ground up, load in weights from a pretrained checkpoint, and adapt it to a simple task of Image Classification.\nThis implementation has been inspired a lot from the timm implementation - many of the snippets will be similar but this simplified implementation is intended to give more flexibility in terms of tinkering with the model and bringing in your own tweaks.\n\nfrom typing import Optional, Tuple, Callable, Optional, Type, Union\nfrom functools import partial\nfrom IPython.display import clear_output\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\nimport timm\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#getting-our-data",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#getting-our-data",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Getting our Data",
    "text": "Getting our Data\nLet’s start off by bringing in our dataset that we’ll be using going forward.\nWe could just implement the components willy-nilly but it helps to perform the forward passes on actual tensors as we go, and understand the shapes.\nLet’s keep it simple and use the CIFAR-100 dataset; torchvision makes it very easy to use.\n\n# Define some variables pertaining to our dataset\nIMAGE_SIZE = 224\nTRAIN_TFMS = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\nTEST_TFMS = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ndef get_cifar100_dataset(root: str):\n    \n    trainset = datasets.CIFAR100(\n        root, train=True, download=True, transform=TRAIN_TFMS\n    )\n    valset = datasets.CIFAR100(\n        root, train=False, download=True, transform=TEST_TFMS\n    )\n    return trainset, valset\n\n# Create the Dataset\ntrain_ds, val_ds = get_cifar100_dataset(\"./data\")\n\n# Turn these into DataLoaders\nBATCH_SIZE = 16\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\nclear_output()\n\n# Let's visualize our data for fun\ngrid_img = torchvision.utils.make_grid(\n    tensor=next(iter(train_dl))[0], # grab a batch of images from the DataLoader\n    nrow=4\n)\nplt.imshow(grid_img.permute(1,2,0)) # move the channel axis to the end\nplt.axis(False)\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n\n\n\n\n\n\n\n\n\n\n# Get an instance from the dataset - returns a tuple of (input, ground truth)\nx, y = train_ds[0]\nprint(x.shape)\n\ntorch.Size([3, 224, 224])\n\n\nSome remarks about the above cells:\n\nThe IMAGE_SIZE being \\(224\\) specifically is because we will later load in a Vision Transformer checkpoint that was pretrained on images that were \\((3, 224, 224)\\) in shape.\nThe Resize and CenterCrop combination here is a common technique for processing images into a specific size.\nThe Normalize transform’s values for the mean and standard deviation looks rather strange, but it again follows what the checkpoint we will load in later was trained on. This is rather irritating since the statistics from the ImageNet dataset are very different, so this feels a bit of an anomaly in some ways."
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#embedding-an-input-image",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#embedding-an-input-image",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Embedding an Input Image",
    "text": "Embedding an Input Image\nTo leverage the transformer architecture for images, we first need to convert an image into a format that the model can process, similar to how words are tokenized and embedded in NLP tasks. This process involves dividing the image into smaller, non-overlapping patches and then embedding these patches into vectors—a step analogous to generating token embeddings from words.\nImagine an image as a grid of pixels, where each pixel carries information about color and intensity. Instead of processing the entire image at once, the Vision Transformer splits the image into fixed-size patches, treating each patch as a “token” in the sequence. For instance, an image of size \\((224,224)\\) pixels could be divided into patches of size \\((16,16)\\), resulting in a grid of \\((14 \\times 14)\\) patches if each patch is also \\((16,16)\\) pixels. This transformation effectively turns a 2D image into a 1D sequence of patches, where each patch contains a small portion of the image’s data.\nOnce the image is divided into patches, the next step is to embed each patch into a vector space that the transformer can work with. This is where convolutional layers come into play. By applying a convolutional layer with an appropriate kernel size and stride, we can extract features from each patch and represent these features as embedding vectors. Each embedding vector corresponds to a specific patch and captures its local information—such as edges, textures, and colors—while also compressing the data into a more manageable form for the transformer.\nThis patch embedding process is akin to the embedding of words in NLP. Just as words are embedded into vectors that encapsulate their semantic meaning, image patches are embedded into vectors that represent visual features. These embeddings are then fed into the transformer model, which applies self-attention to learn relationships and dependencies between different parts of the image. This enables the Vision Transformer to understand the global structure of the image, recognizing patterns and objects in a manner similar to how it understands sentences in language tasks.\nTLDR: - The Vision Transformer can treat an image as a sequence of fixed-size patches.\n\nEach patch is converted into a feature vector, and this is easily done using convolutional layers.\n\n\nPATCH_SIZE = 16\n\n# Let's explore how we end up with 14x14 patches with the hyperparameters defined so far\nnumber_of_patches = int((IMAGE_SIZE**2) / PATCH_SIZE**2)\nprint(f\"Using {PATCH_SIZE=}, we get {number_of_patches=} for each channel.\")\nprint(f\"This is what we expected as {14*14=}.\")\n\nUsing PATCH_SIZE=16, we get number_of_patches=196 for each channel.\nThis is what we expected as 14*14=196.\n\n\n\n# Now if we consider the output as a long sequence of patches, we can compute the expected output shape\nprint(f\"Original input shape: {x.shape}\")\n\npatchified = x.contiguous().view(number_of_patches, -1)\nprint(f\"Transformed input shape: {patchified.shape}\")\n\nOriginal input shape: torch.Size([3, 224, 224])\nTransformed input shape: torch.Size([196, 768])\n\n\n\nUsing Convolutions to Generate Patch Embeddings\nRecall that to convert our image into a sequence of patch embeddings, we need to:\n\nGenerate fixed-size patches from the image.\nEmbed these patches into a vector space.\n\nWe can achieve both of these steps in one go using nn.Conv2d.\nIf the convolution operation consists of a kernel of size \\((k, k)\\) and a stride of \\(k\\), it effectively breaks the input image into non-overlapping patches of size \\(k \\times k\\). The kernel, also known as the filter, slides across the image, covering different sections, or patches, of the input. At each position, the kernel performs a dot product between the filter weights and the corresponding input pixels, followed by summing these products and applying an activation function. The output from each position becomes an element in the resulting feature map.\nIn this context, the filter’s role is twofold. First, it serves as a mechanism for slicing the image into smaller patches, with each patch being a subset of the image’s pixel data. Second, the convolution operation inherently compresses and transforms these patches into a set of feature maps, which can be interpreted as the patch embeddings. By adjusting the kernel size and stride, we control the size of the patches and the level of overlap between them.\nFor example, using a kernel size of \\(k = 16\\) and stride \\(s = 16\\) on an image of size \\(224 \\times 224\\) will produce a grid of \\(14 \\times 14\\) patches, each of size \\(16 \\times 16\\). The convolutional layer outputs a feature map for each filter used, where each feature map represents a different aspect of the image’s data, such as edges, textures, or colors. The depth of the feature map, determined by the number of filters, defines the dimension of the patch embedding vectors.\nThus, using nn.Conv2d, we efficiently transform the input image into a sequence of patch embeddings, with each embedding vector encapsulating the salient features of its corresponding image patch.\n\n# Create the patchifier\npatchifier = nn.Conv2d(in_channels=3,\n                       out_channels=768, # as we computed above\n                       kernel_size=PATCH_SIZE,\n                       stride=PATCH_SIZE,\n                       padding=0                       \n)\n\n# Transform a batch of inputs to see how it works\nx, _ = next(iter(train_dl))\nout = patchifier(x)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Patchified shape: {out.shape}\")\n\nInput shape: torch.Size([16, 3, 224, 224])\nPatchified shape: torch.Size([16, 768, 14, 14])\n\n\nQuick note about the shape:\n\\[(16, 768, 14, 14)\\] \\[\\text{(batch size, embedding dim, number of patches horizontally, number of patches vertically)}\\]\nSince we want to treat this as a sequence, i.e. losing the 2D structure, we can simply flatten this along the last two axes.\nWe will also transpose the tensor so that we have the number of channels/features at the end, this is just convention.\n\npatch_emb = out.flatten(start_dim=2).transpose(1, 2) # NCHW -&gt; NLC\nprint(f\"Final shape: {patch_emb.shape}\")\n\nFinal shape: torch.Size([16, 196, 768])\n\n\nBefore moving forward, another important point to note is how we will incorporate a “CLS token” or “classification token” later on for our task of image classification.\nThis is a technique borrowed from BERT, where you have a learnable embedding meant to represent the entire input sequence. In the context of the Vision Transformer, the CLS token is a special token added to the sequence of patch embeddings. It serves as a summary or a representation of the entire image. The model learns this token’s embedding along with the patch embeddings during training.\nAt the end of the transformer layers, the CLS token’s final embedding is used as the input to a classification head, typically a fully connected layer, to predict the class label. This approach allows the model to consider the entire image’s context when making a classification decision, leveraging the self-attention mechanism to gather information from all patches into this single, informative vector.\n\n\n\n\n# Quick demonstration of prepending a learnable embedding to this activation (along the \"patch length\" axis)\ncls_token = nn.Parameter(\n    torch.zeros(1, 1, 768) # channels-last\n)\n\ntoks = torch.cat([\n    cls_token.expand(BATCH_SIZE, -1, -1), # have to expand out the batch axis\n    patch_emb,\n], dim=1)\n\nprint(f\"Final shape of embeddings with the CLS token: {toks.shape}\")\n\nFinal shape of embeddings with the CLS token: torch.Size([16, 197, 768])\n\n\n\n\nExplicitly adding in positional information\nSelf-attention, while powerful, is inherently permutation invariant. This means that it does not take into account the order of the patches, treating them as a set rather than a sequence. However, in vision tasks, the spatial arrangement of patches is crucial for understanding the structure and relationships within an image.\nTo address this, we introduce positional encodings or embeddings. These are additional vectors added to the patch embeddings to inject information about the relative or absolute position of each patch in the image. In our implementation, we’ll use a set of learnable weights for these positional encodings, allowing the model to learn the most effective way to incorporate positional information during training.\nBy combining these positional encodings with the patch embeddings, we create a richer input representation that not only captures the visual content of the patches but also their spatial arrangement. This enriched input is then fed into the next main component of the transformer, enabling the model to leverage both the content and the position of patches for more accurate understanding and processing of the image.\nIn the cell below, note how we use nn.Parameter to intialize the tensor and the shape following the patch embeddings. We rely on broadcasting to create copies over the batch axis since we do not want there to be different positional encodings for elements in different batches (that would make no sense).\n\n# Initialize a randomly initialized set of positional encodings\npos_embed = nn.Parameter(torch.randn(1, toks.shape[1], toks.shape[2]) * 0.02) # this factor is from the timm implementation\nx = toks + pos_embed\n\nprint(f\"Final shape of input: {x.shape}\")\n\nFinal shape of input: torch.Size([16, 197, 768])"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#multi-head-self-attention",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#multi-head-self-attention",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Multi-Head Self Attention",
    "text": "Multi-Head Self Attention\nSelf-attention is a fundamental mechanism in the Vision Transformer that enables patches, or tokens, to communicate with one another across the entire image. This mechanism allows each patch to consider the information from all other patches, effectively sharing and enriching the context of each patch’s representation. In the scene of computer vision, this means that the model can capture relationships and dependencies between different parts of the image, such as identifying that certain shapes or colors in one part of the image may relate to features in another part. This global interaction helps the model build a more comprehensive understanding of the image, crucial for tasks like image classification.\nThe Self-Attention mechanism can be expressed with the following expression from the 2017 paper: \\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\\]\nHere, the components are:\n\nQueries (\\(Q\\)): These represent the specific aspects or “questions” that each patch wants to know about other patches.\nKeys (\\(K\\)): These act like tags or “keywords” that help identify relevant information in the patches.\nValues (\\(V\\)): These are the actual data or “answers” that the patches contain.\n\nTo explain with an analogy, think of queries, keys, and values as parts of a search engine system:\n\nQueries are like the search terms you enter. They specify what you’re looking for.\nKeys are the tags or metadata associated with each document or webpage, helping to determine if a document matches a search query.\nValues are the actual content of the documents that are retrieved and displayed.\n\nIn the Vision Transformer, these components are extracted from the patch embeddings using learned linear transformations. The three matrices are computed as follows: \\[Q = W^Q X\\] \\[K = W^K X\\] \\[V = W^V X\\]\nwhere \\(W^Q, W^K, W^V\\) are learnable weight matrices, and \\(X\\) is the tensor corresponding to the input embeddings.\n\n# Define variables before moving further\nembed_dim = 768\nB, P, C = x.shape\n\n# We can use nn.Linear layers (without the bias) to perform these computations\nquery = nn.Linear(embed_dim, embed_dim, bias=False)\nkey = nn.Linear(embed_dim, embed_dim, bias=False)\nvalue = nn.Linear(embed_dim, embed_dim, bias=False)\n\n# Get the projections, these are the Q, K, V matrices in the equation above\nq = query(x)\nk = key(x)\nv = value(x)\n\n# Get the shapes\nq.shape, k.shape, v.shape\n\n(torch.Size([16, 197, 768]),\n torch.Size([16, 197, 768]),\n torch.Size([16, 197, 768]))\n\n\n\nThe Multiple “Head(s)”\nIn the context of self-attention, a “head” refers to an individual attention mechanism within the multi-head attention framework. Each head operates independently, learning different aspects of the relationships between patches or tokens. By having multiple heads, the model can capture a diverse range of interactions and dependencies, enhancing its ability to understand complex patterns in the data.\nEach head has its own set of learnable parameters for queries, keys, and values. The use of multiple heads allows the model to focus on different types of relationships or features in parallel. For instance, one head might learn to focus on spatial locality, while another might capture more global interactions.\nNote how the in_features and out_features are the same for this setup. This actually makes it much easier for us to work with multiple heads in one go: we can partition the projections from these matrices by introducing a “head axis”, this would then let us perform computations with H of these vectors of size C//H in parallel.\n\nH = 12 # hyperparam from timm's implementation\nhead_size = C // H\n\n# MHSA is usually implemented as such\nq = query(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\nk = key(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\nv = value(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\n\nq.shape, k.shape, v.shape\n\n(torch.Size([16, 12, 197, 64]),\n torch.Size([16, 12, 197, 64]),\n torch.Size([16, 12, 197, 64]))\n\n\nWith the projections ready, we can finally implement the meat of the component: the equation.\n\\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\\]\nHere’s a few things to note: 1. \\(Q\\) and \\(K\\) are 4D tensors so the notion of a “tranpose” sounds rather strange. Note however, that the output we want from this operation is an Attention Map - a map that lays out the affinities between each and every pair of patches in the input tensor. This means what we really want is a (B, H, P, P) tensor - this means all we have to do is swap the last two axes of \\(K\\) and follow the rules of Batched Matrix Multiplication.\n\nThe scale factor \\(\\sqrt{d_k}\\) is helpful for stable training. Without this, the activations would blow up exactly on the order of \\(d_k\\), and this can lead to unstable gradients - so we scale everything down by this amount to end up with activations of unit-variance.\nThe Softmax here is applied on a row axis - i.e. the rows of the 2D slice must contain values that sum up to 1. This is important to note since we consider the dot product of the row of the query matrix with the columns of the transpose of the key (so if you ask a specific question following the analogy above, you’d want the answers to be weighed according to the question, not all questions weighed according to a single answer).\n\n\n# Let's implement this all in one go\n\nH = 12\nhead_size = C // H\n\nprint(f\"{x.shape=}\")\n\n# MHSA is usually implemented as such\nq = query(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\nk = key(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\nv = value(x).view(B, P, H, head_size).transpose(1, 2) # (B, H, P, head_size)\n\nscale = q.size(-1) # the head size\n\nq = q*scale\n\n# Take the dot product\nattn = q @ k.transpose(-2, -1) # (B, H, P, head_size) @ (B, H, head_size, P) --&gt; (B, H, P, P)\nprint(f\"{attn.shape=}\")\n\n# Softmax along the final axis, i.e. rows\nattn = attn.softmax(dim=-1)\nout = attn @ v # (B, H, P, P) @ (B, H, P, head_size) --&gt; (B, H, P, head_size)\nprint(f\"{out.shape=}\")\n\n# Collapse the head dimension to get back \nsa_out = out.transpose(1, 2).contiguous().view(B, P, C)\nprint(f\"{sa_out.shape=}\")\n\nx.shape=torch.Size([16, 197, 768])\nattn.shape=torch.Size([16, 12, 197, 197])\nout.shape=torch.Size([16, 12, 197, 64])\nsa_out.shape=torch.Size([16, 197, 768])\n\n\nThe shapes can help give us a very interesting interpretation of what Self-Attention really does: if the input and output shapes are exactly the same (i.e. [16, 197, 768]) but with so much of processing with learnable parameters in the middle, then we can think of this module as simply refining the representations.\nTo expand on this, your input is provided as a sequence of embeddings (order matters here) - the Self-Attention operation allows the elements to communicate and share information with one another, enriching each other with context, so that the final consolidated output is simply an enriched version of the input. It also helps that having the same shape allows you to stack these components on top of one another very easily, as we will see later.\n\n\nBuilding MHSA\nLet’s put all of these operations, with a few other bells and whistles into a single nn.Module class.\nNote how we add two things here: 1. The proj component: this is a projection back into the same vector space, so it again acts like simply refining what you already have. Andrej Karpathy calls this as thinking on what you have just computed from the Self-Attention operation.\n\nThe proj_drop component: This is a form of Dropout which acts as a regularizing mechanism for the Vision Transformer. This becomes very important since they are prone to overfit on simplistic datasets, so this and attn_drop (functionally the same thing) can help mitigate this heinous activity.\n\n\nclass MHSA(nn.Module):\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int = 8,\n            qkv_bias: bool = False,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.\n    ) -&gt; None:\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, P, C = x.shape\n        H = self.num_heads\n        q = self.q(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        k = self.k(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        v = self.v(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        \n        q = q * self.scale\n\n        attn = q @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, P, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\n# Perform a forward pass on a dummy input\nmhsa = MHSA(dim=768,\n            num_heads=12,\n            qkv_bias=True,\n            )\n\n# Create sample input tensor\nB, P, C = 16, 196, 768\nx = torch.randn(B, P, C)\n\n# Pass the input through the MHSA layer to trigger the hook\noutput = mhsa(x)\nprint(f\"{x.shape=} --&gt; {output.shape=}\")\ndel mhsa\n\nx.shape=torch.Size([16, 196, 768]) --&gt; output.shape=torch.Size([16, 196, 768])"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#the-encoder-block",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#the-encoder-block",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "The “Encoder Block”",
    "text": "The “Encoder Block”\nLet’s go back to the main illustration in the original paper:\n\n\n\nThe grayed out block on the right side represents a single “Encoder Block”.\nThe Encoder Block is a fundamental component of the Vision Transformer, inheriting its architecture from the original Transformer model used in natural language processing. Each Encoder Block is designed to process and refine the input sequence—here, the sequence of patch embeddings enriched with positional encodings.\nThe Encoder Block consists of two main sub-layers:\nMulti-Head Self-Attention Mechanism: This layer allows the model to focus on different parts of the input sequence simultaneously, capturing various relationships and dependencies among the patches. As discussed earlier, multiple heads enable the model to learn different aspects of the data, providing a comprehensive representation.\nFeed-Forward Neural Network (FFN): Following the self-attention layer, a position-wise feed-forward neural network processes the output. This network typically consists of two linear transformations separated by a non-linear activation function, such as GeLU. It acts on each position independently and helps in further transforming and refining the representation learned from the self-attention layer.\nTo ensure the effective flow of gradients during training and to preserve the original input information, each sub-layer is equipped with Skip Connections (also known as Residual Connections). These connections add the input of each sub-layer to its output, forming a residual path that facilitates better gradient propagation and helps prevent the vanishing gradient problem. Mathematically, this can be expressed as:\n\\[\\text{Output} = \\text{Layernorm}(x + \\text{SubLayer}(x))\\]\nIn the equation above, \\(x\\) represents the input to the sub-layer, and the sum \\(x + \\text{SubLayer}(x)\\) forms the residual connection. This is a very old idea in Deep Learning, going back to 2015 with ResNets, and can perhaps be better understood with the following illustration:\n\n\n\nThe output is then normalized using Layer Normalization (LayerNorm), which stabilizes the training by normalizing the summed outputs across each patch, ensuring that the model’s activations are within a stable range. LayerNorm adjusts the output by scaling and shifting, allowing the model to maintain useful information while preventing excessive internal covariate shifts.\nThe Encoder Block’s design, with its combination of self-attention, feed-forward neural networks, skip connections, and layer normalization, enables the Vision Transformer to learn rich, hierarchical representations of the input data. This structure is repeated multiple times in the model, each block building upon the representations learned by the previous one, gradually refining the understanding of the input image.\nLet’s go ahead and implement this directly.\n\nclass Mlp(nn.Module):\n    def __init__(self, \n                 in_features: int,\n                 hidden_features: int,\n                 drop: float,\n                 norm_layer: nn.Module = None) -&gt; None:\n        super().__init__()\n\n        # There are two Linear layers in the conventional implementation\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, in_features)\n\n        # Dropout is used twice, once after each linear layer\n        self.drop1 = nn.Dropout(drop)\n        self.drop2 = nn.Dropout(drop)\n\n        # The paper uses the GeLU activation function\n        self.act = nn.GELU()\n\n        # Optional normalization layer (following timm's convention)\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = False,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n    ) -&gt; None:\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            drop=proj_drop,\n        )\n\n        self.attn = MHSA(\n            dim=dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n    \nblock = Block(dim=768,\n              num_heads=12)\nblock_out = block(x)\n\nprint(f\"{x.shape=} --&gt; {block_out.shape=}\")\ndel block\n\nx.shape=torch.Size([16, 196, 768]) --&gt; block_out.shape=torch.Size([16, 196, 768])\n\n\nA wonderful observation here is again that the input and output shapes are exactly the same!\nThis means we can again fall back on the interpretation that each Block (on top of each MHSA module) is simply refining and embedding rich context into whatever input is fed into it.\nPlus, having the same shape as the input allows us to stack these encoders nicely on top of each other without much thought or care."
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#putting-it-all-together",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#putting-it-all-together",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Putting it all together",
    "text": "Putting it all together\nNow with everything we’ve learned so far, let’s make one final class that aggregates everything.\nRecall what we had to do with\n\nThe Patch Embeddings to represent our image as a sequence and let Self-Attention link parts of it with one another,\nThe Positional Encodings/Embeddings to move past the permutation invariant nature of Self-Attention and embed information regarding the position of each patch into the mix,\nThe CLS Token to let the model have an overall representation of the entire image which would provide a means of performing classification,\nThe Multi-Head Self-Attention class (MHSA) to let the patches communicate and share information with one another in the hopes of enriching the representations,\nThe Block class (Block) to be able to string together the computations performed by the Self-Attention, Feedforward, and Layer Normalization modules.\n\nNow we put it all together.\n\nclass PatchEmbed(nn.Module):\n    def __init__(self,\n                 img_size: int,\n                 patch_size: int,\n                 in_chans: int,\n                 embed_dim: int,\n                 bias: bool = True,\n                 norm_layer: Optional[Callable] = None) -&gt; None:\n        super().__init__()\n        \n        self.img_size = img_size\n        self.patch_size = patch_size\n\n        # self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n        self.grid_size = (self.img_size // self.patch_size, ) * 2\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n        self.proj = nn.Conv2d(in_chans, \n                              embed_dim, \n                              kernel_size=patch_size, \n                              stride=patch_size, \n                              bias=bias, \n                              padding=0)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1,2) # flatten and transpose: BCHW -&gt; BLC\n        return x\n    \nclass MHSA(nn.Module):\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int = 8,\n            qkv_bias: bool = False,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.\n    ) -&gt; None:\n        super().__init__()\n        \n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, P, C = x.shape\n        H = self.num_heads\n        q = self.q(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        k = self.k(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        v = self.v(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        \n        q = q * self.scale\n\n        attn = q @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, P, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass Mlp(nn.Module):\n    def __init__(self, \n                 in_features: int,\n                 hidden_features: int,\n                 drop: float,\n                 norm_layer: nn.Module = None) -&gt; None:\n        super().__init__()\n\n        # There are two Linear layers in the conventional implementation\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, in_features)\n\n        # Dropout is used twice, once after each linear layer\n        self.drop1 = nn.Dropout(drop)\n        self.drop2 = nn.Dropout(drop)\n\n        # The paper uses the GeLU activation function\n        self.act = nn.GELU()\n\n        # Optional normalization layer (following timm's convention)\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = False,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n    ) -&gt; None:\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            drop=proj_drop,\n        )\n\n        self.attn = MHSA(\n            dim=dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nclass VisionTransformer(nn.Module):\n\n    def __init__(\n            self,\n            img_size: Union[int, Tuple[int, int]] = 224,\n            patch_size: Union[int, Tuple[int, int]] = 16,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            embed_dim: int = 768,\n            depth: int = 12,\n            num_heads: int = 12,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            drop_rate: float = 0.,\n            pos_drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of image input channels.\n            num_classes: Number of classes for classification heads\n            embed_dim: Transformer embedding dimension.\n            depth: Depth of transformer.\n            num_heads: Number of attention heads.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: Enable bias for qkv projections if True.\n            drop_rate: Head dropout rate.\n            pos_drop_rate: Position embedding dropout rate.\n            attn_drop_rate: Attention dropout rate.\n        \"\"\"\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_features = self.head_hidden_size = self.embed_dim = embed_dim  # for consistency with other models\n\n        # Define the Patch Embedding module - note this does not include the CLS token yet\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            bias=True,\n        )\n        num_patches = self.patch_embed.num_patches\n        embed_len = num_patches + 1 # don't forget we need to incorporate the CLS token\n\n        # Define the CLS token, the Positional Encodings/Embeddings, and a Dropout for the Positional information\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        # Define LayerNorms for before and after the Encoder block processing\n        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n        self.norm_pre = norm_layer(embed_dim)\n        self.norm = norm_layer(embed_dim)\n\n        # Initialize the blocks\n        self.blocks = nn.Sequential(*[\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate\n            )\n            for i in range(depth)])\n        \n        self.feature_info = [\n            dict(module=f'blocks.{i}', num_chs=embed_dim) for i in range(depth)\n        ]\n\n        # Classifier Head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes)\n\n    def forward_features(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \n        # Get the Patch Embeddings\n        x = self.patch_embed(x)\n\n        # Prepend the CLS token, THEN add in the positional information\n        x = torch.cat([\n            self.cls_token.expand(x.shape[0], -1, -1), # have to expand out the batch axis\n            x,\n        ], dim=1)\n        x = x + self.pos_embed\n\n        # Pass through the LayerNorms, the Encoder Blocks, then the final Norm\n        x = self.norm_pre(x)\n        x = self.blocks(x)\n        x = self.norm(x)\n\n        return x\n\n    def forward_head(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \n        # Get the CLS token (also called \"pooling\" our logits)\n        x = x[:, 0]\n\n        # Pass through the Dropout then the classification head\n        x = self.head_drop(x)\n        x = self.head(x)\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n    \nmodel = VisionTransformer()\n\n\nx, _ = next(iter(train_dl))\nout = model(x)\nprint(out.shape)\n\ntorch.Size([16, 1000])"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#loading-in-pretrained-weights",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#loading-in-pretrained-weights",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Loading in Pretrained Weights",
    "text": "Loading in Pretrained Weights\nNow that we’ve built our simplistic version of a Vision Transformer, let’s load in the parameters from a pretrained checkpoint, specifically from the Base variant of the timm library.\nThe procedure is simple: load in the state_dict from the pretrained model, and match with the corresponding parameters in your implementation. Since we were careful to stick with a naming convention similar to the implementation already in timm, we’ve made our job simpler already.\nWe will do this in a slightly unconventional way - we will define methods for loading in the parameters in each module. This makes our code arguably more readable, but requires us to rewrite the module again for the third time in this article (last time, promise!).\nThis form of matching and copying has been inspired from Sebastian Raschka’s book on “LLMs from Scratch”, specifically this notebook.\n\ndef assign_check(left: torch.Tensor, right: torch.Tensor):\n    '''Utility for checking and creating parameters to be used in loading a pretrained checkpoint'''\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch: Left: {left.shape}, Right: {right.shape}\")\n    return torch.nn.Parameter(right.clone().detach())\n\nclass PatchEmbed(nn.Module):\n    def __init__(self,\n                 img_size: int,\n                 patch_size: int,\n                 in_chans: int,\n                 embed_dim: int,\n                 bias: bool = True,\n                 norm_layer: Optional[Callable] = None) -&gt; None:\n        super().__init__()\n        \n        self.img_size = img_size\n        self.patch_size = patch_size\n\n        # self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n        self.grid_size = (self.img_size // self.patch_size, ) * 2\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n        self.proj = nn.Conv2d(in_chans, \n                              embed_dim, \n                              kernel_size=patch_size, \n                              stride=patch_size, \n                              bias=bias, \n                              padding=0)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1,2) # flatten and transpose: BCHW -&gt; BLC\n        return x\n    \nclass MHSA(nn.Module):\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int = 8,\n            qkv_bias: bool = False,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.\n    ) -&gt; None:\n        super().__init__()\n        \n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, P, C = x.shape\n        H = self.num_heads\n        q = self.q(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        k = self.k(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        v = self.v(x).view(B, P, H, -1).transpose(1, 2) # (B, H, P, head_size)\n        \n        q = q * self.scale\n\n        attn = q @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, P, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n    \n    def att_weight_conversion(self, qkv_params):\n        '''\n        Split and convert the QKV parameters from ViT checkpoints for the GQA implementation\n        '''\n        q, k, v = torch.split(qkv_params, qkv_params.shape[0] // 3, dim=0)\n        \n        return {\n            \"q\": q,\n            \"k\": k,\n            \"v\": v\n        }\n    \n    def load_pretrained_weights(self, state_dict, block_idx):\n\n        # Load in parameters for the Query Key Value layers\n        qkv_weight = state_dict[f'blocks.{block_idx}.attn.qkv.weight']\n        qkv_bias = state_dict[f'blocks.{block_idx}.attn.qkv.bias']\n\n        wdict = self.att_weight_conversion(qkv_weight)\n        bdict = self.att_weight_conversion(qkv_bias)\n\n        self.q.weight = assign_check(self.q.weight, wdict['q'])\n        self.q.bias = assign_check(self.q.bias, bdict['q'])\n\n        self.k.weight = assign_check(self.k.weight, wdict['k'])\n        self.k.bias = assign_check(self.k.bias, bdict['k'])\n        \n        self.v.weight = assign_check(self.v.weight, wdict['v'])\n        self.v.bias = assign_check(self.v.bias, bdict['v'])\n\n        # Load in parameters for the output projection\n        self.proj.weight = assign_check(self.proj.weight, state_dict[f'blocks.{block_idx}.attn.proj.weight'])\n        self.proj.bias = assign_check(self.proj.bias, state_dict[f'blocks.{block_idx}.attn.proj.bias'])\n\nclass Mlp(nn.Module):\n    def __init__(self, \n                 in_features: int,\n                 hidden_features: int,\n                 drop: float,\n                 norm_layer: nn.Module = None) -&gt; None:\n        super().__init__()\n\n        # There are two Linear layers in the conventional implementation\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, in_features)\n\n        # Dropout is used twice, once after each linear layer\n        self.drop1 = nn.Dropout(drop)\n        self.drop2 = nn.Dropout(drop)\n\n        # The paper uses the GeLU activation function\n        self.act = nn.GELU()\n\n        # Optional normalization layer (following timm's convention)\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = False,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n    ) -&gt; None:\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            drop=proj_drop,\n        )\n\n        self.attn = MHSA(\n            dim=dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n    \n    def load_pretrained_weights(self, state_dict, block_idx):\n\n        self.attn.load_pretrained_weights(state_dict, block_idx)\n\n        self.norm1.weight = assign_check(self.norm1.weight, state_dict[f'blocks.{block_idx}.norm1.weight'])\n        self.norm1.bias = assign_check(self.norm1.bias, state_dict[f'blocks.{block_idx}.norm1.bias'])\n        \n        self.norm2.weight = assign_check(self.norm2.weight, state_dict[f'blocks.{block_idx}.norm2.weight'])\n        self.norm2.bias = assign_check(self.norm2.bias, state_dict[f'blocks.{block_idx}.norm2.bias'])\n\n        self.mlp.fc1.weight = assign_check(self.mlp.fc1.weight, state_dict[f'blocks.{block_idx}.mlp.fc1.weight'])\n        self.mlp.fc1.bias = assign_check(self.mlp.fc1.bias, state_dict[f'blocks.{block_idx}.mlp.fc1.bias'])\n        self.mlp.fc2.weight = assign_check(self.mlp.fc2.weight, state_dict[f'blocks.{block_idx}.mlp.fc2.weight'])\n        self.mlp.fc2.bias = assign_check(self.mlp.fc2.bias, state_dict[f'blocks.{block_idx}.mlp.fc2.bias'])\n    \nclass VisionTransformer(nn.Module):\n\n    def __init__(\n            self,\n            img_size: Union[int, Tuple[int, int]] = 224,\n            patch_size: Union[int, Tuple[int, int]] = 16,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            embed_dim: int = 768,\n            depth: int = 12,\n            num_heads: int = 12,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            drop_rate: float = 0.,\n            pos_drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of image input channels.\n            num_classes: Number of classes for classification heads\n            embed_dim: Transformer embedding dimension.\n            depth: Depth of transformer.\n            num_heads: Number of attention heads.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: Enable bias for qkv projections if True.\n            drop_rate: Head dropout rate.\n            pos_drop_rate: Position embedding dropout rate.\n            attn_drop_rate: Attention dropout rate.\n        \"\"\"\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_features = self.head_hidden_size = self.embed_dim = embed_dim  # for consistency with other models\n\n        # Define the Patch Embedding module - note this does not include the CLS token yet\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            bias=True,\n        )\n        num_patches = self.patch_embed.num_patches\n        embed_len = num_patches + 1 # don't forget we need to incorporate the CLS token\n\n        # Define the CLS token, the Positional Encodings/Embeddings, and a Dropout for the Positional information\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        # Define LayerNorms for before and after the Encoder block processing\n        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n        self.norm_pre = norm_layer(embed_dim)\n        self.norm = norm_layer(embed_dim)\n\n        # Initialize the blocks\n        self.blocks = nn.Sequential(*[\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate\n            )\n            for i in range(depth)])\n        \n        self.feature_info = [\n            dict(module=f'blocks.{i}', num_chs=embed_dim) for i in range(depth)\n        ]\n\n        # Classifier Head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes)\n\n    def forward_features(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \n        # Get the Patch Embeddings\n        x = self.patch_embed(x)\n\n        # Prepend the CLS token, THEN add in the positional information\n        x = torch.cat([\n            self.cls_token.expand(x.shape[0], -1, -1), # have to expand out the batch axis\n            x,\n        ], dim=1)\n        x = x + self.pos_embed\n\n        # Pass through the LayerNorms, the Encoder Blocks, then the final Norm\n        x = self.norm_pre(x)\n        x = self.blocks(x)\n        x = self.norm(x)\n\n        return x\n\n    def forward_head(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \n        # Get the CLS token (also called \"pooling\" our logits)\n        x = x[:, 0]\n\n        # Pass through the Dropout then the classification head\n        x = self.head_drop(x)\n        x = self.head(x)\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n    \n    def load_pretrained_weights(self, state_dict):\n        \n        print(\"Loading in weights...\")\n        \n        for b, block in enumerate(self.blocks):\n            block.load_pretrained_weights(state_dict, b)\n        print(f\"Finished with {b+1} blocks...\")\n\n        self.patch_embed.proj.weight = assign_check(self.patch_embed.proj.weight, state_dict['patch_embed.proj.weight'])\n        self.patch_embed.proj.bias = assign_check(self.patch_embed.proj.bias, state_dict['patch_embed.proj.bias'])\n        self.cls_token = assign_check(self.cls_token, state_dict['cls_token'])\n        self.pos_embed = assign_check(self.pos_embed, state_dict['pos_embed'])\n\n        print(\"Success!\")\n\n\ndef vit_small_patch16_224(\n        num_classes: int = 10,\n        pretrained: bool = False,\n        in_chans: int = 3,\n        drop_rate: float = 0,\n        pos_drop_rate: float = 0,\n        attn_drop_rate: float = 0,\n        proj_drop_rate: float = 0.\n        ):\n    \n    model = VisionTransformer(\n        img_size=224,\n        patch_size=16,\n        in_chans=in_chans,\n        num_classes=num_classes,\n        embed_dim=384,\n        num_heads=6,\n        depth=12,\n        drop_rate=drop_rate,\n        pos_drop_rate=pos_drop_rate,\n        attn_drop_rate=attn_drop_rate,\n        proj_drop_rate=proj_drop_rate\n    )\n\n    if pretrained:\n        ckpt = 'vit_small_patch16_224'\n        if in_chans != 3:\n            raise ValueError(f\"Cannot load in checkpoint with {in_chans=}\")\n        print(f'Using checkpoint {ckpt}...')\n        hf_model = timm.create_model(ckpt, pretrained=True)\n        model.load_pretrained_weights(hf_model.state_dict())\n    \n    return model\n\n\n# Load in our model\nmodel = vit_small_patch16_224(num_classes=100,\n                             pretrained=True)\n\nUsing checkpoint vit_small_patch16_224...\nLoading in weights...\nFinished with 12 blocks...\nSuccess!"
  },
  {
    "objectID": "blog/posts/2024-06-13-vision-transformer/index.html#finetuning-our-vision-transformer",
    "href": "blog/posts/2024-06-13-vision-transformer/index.html#finetuning-our-vision-transformer",
    "title": "Implementing a Vision Transformer from Scratch",
    "section": "Finetuning our Vision Transformer",
    "text": "Finetuning our Vision Transformer\nNow with everything in place, we can finally move on to actually finetuning our model.\nWe will use the CIFAR100 dataset since it’s (arguably) rather difficult to perform well on: there’s only 600 images for each of its 100 classes, so training a model from scratch would be very difficult to get to perform well on this dataset. This serves as a nice pressure test for our model, provided we did our loading of the pretrained checkpoint properly.\nTo spice things up, we’ll add in random augmentations during the training process.\n\n# Prepare CIFAR100\nIMAGE_SIZE = 224\nTRAIN_TFMS = transforms.Compose([\n    transforms.RandAugment(),\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\nTEST_TFMS = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ndef get_dataloader(dataset: torch.utils.data.Dataset,\n                   batch_size: int,\n                   is_train: bool,\n                   num_workers: int = 1):\n    \n    loader = DataLoader(dataset, batch_size=batch_size,\n                        shuffle=is_train, num_workers=num_workers)\n    return loader\n\ndef get_cifar100_dataset(root: str):\n\n    trainset = torchvision.datasets.CIFAR100(\n        root, train=True, download=True, transform=TRAIN_TFMS\n    )\n\n    testset = torchvision.datasets.CIFAR100(\n        root, train=False, download=True, transform=TEST_TFMS\n    )\n\n    return trainset, testset\n\ntrain, val = get_cifar100_dataset(\"./data\")\ntrain_dl = get_dataloader(train, 32, True)\nval_dl = get_dataloader(val, 32, False)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\nNow we’ll define our training and validation functions.\nIt’s always good to make your code modular.\n\ndef train_step(model, dataloader, criterion, optimizer, device):\n    '''Train for one epoch'''\n\n    model.train()\n\n    train_loss = 0.\n    train_acc = 0.\n\n    for step, (X, y) in tqdm(enumerate(dataloader), desc=\"Training\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        train_acc += ((y_pred == y).sum().item() / len(y))\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\n@torch.inference_mode()\ndef eval_step(model, dataloader, criterion, device):\n    \n    model.eval()\n\n    eval_loss = 0.\n    eval_acc = 0.\n\n    for (X, y) in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        X, y = X.to(device), y.to(device)\n\n        logits = model(X)\n        loss = criterion(logits, y)\n\n        eval_loss += loss.item()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        eval_acc += ((y_pred == y).sum().item() / len(y))\n\n    eval_loss = eval_loss / len(dataloader)\n    eval_acc = eval_acc / len(dataloader)\n    return eval_loss, eval_acc\n\nNow to actually kick off training.\nWe’ll keep things simple: - Finetune for 3 epochs - Use a learning rate of 0.0001 - Use the AdamW optimizer since this usually works well for something as annoying to train as a Vision Transformer\nTo speed up the training process, we’ll also use torch.compile since our implementation is very straightforward and will not incur any nasty graph breaks or such during the compilation process.\n\nlearning_rate = 1e-4\nepochs = 3\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.to(device)\nmodel = torch.compile(model)\n\nhistory = {k: [] for k in (\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\")}\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    train_loss, train_acc = train_step(model, train_dl, criterion, optimizer, device)\n    val_loss, val_acc = eval_step(model, val_dl, criterion, device)\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n\nEpochs: 100%|██████████| 3/3 [04:21&lt;00:00, 87.07s/it] \n\n\n\n# Plot the loss curves\nplt.figure(figsize=(6,6))\nplt.plot(history[\"train_loss\"], label=\"Train Loss\")\nplt.plot(history[\"val_loss\"], label=\"Validation Loss\")\nplt.title(\"Loss Curves\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the loss curves\nplt.figure(figsize=(6,6))\nplt.plot(history[\"train_acc\"], label=\"Train Accuracy\")\nplt.plot(history[\"val_acc\"], label=\"Validation Accuracy\")\nplt.title(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nHopefully you can appreciate what we’ve built together from start to end.\nYou can use the same type of recipe to make a BERT clone, the only real differences there would be (1) the data munging, (2) the positional encodings, and (3) the token embeddings in place of the patch embeddings. Otherwise, the same general principles would apply here and big chunks of the same code can be copied verbatim."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome!\nI’m documenting parts of my learning journey through this blog. If I can’t code it, I don’t understand it.\nIf you want to learn more about me, visit the About Page. If you’re looking for something to read, check out my latest blog posts."
  }
]