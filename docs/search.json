[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Zohaib Khan",
    "section": "",
    "text": "Still trynna figure out how to make this website lol"
  },
  {
    "objectID": "07-nanogptv2.html",
    "href": "07-nanogptv2.html",
    "title": "Let’s build GPT",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# Read in the text file\nwith open('input.txt') as f:\n    text = f.read()\n\n# Get all the unique chars in this list to create our vocab\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\nprint(len(text), vocab_size)\n\n1115394 65\n\n\n\n# Tokenize the text\nstoi = {ch: i for i,ch in enumerate(chars)}\nitos = {i: ch for i,ch in enumerate(chars)}\n\nencode = lambda s: [stoi[c] for c in s]             # encode: string -&gt; list of ints\ndecode = lambda l: ''.join([itos[i] for i in l])    # decode: list of ints -&gt; string\n\nprint(encode(\"hello world\"))\n\n[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n\n\n\n# Encode the entire piece of text and store as torch.tensor\ndata = torch.tensor(encode(text), dtype=torch.long)\ndata.shape  # vector of ints corresponding to each and every char\n\ntorch.Size([1115394])\n\n\nNow we can start thinking of what our model will actually do.\nIt works off taking a chunk of integers, whose length is at most ctx_len, and will predict the next integer in the sequence. Note that the chunks can be any length, we just have to specify the maximum context length for our model.\nWhen we take a chunk of 8 chars, we don’t just predict the next character after this sequence of 8 chars - we train our model to predict at each and every one of these positions. This means we have \\(n\\) different training examples for each context of length \\(n\\).\nThe model is being made to predict at contexts with sizes all the way from 1 till ctx_len; this means it has the ability to predict the next token and start generating when it’s been given just one token of context.\n\nctx_len = 8\n\nx = data[:ctx_len]\ny = data[1:ctx_len+1]   # the target is the window offset by 1 token\n\nfor t in range(ctx_len):\n    context = x[:t+1]   # grab the first t tokens in x\n    target = y[t]       # since y is just x shifted by 1\n    print(f\"{context.tolist()} --&gt; {target}\")\n\n[18] --&gt; 47\n[18, 47] --&gt; 56\n[18, 47, 56] --&gt; 57\n[18, 47, 56, 57] --&gt; 58\n[18, 47, 56, 57, 58] --&gt; 1\n[18, 47, 56, 57, 58, 1] --&gt; 15\n[18, 47, 56, 57, 58, 1, 15] --&gt; 47\n[18, 47, 56, 57, 58, 1, 15, 47] --&gt; 58\n\n\n\n# Now to create a function to generate a batch of these chunks\nbatch_size = 4\n\ndef get_batch():\n    # All we really need is a set of batch_size random indices\n    idxs = torch.randint(len(data)-ctx_len, (batch_size,))  # (B,)\n    x = torch.stack([data[i:i+ctx_len] for i in idxs])      # (B, ctx_len)\n    y = torch.stack([data[i+1:i+ctx_len+1] for i in idxs])  # (B, ctx_len) (x offset by 1)\n    return x, y\n\nx, y = get_batch()\nprint(x)\nprint()\nprint(y)\nprint()\n\n# Print out the examples for the first sequence in the batch\nfor t in range(ctx_len):\n    context = x[0, :t+1]   # grab the first t tokens in x\n    target = y[0, t]       # since y is just x shifted by 1\n    print(f\"{context.tolist()} --&gt; {target}\")\n\ntensor([[41, 58, 47, 53, 52,  1, 42, 47],\n        [56, 47, 52, 45,  1, 46, 43, 50],\n        [15, 39, 50, 39, 47, 57,  0, 16],\n        [58, 46, 53, 59, 57, 39, 52, 42]])\n\ntensor([[58, 47, 53, 52,  1, 42, 47, 45],\n        [47, 52, 45,  1, 46, 43, 50, 51],\n        [39, 50, 39, 47, 57,  0, 16, 47],\n        [46, 53, 59, 57, 39, 52, 42,  1]])\n\n[41] --&gt; 58\n[41, 58] --&gt; 47\n[41, 58, 47] --&gt; 53\n[41, 58, 47, 53] --&gt; 52\n[41, 58, 47, 53, 52] --&gt; 1\n[41, 58, 47, 53, 52, 1] --&gt; 42\n[41, 58, 47, 53, 52, 1, 42] --&gt; 47\n[41, 58, 47, 53, 52, 1, 42, 47] --&gt; 45"
  },
  {
    "objectID": "07-nanogptv2.html#loading-and-tokenizing-the-data",
    "href": "07-nanogptv2.html#loading-and-tokenizing-the-data",
    "title": "Let’s build GPT",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# Read in the text file\nwith open('input.txt') as f:\n    text = f.read()\n\n# Get all the unique chars in this list to create our vocab\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\nprint(len(text), vocab_size)\n\n1115394 65\n\n\n\n# Tokenize the text\nstoi = {ch: i for i,ch in enumerate(chars)}\nitos = {i: ch for i,ch in enumerate(chars)}\n\nencode = lambda s: [stoi[c] for c in s]             # encode: string -&gt; list of ints\ndecode = lambda l: ''.join([itos[i] for i in l])    # decode: list of ints -&gt; string\n\nprint(encode(\"hello world\"))\n\n[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n\n\n\n# Encode the entire piece of text and store as torch.tensor\ndata = torch.tensor(encode(text), dtype=torch.long)\ndata.shape  # vector of ints corresponding to each and every char\n\ntorch.Size([1115394])\n\n\nNow we can start thinking of what our model will actually do.\nIt works off taking a chunk of integers, whose length is at most ctx_len, and will predict the next integer in the sequence. Note that the chunks can be any length, we just have to specify the maximum context length for our model.\nWhen we take a chunk of 8 chars, we don’t just predict the next character after this sequence of 8 chars - we train our model to predict at each and every one of these positions. This means we have \\(n\\) different training examples for each context of length \\(n\\).\nThe model is being made to predict at contexts with sizes all the way from 1 till ctx_len; this means it has the ability to predict the next token and start generating when it’s been given just one token of context.\n\nctx_len = 8\n\nx = data[:ctx_len]\ny = data[1:ctx_len+1]   # the target is the window offset by 1 token\n\nfor t in range(ctx_len):\n    context = x[:t+1]   # grab the first t tokens in x\n    target = y[t]       # since y is just x shifted by 1\n    print(f\"{context.tolist()} --&gt; {target}\")\n\n[18] --&gt; 47\n[18, 47] --&gt; 56\n[18, 47, 56] --&gt; 57\n[18, 47, 56, 57] --&gt; 58\n[18, 47, 56, 57, 58] --&gt; 1\n[18, 47, 56, 57, 58, 1] --&gt; 15\n[18, 47, 56, 57, 58, 1, 15] --&gt; 47\n[18, 47, 56, 57, 58, 1, 15, 47] --&gt; 58\n\n\n\n# Now to create a function to generate a batch of these chunks\nbatch_size = 4\n\ndef get_batch():\n    # All we really need is a set of batch_size random indices\n    idxs = torch.randint(len(data)-ctx_len, (batch_size,))  # (B,)\n    x = torch.stack([data[i:i+ctx_len] for i in idxs])      # (B, ctx_len)\n    y = torch.stack([data[i+1:i+ctx_len+1] for i in idxs])  # (B, ctx_len) (x offset by 1)\n    return x, y\n\nx, y = get_batch()\nprint(x)\nprint()\nprint(y)\nprint()\n\n# Print out the examples for the first sequence in the batch\nfor t in range(ctx_len):\n    context = x[0, :t+1]   # grab the first t tokens in x\n    target = y[0, t]       # since y is just x shifted by 1\n    print(f\"{context.tolist()} --&gt; {target}\")\n\ntensor([[41, 58, 47, 53, 52,  1, 42, 47],\n        [56, 47, 52, 45,  1, 46, 43, 50],\n        [15, 39, 50, 39, 47, 57,  0, 16],\n        [58, 46, 53, 59, 57, 39, 52, 42]])\n\ntensor([[58, 47, 53, 52,  1, 42, 47, 45],\n        [47, 52, 45,  1, 46, 43, 50, 51],\n        [39, 50, 39, 47, 57,  0, 16, 47],\n        [46, 53, 59, 57, 39, 52, 42,  1]])\n\n[41] --&gt; 58\n[41, 58] --&gt; 47\n[41, 58, 47] --&gt; 53\n[41, 58, 47, 53] --&gt; 52\n[41, 58, 47, 53, 52] --&gt; 1\n[41, 58, 47, 53, 52, 1] --&gt; 42\n[41, 58, 47, 53, 52, 1, 42] --&gt; 47\n[41, 58, 47, 53, 52, 1, 42, 47] --&gt; 45"
  },
  {
    "objectID": "07-nanogptv2.html#language-model",
    "href": "07-nanogptv2.html#language-model",
    "title": "Let’s build GPT",
    "section": "Language Model",
    "text": "Language Model\nThe simplest model we can start with is the language model, where we predict the next character in the sequence purely by looking at the current character.\nThis can be formulated as using Embeddings - a simple lookup table - to get the representation of the current character. We could set the embedding dimensionality to be the same as the vocab size so that our output from the table can be interpreted as a probability distribution over the vocabulary.\nSo, we’re taking as input a \\((B,T)\\) tensor, and the model will output a \\((B,T,V)\\) tensor, where \\(V\\) is the vocabulary size. This is the case because we are predicting a token (i.e. predicting the probability distribution over the entire vocab), for all \\(B \\times T\\) positions in the input.\nThe point of interest is that the tokens are treated independently of each other, in a way, i.e. only one token is used to predict the next token. We’d like to let the tokens interact with each other in a way, so that the model can learn more complex patterns, with the context windows we’ve defined.\n\nclass LanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.emb_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, x, targets=None):\n        \n        logits = self.emb_table(x) # (B, T, C)\n\n        if targets is not None:\n            # Get the loss here too\n            loss = F.cross_entropy(\n                logits.view(-1, vocab_size), # (B*T, C) - predicting for each token\n                targets.view(-1)            # (B*T)   - the true target\n            )\n        else:\n            loss = None\n\n        return logits, loss\n    \n    def generate(self, ctx, max_new_tokens):\n        '''\n        Generate a sequence of new tokens given a context.\n\n        Parameters\n        ----------\n        ctx: torch.tensor (B, T)\n            The starting context to condition on.\n\n        max_new_tokens: int\n            The maximum number of new tokens to generate.\n        '''\n\n        for _ in range(max_new_tokens):\n\n            # Get the predictions\n            logits, _ = self(ctx)\n            logits = logits[:, -1, :]  # focus on the last token to predict what comes next - (B,C) now\n\n            # Normalize and sample the next token\n            probas = F.softmax(logits, dim=-1) # (B, C)\n            next_token = torch.multinomial(probas, 1) # (B, 1)\n\n            # Update context\n            ctx = torch.cat([ctx, next_token], dim=1)\n        \n        return ctx\n\n    \nmodel = LanguageModel(vocab_size)\nlogits, loss = model(x, y)\nprint(logits.shape)\nprint(loss.item())\n\ntorch.Size([4, 8, 65])\n4.483732223510742\n\n\n\n# Generate from the model with an empty context - 0 is newline char\nidx = torch.tensor([0]).view(1,1) # (B,T)\n\npreds = model.generate(idx, 10)\n\ndecode(preds[0].tolist())           # decode the first sequence in the batch\n\n'\\n.a!BmqpFup'\n\n\nLet’s improve upon this design.\nLet’s use the aforementioned Embedding table just to get the embeddings or representations for the tokens themselves. Then, we can use a simple feedforward network to predict the next token, using the embeddings of the tokens as input.\nAlongside this, we will put in an additional Embedding table to encode the position of the token in the sequence. This is because the model should be able to learn that the first token in the sequence is different from the second token, and so on.\n\nclass LanguageModelv2(nn.Module):\n\n    def __init__(self, emb_dim=32):\n\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n        self.pos_emb = nn.Embedding(ctx_len, emb_dim)\n        self.lm_head = nn.Linear(emb_dim, vocab_size)\n\n    def forward(self, x, targets=None):\n\n        B, T = x.shape\n        \n        # Get the logits (incorporating positional information too)\n        tok_emb = self.tok_emb(x)\n        pos_emb = self.pos_emb(\n            torch.arange(T, device=x.device) # (T,) - the position of each token\n        )\n        x = tok_emb + pos_emb   # (B, T, C)\n        logits = self.lm_head(x)    # (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n\n        else:\n            # Get the loss\n            loss = F.cross_entropy(\n                logits.view(-1, vocab_size), # (B*T, C) - predicting for each token\n                targets.view(-1)            # (B*T)   - the true target\n            )\n\n        return logits, loss\n    \n    def generate(self, ctx, max_new_tokens):\n        '''\n        Generate a sequence of new tokens given a context.\n\n        Parameters\n        ----------\n        ctx: torch.tensor (B, T)\n            The starting context to condition on.\n\n        max_new_tokens: int\n            The maximum number of new tokens to generate.\n        '''\n\n        for _ in range(max_new_tokens):\n\n            # Get the predictions\n            logits, _ = self(ctx)\n            logits = logits[:, -1, :]\n\n            # Normalize and sample the next token\n            probas = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probas, 1)\n\n            # Update context\n            ctx = torch.cat([ctx, next_token], dim=1)\n\n        return ctx"
  },
  {
    "objectID": "07-nanogptv2.html#the-mathematical-trick-in-self-attention",
    "href": "07-nanogptv2.html#the-mathematical-trick-in-self-attention",
    "title": "Let’s build GPT",
    "section": "The mathematical trick in Self-Attention",
    "text": "The mathematical trick in Self-Attention\nThe whole point of Self-Attention is to get the tokens interacting and talking with one another. In the previous implementation of just using the last token to predict the next one, the tokens were decoupled in a way, and incapable of interacting with each other.\nThe simplest way of getting other tokens to mingle with one another is to compute the average of the \\(t\\)-th token’s representation with all the other tokens’ representations.\nWe want to do this in a very specific way, as to only incorporate information that is not from the future, i.e. only the previous tokens in the context can be used for this aggregation.\n\nB, T, C = 4, 8, 2 # batch size, context length, channels/hidden size\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\n# Compute the new representation of each token as the average of all the previous tokens\n# Version 1: naive implementation\nxbow = torch.zeros((B, T, C))\n\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]              # grab the first t tokens in x - (t+1, C)\n        xbow[b, t] = xprev.mean(dim=0)  # average across the time dimension - (B, T, C)\n\n# Note how the first t rows of x are averaged to give the t-th row of xbow\nprint(x[0])\nprint()\nprint(xbow[0])\n\ntensor([[-0.1816,  0.1048],\n        [ 0.3941,  0.5729],\n        [ 1.5386,  0.4016],\n        [ 0.8023,  1.2386],\n        [ 0.6774, -1.2224],\n        [-1.0634,  0.3195],\n        [-0.4746,  0.6998],\n        [ 0.4340,  0.5938]])\n\ntensor([[-0.1816,  0.1048],\n        [ 0.1062,  0.3389],\n        [ 0.5837,  0.3598],\n        [ 0.6384,  0.5795],\n        [ 0.6462,  0.2191],\n        [ 0.3612,  0.2358],\n        [ 0.2418,  0.3021],\n        [ 0.2659,  0.3386]])\n\n\nNote how the first element in both matrices is the same (since it has no extra context). On top of this, the second row in the second matrix is an average of the first two rows in the first matrix, and so on. This is taking as much context as we can.\nThere is a lot of information lost when we process things this way. We can be more efficient if we use Matrix Multiplication.\nIf we can set up the matrices as being of shape \\((T, C)\\), this means creating a matrix of weights of shape \\((T, T)\\), and multiplying the two matrices together, we can get the same result as the previous method.\nThis is because each row in the weight matrix (having a uniform distribution) will be multiplied with a column of the input matrix (representing a single feature across all tokens).\nA lower-triangular matrix is perfect for this.\n\n# Version 2\nws = torch.tril(torch.ones(T, T))\nws = ws / ws.sum(-1, keepdim=True)  # normalize the ws to sum to 1 along the time dimension\nprint(ws)\n\n# ws @ x --&gt; (T, T) @ (B, T, C) --&gt; (B, T, T) @ (B, T, C) --&gt; (B, T, C)\nxavg = ws @ x\ntorch.allclose(xavg, xbow)\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\nTrue\n\n\nAn even cooler way of doing this is with the Softmax Activation Function, in that we perform it across the rows of each row in the weight matrix.\nIf we start off with a matrix of all ones, we can set up the post-softmax matrix as getting zeros in the upper region, by filling it with negative infinity in the upper region, and then applying the softmax function.\n\n# Version 3: Softmax\ntril = torch.tril(torch.ones(T, T))\n\n# Create a weights matrix that is filled with -inf in the top right corner\n# This is what measures the affinities between tokens when we perform the aggregation\nws = torch.zeros((T, T))\nws[tril == 0] = float('-inf')\nprint(ws)\n\n# Apply softmax to the ws\n# The -inf masking will say \"tokens cannot communicate with anything in the future\"\nws = F.softmax(ws, dim=-1)\nprint(ws)\n\n# Apply our matmul again\n# ws @ x --&gt; (T, T) @ (B, T, C) --&gt; (B, T, T) @ (B, T, C) --&gt; (B, T, C)\nxavg = ws @ x\ntorch.allclose(xavg, xbow)\n\ntensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., -inf],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\nTrue"
  },
  {
    "objectID": "07-nanogptv2.html#self-attention",
    "href": "07-nanogptv2.html#self-attention",
    "title": "Let’s build GPT",
    "section": "Self-Attention",
    "text": "Self-Attention\nWe can use the tricks above to implement Self Attention properly now. Instead of using the simple average with the Lower Triangular Matrix (that implements the notion of blocking out the future), we want to more towards a more sophisticated form of these affinities (rather than relying on uniform numbers). Self-Attention solves this in a data-driven way.\nThe idea is this: every token in the input sequence emits a query and a key (alongside the value). These have the following ideas behind them: * The Query says “What am I looking for/Here’s what I’m interested in…” * The Key says “What do I contain/This is what I have…” * The Value says “If you find me interesting, here’s what I will communicate to you…”\nThe way we get the affinities between tokens now is to simply take a dot product between the Query and the Key.\nThe Query for a specific token emits a certain value, representing what it’s looking for. Now, all the tokens in the input sequence emit their Keys, representing what that token is offering. If that specific token, say at position 8 (whose Query we take), finds that a token at postion 4 produces a high value when we take the dot product of the Query and Key (at positions 8 and 4 respectively), then the model has learned something meaningful about the meaning of that 8th token (new information has been aggregated, so the model has learned more about it).\n\n# Create an input matrix whose token representations we wish to refine\nB, T, C = 4, 8, 2\nx = torch.randn(B, T, C)\n\n# Create linear functions to get the query and key matrices\nhead_size = 16  # the size of the query and key matrices\n\nquery = nn.Linear(C, head_size, bias=False)\nkey = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nq = query(x)  # (B, T, head_size) - produce all the keys of the tokens independently of each other\nk = key(x)    # (B, T, head_size) - produce all the queries of the tokens independently of each other\nv = value(x)  # (B, T, head_size) - produce all the values of the tokens independently of each other\n\n# Compute the attention scores/affinities (that function as the ws from before)\n# This is where they start mingling with each other\n# q @ k.T --&gt; (B, T, head_size) @ (B, head_size, T) --&gt; (B, T, T)\nws = q @ k.transpose(-2, -1) # (B, T, T)\nprint(f\"Shape of attention weights: {ws.shape}\")\n\n# With our attention scores, now we can do the same masking+softmax procedure as before\nmask = torch.tril(torch.ones(T, T))\nws = ws.masked_fill(mask == 0, float('-inf'))\nws = F.softmax(ws, dim=-1)\n\n# Use these attention weights to aggregate the values (what each token has to offer)\nout = ws @ v  # (B, T, T) @ (B, T, head_size) --&gt; (B, T, head_size)\nprint(f\"Shape of aggregated values: {out.shape}\")\n\nShape of attention weights: torch.Size([4, 8, 8])\nShape of aggregated values: torch.Size([4, 8, 16])\n\n\n\n# Note how this time the weights are different across batches - i.e. they aren't uniform anymore\nws\n\ntensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [6.2145e-01, 3.7855e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [2.0093e-01, 4.2380e-01, 3.7527e-01, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [1.6418e-01, 3.2049e-01, 2.6588e-01, 2.4945e-01, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [1.6415e-01, 2.0907e-01, 2.3029e-01, 1.8816e-01, 2.0834e-01,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [3.4858e-01, 1.3391e-01, 8.5627e-02, 2.0435e-01, 1.3707e-01,\n          9.0452e-02, 0.0000e+00, 0.0000e+00],\n         [1.1828e-01, 1.5938e-01, 5.2063e-02, 1.5644e-01, 1.9018e-01,\n          1.1777e-01, 2.0589e-01, 0.0000e+00],\n         [2.1552e-01, 1.1294e-01, 8.1352e-02, 1.5061e-01, 1.1517e-01,\n          8.5889e-02, 3.5706e-02, 2.0280e-01]],\n\n        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [7.5342e-01, 2.4658e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [1.5640e-02, 6.6041e-01, 3.2395e-01, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [2.9742e-01, 2.3395e-01, 2.2253e-01, 2.4610e-01, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [7.3252e-02, 1.7358e-01, 4.5756e-01, 1.4860e-01, 1.4701e-01,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [1.9234e-01, 1.5837e-01, 1.5045e-01, 1.6494e-01, 1.6525e-01,\n          1.6866e-01, 0.0000e+00, 0.0000e+00],\n         [2.6379e-02, 1.9916e-01, 1.3305e-01, 1.2642e-01, 1.2426e-01,\n          1.3929e-01, 2.5144e-01, 0.0000e+00],\n         [1.6238e-01, 1.1107e-01, 1.5701e-02, 1.1294e-01, 1.1400e-01,\n          2.2634e-01, 1.7201e-01, 8.5567e-02]],\n\n        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [8.2961e-01, 1.7039e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [1.5479e-01, 1.2077e-02, 8.3313e-01, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [2.4634e-01, 1.9325e-01, 3.7714e-01, 1.8327e-01, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [8.3693e-02, 3.3405e-01, 6.0898e-02, 2.6841e-01, 2.5295e-01,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [6.5232e-02, 2.7612e-01, 4.9553e-02, 2.1677e-01, 2.1548e-01,\n          1.7684e-01, 0.0000e+00, 0.0000e+00],\n         [1.7053e-01, 1.0851e-01, 2.0595e-01, 1.1413e-01, 1.2655e-01,\n          1.2716e-01, 1.4717e-01, 0.0000e+00],\n         [1.0383e-01, 6.3937e-03, 5.8486e-01, 7.5932e-03, 2.5099e-02,\n          1.8876e-02, 1.9880e-01, 5.4548e-02]],\n\n        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [4.5356e-01, 5.4644e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [2.6181e-01, 7.3380e-01, 4.3913e-03, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [1.0160e-01, 7.3547e-02, 7.7035e-01, 5.4499e-02, 0.0000e+00,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [1.8555e-01, 2.4556e-01, 6.3503e-02, 2.6611e-01, 2.3928e-01,\n          0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [2.0220e-03, 3.1959e-04, 9.9449e-01, 2.3598e-04, 4.9655e-04,\n          2.4335e-03, 0.0000e+00, 0.0000e+00],\n         [5.9158e-02, 4.3681e-02, 4.3205e-01, 3.2226e-02, 3.4650e-02,\n          1.9151e-01, 2.0672e-01, 0.0000e+00],\n         [2.4390e-02, 6.2191e-03, 8.8002e-01, 6.4671e-03, 1.1816e-02,\n          8.5374e-03, 3.3710e-02, 2.8837e-02]]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\nSome notes: * Attention is a communication mechanism in that it allows nodes in any directed graph to aggregate information from other nodes, specifically those that point to it. We can think of text as being a specific type of directed graph where the first token only points to itself, the second is being pointed to by the first (and itself), and so on, until the last token is being pointed to by all the previous nodes, and itself.\n\nAttention has no notion of space, which is the reason why we implement the Positional Embeddings that we’ll see later.\nEach instance in the batch is processed independently of one another. So if we have 4 batches each having a max context length of 8, then we really have a total of 32 nodes in our graph. But because each batch is processed independently, the nodes in one batch don’t communicate with the nodes in another batch - i.e. there are no edges between nodes of different batches.\nIn an Encoder, if we don’t want to restrict the tokens talking to the future tokens, we just get rid of the masking portion of the code with the lower triangular matrix. In the Decoder though, when we are trying to generate new tokens, we don’t want the future tokens communicating with the past tokens (otherwise they’d be giving away the answer, on top of this interaction not even being possible), which is why we’d bring the masking feature in there.\n“Self-Attention” is specifically Attention when the Keys, Queries and Values are coming from the same source. Attention is more general than that: “Cross Attention” is when the Keys and the Values are coming from an external source outside the source of the Keys.\n\nSide note: why is it we divide by \\(\\sqrt{d_k}\\) in computing the attention scores?\nWe find that if we don’t do this, then the result of the pre-softmax Attention Scores incur a very high variance, on the order of \\(d_k\\). When they incur this high variance, this means some values are highly positive while others are much smaller - this means that post-Softmax, we may find a vector that is similar to a one-hot vector (which means that each token will only aggregate information from a single node only).\n\n# Instantiate query and keys as being unit-gaussian\nq = torch.randn(B, T, 16)\nk = torch.randn(B, T, 16)\n\nprint(f\"Variance of query: {q.var()}\")\nprint(f\"Variance of key: {k.var()}\")\nprint(f\"Variance of Q @ K.T: {(q @ k.transpose(-2, -1)).var()}\")\n\n# Note how the variance of the dot prod is on the order of head size\n# We divide by that amount to keep the variance of the dot prod at 1\nws = q @ k.transpose(-2, -1)\nws = ws / (16**0.5)\nprint(f\"Variance of (Q @ K.T)/sqrt(head_size): {ws.var()}\")\n\nVariance of query: 0.9703316688537598\nVariance of key: 0.9714754819869995\nVariance of Q @ K.T: 13.188318252563477\nVariance of (Q @ K.T)/sqrt(head_size): 0.8242698907852173"
  },
  {
    "objectID": "07-nanogptv2.html#self-attention-1",
    "href": "07-nanogptv2.html#self-attention-1",
    "title": "Let’s build GPT",
    "section": "Self-Attention",
    "text": "Self-Attention\nWe start off by incorporating this mechanism as its own module.\nThe procedure is the following:\n\nTake the input embeddings as a (B, T, C) matrix.\nExtract the Query, Key, and Value matrices from it.\nCompute the scaled Attention Scores, with the dot product and constant factor.\nApply the masking to the upper triangular region, filling in with -inf, before passing through a Softmax.\nTake the matmul with the Value matrices.\n\n\nemb_dim = head_size\n\nclass Head(nn.Module):\n\n    def __init__(self, emb_dim, head_size):\n        super().__init__()\n\n        self.query = nn.Linear(emb_dim, head_size, bias=False)\n        self.key = nn.Linear(emb_dim, head_size, bias=False)\n        self.value = nn.Linear(emb_dim, head_size, bias=False)\n\n        # Register a buffer that we use for masking (Decoder structure)\n        self.register_buffer(\n            \"tril\",\n            torch.tril(torch.ones((ctx_len, ctx_len)))  # same thing as before\n        )\n\n    def forward(self, x):\n\n        B, T, C = x.shape\n\n        # Get the qkv matrices\n        q = self.query(x)   # (B, T, C)\n        k = self.key(x)     # (B, T, C)\n        v = self.value(x)   # (B, T, C)\n\n        # Compute the (scaled) Attention Scores\n        # (B, T, C) @ (B, C, T) --&gt; (B, T, T)\n        att_scores = q @ k.transpose(-2, -1) * (C ** -0.5)\n\n        # Perform the masking\n        att_scores = att_scores.masked_fill(\n            self.tril[:T, :T] == 0,             # be careful if T &lt; ctx_len\n            float('-inf')\n        )\n        att_scores = F.softmax(att_scores, dim=-1)\n\n        # Aggregate with the value vectors\n        # (B, T, T) @ (B, T, C) --&gt; (B, T, T)\n        out = att_scores @ v\n        \n        return out\n    \nx = torch.randn(4, 8, 2)\nsa_head = Head(2, 2)\nout = sa_head(x)\nprint(f\"{x.shape} ---&gt; {out.shape}\")\n\ntorch.Size([4, 8, 2]) ---&gt; torch.Size([4, 8, 2])\n\n\nNow that we’ve made this class, we can incorporate it into our Language Model implementation.\nThis is just one additional line of code, where we pass our embedded inputs, with positional encodings, into this module - this just refines the representations, making no changes to the actual shapes (provided we let emb_dim == head_size).\n\nclass LanguageModelv3(nn.Module):\n\n    def __init__(self, emb_dim=32):\n\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n        self.pos_emb = nn.Embedding(ctx_len, emb_dim)\n        self.sa_head = Head(emb_dim, head_size)\n        self.lm_head = nn.Linear(emb_dim, vocab_size)\n\n    def forward(self, x, targets=None):\n\n        B, T = x.shape\n        \n        # Get the logits (incorporating positional information too)\n        tok_emb = self.tok_emb(x)\n        pos_emb = self.pos_emb(\n            torch.arange(T, device=x.device) # (T,) - the position of each token\n        )\n        x = tok_emb + pos_emb   # (B, T, C)\n\n        # New line here: refine the representations with Self-Attention\n        x = self.sa_head(x)\n        # END\n\n        logits = self.lm_head(x)    # (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n\n        else:\n            # Get the loss\n            loss = F.cross_entropy(\n                logits.view(-1, vocab_size), # (B*T, C) - predicting for each token\n                targets.view(-1)            # (B*T)   - the true target\n            )\n\n        return logits, loss\n    \n    def generate(self, ctx, max_new_tokens):\n        '''\n        Generate a sequence of new tokens given a context.\n\n        Parameters\n        ----------\n        ctx: torch.tensor (B, T)\n            The starting context to condition on.\n\n        max_new_tokens: int\n            The maximum number of new tokens to generate.\n        '''\n\n        for _ in range(max_new_tokens):\n\n            # Crop to the last ctx_len tokens\n            idx_cond = idx[:, -ctx_len:]\n\n            # Get the predictions\n            logits, _ = self(ctx)\n            logits = logits[:, -1, :]\n\n            # Normalize and sample the next token\n            probas = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probas, 1)\n\n            # Update context\n            ctx = torch.cat([ctx, next_token], dim=1)\n\n        return ctx"
  },
  {
    "objectID": "07-nanogptv2.html#multihead-self-attention",
    "href": "07-nanogptv2.html#multihead-self-attention",
    "title": "Let’s build GPT",
    "section": "MultiHead Self Attention",
    "text": "MultiHead Self Attention\nTo allow for more independent communication channels, which means allowing for tokens to take on more than just one combination of context clues, we allow for the Transformer to use multiple of these Self Attention Heads.\nNote however that we change up the dimension of the Head Size, since at the end, we want the output to stay the same, after we concatenate information from all the heads.\nNote that the processing for each head will run in parallel so we suffer no cost in terms of time.\n\nclass MultiHeadSelfAttention(nn.Module):\n\n    def __init__(self, emb_dim, num_heads, head_size):\n\n        super().__init__()\n\n        self.heads = nn.ModuleList([\n            Head(emb_dim, head_size) for _ in range(num_heads)\n        ])\n\n    def forward(self, x):\n        return torch.cat([\n            head(x) for head in self.heads      # compute output for each head in parallel\n        ], dim=-1)                              # concatenate along the C channel\n\nx = torch.randn(4, 8, 32)\nmhsa = MultiHeadSelfAttention(emb_dim=32, num_heads=4, head_size=32//4)\nout = mhsa(x)\nprint(f\"{x.shape} ---&gt; {out.shape}\")\n\ntorch.Size([4, 8, 32]) ---&gt; torch.Size([4, 8, 32])\n\n\nWhile we created this using our single Head class, we could have also done this in one go.\nThe following snippet is by Copilot:\nclass MultiHeadSelfAttentionv0(nn.Module):\n    def __init__(self, emb_dim, num_heads):\n        super().__init__()\n        self.emb_dim = emb_dim\n        self.num_heads = num_heads\n        self.head_dim = emb_dim // num_heads\n        \n        self.query = nn.Linear(emb_dim, emb_dim)\n        self.key = nn.Linear(emb_dim, emb_dim)\n        self.value = nn.Linear(emb_dim, emb_dim)\n        self.fc = nn.Linear(emb_dim, emb_dim)\n        \n    def forward(self, x):\n        B, T, C = x.size()\n        \n        queries = self.query(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n        keys = self.key(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n        values = self.value(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n        \n        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, T, T)\n        attention_probs = F.softmax(attention_scores, dim=-1)  # (B, num_heads, T, T)\n        \n        attended_values = torch.matmul(attention_probs, values)  # (B, num_heads, T, head_dim)\n        attended_values = attended_values.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, emb_dim)\n        \n        output = self.fc(attended_values)  # (B, T, emb_dim)\n        \n        return output"
  },
  {
    "objectID": "07-nanogptv2.html#feedforward-layers",
    "href": "07-nanogptv2.html#feedforward-layers",
    "title": "Let’s build GPT",
    "section": "Feedforward Layers",
    "text": "Feedforward Layers\nIn the paper, the authors did not immediately feed in the processed inputs to a classification layer, rather there were intermediate FeedForward layers that allowed for more of this intermediate processing of activations.\nThis can be thought of as Self-Attention and the multiple heads aggregating that information, and the Feedforward layers allowing these tokens to think on this new aggregated information.\n\nclass FeedForward(nn.Module):\n\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.ff = nn.Sequential(\n            nn.Linear(emb_dim, 4*emb_dim),\n            nn.ReLU(),\n            nn.Linear(4*emb_dim, emb_dim),\n        )\n    \n    def forward(self, x):\n        return self.ff(x)"
  },
  {
    "objectID": "07-nanogptv2.html#blocks-residual-connections-and-layer-normalization",
    "href": "07-nanogptv2.html#blocks-residual-connections-and-layer-normalization",
    "title": "Let’s build GPT",
    "section": "Blocks, Residual Connections and Layer Normalization",
    "text": "Blocks, Residual Connections and Layer Normalization\nThe Transformer can be divided blocks that are further segmented into two separate components:\n\nthe MHSA that performs the communication\nthe Feedforward layers that perform the computation\n\nEach block consists of the MHSA module, followed by the Feedforward network on all the tokens independently.\n\nclass Block(nn.Module):\n\n    def __init__(self, emb_dim, num_heads):\n\n        super().__init__()\n        self.head_size = emb_dim // num_heads\n        self.mhsa = MultiHeadSelfAttention(emb_dim, num_heads, self.head_size)\n        self.ff = FeedForward(emb_dim)\n\n    def forward(self, x):\n\n        x = self.mhsa(x)\n        x = self.ff(x)\n        return x\n\nNow when we start to create a lot more blocks in the architecture, we find that this model ends up becoming rather deep.\nThis could pose certain issues regarding the optimization of the parameters.\nThe authors of the paper utilize two approaches that aim to resolve these optimization issues.\nFirst we have (1) Skip/Residual Connections that distributes gradients equally along both a residual pathway (going unimpeded from the inputs to the outputs) - we fork off, do some computation, then come back. We have to implement these skip connections (via addition) and projections in a few of our classes.\nThe other improvement we have is (2) Layer Norm. This is very similar to Batch Normalization, except that now, instead of normalizing along the columns, we normalize along the rows (for every single example).\nWe don’t need any distinction between training and test time, we don’t require any buffers for the running mean and variances, we can apply this simpler algorithm any time we wish with no previous state (except the parameters).\nSlight deviation from the initial paper: the LayerNorm is applied before each of the transformations, rather than after.\n\n# Write the old classes here again\n\nclass Head(nn.Module):\n\n    def __init__(self, head_size, emb_dim=32, ctx_len=8):\n        super().__init__()\n        \n        self.key = nn.Linear(emb_dim, head_size, bias=False)\n        self.query = nn.Linear(emb_dim, head_size, bias=False)\n        self.value = nn.Linear(emb_dim, head_size, bias=False)\n\n        self.register_buffer('tril', torch.tril(torch.ones(ctx_len, ctx_len)))\n\n    def forward(self, x):\n\n        B,T,C = x.shape # batch size, num tokens, embedding dim\n\n        k = self.key(x)\n        q = self.query(x)\n\n        weights = q @ (k.transpose(-1, -2)) / (C**0.5)\n        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # decoder block\n        weights = F.softmax(weights, dim=-1)\n\n        v = self.value(x)\n        out = weights @ v\n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, emb_dim, num_heads, ctx_len):\n        super().__init__()\n        head_size = emb_dim // num_heads\n        self.heads = nn.ModuleList([Head(head_size, emb_dim=emb_dim, ctx_len=ctx_len) for _ in range(num_heads)])\n\n        # NEW: add in a projection layer to let the model \"think\" on the aggregated information even more\n        self.proj = nn.Linear(emb_dim, emb_dim) # project back to original embedding dim\n        # -----\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # output of raw MHSA: (B, T, C)\n        out = self.proj(out) # projection back to original embedding dim\n        return out\n\nclass LayerNorm:\n\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        \n        # Normalize the rows for each example\n        xmean = x.mean(dim=1, keepdim=True)\n        xvar = x.var(dim=1, keepdim=True)\n        xhat = (x-xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n        \n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\nclass Feedforward(nn.Module):\n\n    def __init__(self, emb_dim):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(emb_dim, 4*emb_dim), # small change: 4x bigger hidden layer (from the paper)\n            nn.ReLU(),\n            nn.Linear(4*emb_dim, emb_dim), # projection back into residual pathway\n            nn.Dropout(0.2), # NEW: dropout\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nNow we bring all of this together to form one Block, which contains the MHSA module, the FF net, the LayerNorm modules, and the skip connections.\nNote the output shape of this: its still really just refining the input representations.\n\nclass Block(nn.Module):\n\n    def __init__(self, emb_dim, num_heads, ctx_len):\n        super().__init__()\n        \n        self.sa_heads = MultiHeadAttention(emb_dim=emb_dim, num_heads=num_heads, ctx_len=ctx_len)\n        \n        self.ffwd = Feedforward(emb_dim=emb_dim)\n        self.ln1 = nn.LayerNorm(emb_dim) # for each MHSA layer\n        self.ln2 = nn.LayerNorm(emb_dim) # for the feedforward layer\n\n    def forward(self, x):\n        # NEW: Add in the skip connections, and the LayerNorms\n        x = x + self.sa_heads(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n    \nx = torch.randn(64, 256, 384)\nblock = Block(384, 8, 256)\nout = block(x)\nprint(f\"{x.shape} ---&gt; {out.shape}\")\n\ntorch.Size([64, 256, 384]) ---&gt; torch.Size([64, 256, 384])"
  },
  {
    "objectID": "07-nanogptv2.html#the-final-transformer-decoder",
    "href": "07-nanogptv2.html#the-final-transformer-decoder",
    "title": "Let’s build GPT",
    "section": "The final Transformer Decoder",
    "text": "The final Transformer Decoder\nNow we can take all of these pieces to make our complete Transformer Decoder.\nThe hyperparameters to take note of here are:\n\nnum_layers\nnum_heads\nemb_dim\nctx_len\n\n\nclass GPT(nn.Module):\n\n    def __init__(self, num_layers, num_heads, emb_dim, ctx_len):\n\n        super().__init__()\n\n        # Create the two embedding tables\n        self.tok_emb_table = nn.Embedding(vocab_size, emb_dim)\n        self.pos_emb_table = nn.Embedding(ctx_len, emb_dim)\n\n        # Create the blocks for communication + computation\n        self.blocks = nn.Sequential(*[Block(emb_dim, num_heads, ctx_len) for _ in range(num_layers)])\n        self.ln_final = nn.LayerNorm(emb_dim) # for right after all the blocks\n\n        self.lm_head = nn.Linear(emb_dim, vocab_size)\n\n    def forward(self, x, targets=None):\n\n        B, T = x.shape\n\n        # Create the inputs for the actual Decoder module\n        tok_embs = self.tok_emb_table(x)                    # (B, T, C)\n        pos_embs = self.pos_emb_table(torch.arange(T))      # (T, C)\n        x = tok_embs + pos_embs                             # (B, T, C)\n\n        # Pass through the blocks, and the final LayerNorm\n        x = self.blocks(x)                                  # (B, T, C)\n        x = self.ln_final(x)                                # (B, T, C)\n\n        # Get the logits\n        logits = self.lm_head(x)                                 # (B, T, V)\n\n        # Ready the loss\n        if targets is None:\n            loss = None\n        else:\n            B, T, V = logits.shape\n            loss = F.cross_entropy(\n                logits.view(B*T, V),\n                targets.view(-1)\n            )\n        \n        return logits, loss\n    \n    def generate(self, idxs, max_new_tokens):\n        \n        # idxs is a B,T tensor of token indices\n        for _ in range(max_new_tokens):\n\n            # Crop the input to be some max length\n            idxs_cropped = idxs[:, -ctx_len:]\n\n            logits, _ = self(idxs_cropped) # forward pass\n            logits = logits[:, -1, :] # focus only on last token (Bigram model)\n            probs = F.softmax(logits, dim=-1) # get probabilities\n\n            next_idx = torch.multinomial(probs, num_samples=1) # sample from this distribution\n\n            idxs = torch.cat((idxs, next_idx), dim=1) # (B, T) -&gt; (B, T+1) append to the right\n\n        return idxs\n\nmodel = GPT(\n    num_layers=3, \n    num_heads=8, \n    emb_dim=384, \n    ctx_len=256\n)\n\nprint(f\"The model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n\nThe model has 5,468,993 parameters\n\n\n\n# Check whether the model is able to generate something\ncontext = torch.zeros((1,1), dtype=torch.long)\nmodel_gen = model.generate(context, max_new_tokens=500)[0].tolist()\nprint(decode(model_gen))\n\n\nPDf--apdsNfKJlmlmQZTtOoe?DXhwpvZy3\nJ:KKRPO!gdFykhdCQUMccBmSKoUeXnOdnxhGB:LGrYqCnAfqOpnfhj:xHPBfI,$3!VgZTdCrsI bjXAH?J:ZMdNRy:g$tGMbqLj?YA-XQBSbZASiXlua\n.$.ILVr\nd$NCHKjvkoqu,xmx,v:3CYz?dv!xj3'DZgvsx?C.Igf;laF HO!vpMf wjK&BRk-.\njxiZ'hWjsBc'y3GmGKxvIRc:3:.AVF,D\n.AV3Wv?GVAO3-ddv-\n?L:w:sDLEE\nBwGj$gdr3XNgzolYZskdj$\nxp xt\n,KqKEO?;MzVEjK!N3-cdZmLRvsBC-h:LqvjKPJZrHXwrtHE$&ZeOskmT$H:dfq h'xzwKESg$mgJY\nAOXilyuXmkR$voBt3HMVat':QRFedYWrk$ S XNJa.wq:p\n\n-esrmFbEv'$gk&-R\nojEBB:so\nTLXEJr$Z:'zDfc:JiPJ;Nq--G&eLZOz\n\n\nBefore we finish, let’s create some utility functions to kick off training and evaluation for our model.\n\n@torch.no_grad()\ndef estimate_loss(split=\"val\", eval_iters=200):\n    out = {}\n    model.eval()\n\n    losses = torch.zeros(eval_iters)\n\n    for k in range(eval_iters): # find the average loss over 200 batches\n        \n        X, Y = get_batch(split)\n        logits, loss = model(X, Y)\n        losses[k] = loss.item()\n        \n    model.train()\n    return losses.mean().item()\n\ndef train_model(lr=1e-3, max_epochs=100, eval_interval=10):\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n    for epoch in range(max_epochs):\n\n        if epoch % eval_interval == 0 or epoch == max_epochs-1:\n            losses = estimate_loss()\n            print(f\"Epoch {epoch}: train loss {losses['train']:.3f}, val loss {losses['val']:.3f}\")\n\n        xb, yb = get_batch()\n\n        # Evaluate loss\n        logits, loss = model(xb, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()"
  },
  {
    "objectID": "posts/demo/index.html",
    "href": "posts/demo/index.html",
    "title": "First poost",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "07-nanogpt.html",
    "href": "07-nanogpt.html",
    "title": "Tokenization",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nwith open('./input.txt') as f:\n    text = f.read()\n\nprint(f'Length of text: {len(text)} characters')\nprint('-'*25)\nprint(f\"The first 100 characters:\\n...\\n{text[:100]}\")\n\nLength of text: 1115394 characters\n-------------------------\nThe first 100 characters:\n...\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n# Get all the unique characters from this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(f'Vocabulary size: {vocab_size} characters')\nprint(''.join(chars))\n\nVocabulary size: 65 characters\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\n\n# Encoding and decoding functions\nencode = lambda string: [stoi[s] for s in string]\ndecode = lambda tokens: ''.join([itos[t] for t in tokens])\n\n# Small example\nprint(encode(\"Hello world!\"))\nprint(decode(encode(\"mii gustaa\")))\n\n[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\nmii gustaa\n# Numericalize the whole dataset\ndata = torch.tensor(encode(text), dtype=torch.long)\ndata.shape\n\ntorch.Size([1115394])\n# Make train and dev splits\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\nblock_size = 8\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\n\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f'Context: {context.numpy()} -&gt; Target: {target.item()}')\n\nContext: [18] -&gt; Target: 47\nContext: [18 47] -&gt; Target: 56\nContext: [18 47 56] -&gt; Target: 57\nContext: [18 47 56 57] -&gt; Target: 58\nContext: [18 47 56 57 58] -&gt; Target: 1\nContext: [18 47 56 57 58  1] -&gt; Target: 15\nContext: [18 47 56 57 58  1 15] -&gt; Target: 47\nContext: [18 47 56 57 58  1 15 47] -&gt; Target: 58\ntorch.manual_seed(1337)\nbatch_size = 4 # max number of examples we process in parallel\nblock_size = 8 # max context length for predictions\n\ndef get_batch(split):\n    data = train_data if split == \"train\" else val_data\n    idx = torch.randint(len(data)-block_size, (batch_size,))\n\n    # Stack these rows in a 4x8 tensor\n    x = torch.stack([data[i:i+block_size] for i in idx])\n    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n    return x, y\n\nxb, yb = get_batch(\"train\")\nprint(xb.shape, yb.shape)\n\nfor b in range(1):\n    for t in range(block_size):\n        context = xb[b, :t+1]\n        target = yb[b, t]\n        print(f'Context: {context.tolist()} -&gt; Target: {target.item()}')\n\ntorch.Size([4, 8]) torch.Size([4, 8])\nContext: [24] -&gt; Target: 43\nContext: [24, 43] -&gt; Target: 58\nContext: [24, 43, 58] -&gt; Target: 5\nContext: [24, 43, 58, 5] -&gt; Target: 57\nContext: [24, 43, 58, 5, 57] -&gt; Target: 1\nContext: [24, 43, 58, 5, 57, 1] -&gt; Target: 46\nContext: [24, 43, 58, 5, 57, 1, 46] -&gt; Target: 43\nContext: [24, 43, 58, 5, 57, 1, 46, 43] -&gt; Target: 39"
  },
  {
    "objectID": "07-nanogpt.html#simple-bigram-language-model",
    "href": "07-nanogpt.html#simple-bigram-language-model",
    "title": "Tokenization",
    "section": "Simple Bigram Language Model",
    "text": "Simple Bigram Language Model\n\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx, targets: (batch_size, block_size)\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        # F.cross_entropy expects channels last (synonymous with embedding dim)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n\n        for _ in range(max_new_tokens):\n\n            # Get predictions\n            logits, loss = self(idx)\n\n            # Focus only on last time step\n            logits = logits[:, -1, :] # (B,T,C) -&gt; (B, C)\n\n            # Apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n\n            # Sample from this distribution to get new token\n            new_token = torch.multinomial(probs, num_samples=1) # (B, 1) single predictions\n\n            idx = torch.cat([idx, new_token], dim=-1) # (B, T+1)\n\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape, loss.item())\n\n# Sample from the model\nidx = torch.zeros((1,1), dtype=torch.long)\nmodel_gen = m.generate(idx, max_new_tokens=100)[0].tolist()\nprint(decode(model_gen))\n\ntorch.Size([256, 65]) 4.658127307891846\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n# Training the bigram model\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nmax_epochs = 10_000\nfor epoch in range(max_epochs):\n\n    xb, yb = get_batch(\"train\")\n\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % (max_epochs//10) == 0 or epoch==(max_epochs-1):\n        print(f'Step: {epoch}, Loss: {loss.item()}')\n\nStep: 0, Loss: 4.692410945892334\nStep: 1000, Loss: 3.7637593746185303\nStep: 2000, Loss: 3.2342259883880615\nStep: 3000, Loss: 2.892245292663574\nStep: 4000, Loss: 2.703908681869507\nStep: 5000, Loss: 2.5153486728668213\nStep: 6000, Loss: 2.4889943599700928\nStep: 7000, Loss: 2.514069080352783\nStep: 8000, Loss: 2.444497585296631\nStep: 9000, Loss: 2.3975775241851807\nStep: 9999, Loss: 2.382369041442871\n\n\n\nidx = torch.zeros((1,1), dtype=torch.long)\nmodel_gen = m.generate(idx, max_new_tokens=100)[0].tolist()\nprint(decode(model_gen))\n\n\nlso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulsee"
  },
  {
    "objectID": "07-nanogpt.html#the-mathematical-trick-in-self-attention",
    "href": "07-nanogpt.html#the-mathematical-trick-in-self-attention",
    "title": "Tokenization",
    "section": "The Mathematical Trick in Self-Attention",
    "text": "The Mathematical Trick in Self-Attention\nIn the above language model, the tokens weren’t really “speaking with one another”, i.e. they were decoupled and being processed independently of the others. Now we want to change this, to have them interact with one another but in a very specific way.\nWe want to use the previous time-steps (or previous tokens) as context to try and predict the future. In this event, we don’t want token 5 for example to be able to use tokens 6, 7, 8 for its processing. Information from previous time-steps should only be used in processing/predicting the current token.\nThe easiest way for tokens to communicate is to simply take an average of the previous tokens in some way. This is a weak form of interaction, but it serves as a way for the current token to be defined in terms of the context surrounding it (preceding it more specifically).\n\n# A toy example\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2 # batch size, time step (num tokens), channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\nxbow = torch.zeros((B, T, C))\n\n# 1st version: for loops\nfor b in range(B): # iterate through all the instances\n    for t in range(T): # iterate through the time steps\n        xprev = x[b, :t+1] # only take the previous tokens\n        xbow[b,t] = torch.mean(xprev, 0) # average over the tokens\n\n# Compare the BOW representation and the original embeddings\nprint(\"Original embeddings:\")\nprint(x[0])\nprint()\nprint(\"BOW embeddings:\")\nprint(xbow[0])\n\nOriginal embeddings:\ntensor([[ 0.1808, -0.0700],\n        [-0.3596, -0.9152],\n        [ 0.6258,  0.0255],\n        [ 0.9545,  0.0643],\n        [ 0.3612,  1.1679],\n        [-1.3499, -0.5102],\n        [ 0.2360, -0.2398],\n        [-0.9211,  1.5433]])\n\nBOW embeddings:\ntensor([[ 0.1808, -0.0700],\n        [-0.0894, -0.4926],\n        [ 0.1490, -0.3199],\n        [ 0.3504, -0.2238],\n        [ 0.3525,  0.0545],\n        [ 0.0688, -0.0396],\n        [ 0.0927, -0.0682],\n        [-0.0341,  0.1332]])\n\n\nNote how the first element in both matrices is the same (since it has no extra context). On top of this, the second row in the second matrix is an average of the first two rows in the first matrix, and so on. This is taking as much context as we can.\nThere is a lot of information lost when we process things this way. We can be more efficient if we use Matrix Multiplication: a Triangular Matrix is perfect for our use since that has an extra non-zero element in each succeeding row.\n\ntorch.manual_seed(42)\n\n# Use a triangular matrix for a mask\na = torch.tril(input=torch.ones(3,3))\na = a / torch.sum(a, dim=1, keepdim=True) # normalize values to implement an average\n\nb = torch.randint(0, 10, (3, 2)).float()\n\nc = a @ b\n\nprint(a)\nprint('-'*10)\nprint(b)\nprint('-'*10)\nprint(c)\n\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n----------\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n----------\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\nNote how the first element not having any additional context retains its row vector. The next row is an average of the first two row vectors (the previous one and itself). The third vector is an average of all the rows in the input.\nWe can consider these normalized values to be weights, that are initialized to give the same importance to every element that can be used for a given element.\n\n# 2nd version: Now to vectorize the original BOW model\nweights = torch.tril(input=torch.ones(T, T)) # square matrix of num_tokens x num_tokens\nweights = weights / weights.sum(dim=1, keepdim=True)\nxbow2 = weights @ x # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n\ntorch.allclose(xbow, xbow2)\n\nTrue\n\n\nAnother way we can do this is to use the Softmax activation.\nIf we want a zero somewhere, we can recall that the exponential of neg. inf. is 0 (so we fill in the zeros for the weights with neg. inf.). Then if a row has the same elements (like all ones), then Softmax will average the values out itself.\n\n# 3rd version: use Softmax\ntril = torch.tril(torch.ones(T, T))\n\n# Initialize this \"affinity\" matrix\nweights = torch.zeros((T, T))\nprint(weights, '\\n')\n\n# Block out the tokens from the future\nweights = weights.masked_fill(tril == 0, float('-inf'))\nprint(weights, '\\n')\n\n# Normalize the values to 1 (for a proper weighted sum)\nweights = F.softmax(weights, dim=-1)\nprint(weights, '\\n')\n\nxbow3 = weights @ x\n\ntorch.allclose(xbow3, xbow)\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.]]) \n\ntensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., -inf],\n        [0., 0., 0., 0., 0., 0., 0., 0.]]) \n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]) \n\n\n\nTrue"
  },
  {
    "objectID": "07-nanogpt.html#self-attention",
    "href": "07-nanogpt.html#self-attention",
    "title": "Tokenization",
    "section": "Self Attention",
    "text": "Self Attention\nWe can use the tricks above to implement Self Attention properly now. Instead of using the simple average with the Lower Triangular Matrix (that implements the notion of blocking out the future), we want to more towards a more sophisticated form of these affinities (rather than relying on uniform numbers). Self-Attention solves this in a data-driven way.\nThe idea is this: every token in the input sequence emits a query and a key (alongside the value). These have the following ideas behind them: * The Query says “What am I looking for/Here’s what I’m interested in…” * The Key says “What do I contain/This is what I have…” * The Value says “If you find me interesting, here’s what I will communicate to you…”\nThe way we get the affinities between tokens now is to simply take a dot product between the Query and the Key.\nThe Query for a specific token emits a certain value, representing what it’s looking for. Now, all the tokens in the input sequence emit their Keys, representing what that token is offering. If that specific token, say at position 8 (whose Query we take), finds that a token at postion 4 produces a high value when we take the dot product of the Query and Key (at positions 8 and 4 respectively), then the model has learned something meaningful about the meaning of that 8th token (new information has been aggregated, so the model has learned more about it).\n\n# 4th version: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32\nx = torch.randn(B, T, C)\n\n# Implementing a single head for Self Attention\nhead_size = 16 # call this H\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x) # (B, T, C) -&gt; (B, T, H)\nq = query(x) # (B, T, C) -&gt; (B, T, H)\n\nweights = q @ k.transpose(-1, -2) # (B, T, H) @ (B, H, T) -&gt; (B, T, T)\nweights *= (head_size)**-0.5 # scale by sqrt(d_k) for smoother softmax output\n\n# Masking and normalization\ntril = torch.tril(torch.ones(T, T))\nweights = weights.masked_fill(tril == 0, float('-inf')) # specifically for the Decoder\nweights = F.softmax(weights, dim=-1)\n\nv = value(x)\nout = weights @ v # attention weighted input\n\nprint(\"Attention weights for zeroth instance\\n\", weights[0], '\\n')\nprint(\"Final output shape\", out.shape)\n\nAttention weights for zeroth instance\n tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n       grad_fn=&lt;SelectBackward0&gt;) \n\nFinal output shape torch.Size([4, 8, 16])\n\n\nSome notes: 1. Attention is a communication mechanism in that it allows nodes in any directed graph to aggregate information from other nodes, specifically those that point to it. We can think of text as being a specific type of directed graph where the first token only points to itself, the second is being pointed to by the first (and itself), and so on, until the last token is being pointed to by all the previous nodes, and itself. 2. Attention has no notion of space, which is the reason why we implement the Positional Embeddings that we’ll see later. 3. Each instance in the batch is processed independently of one another. So if there are 32 sequences/examples in the batch, then we’re really looking at 32 different pools/components of joined nodes. This means that one sequence cannot take any clues or talk to other examples in that mini-batch. 4. In an Encoder, if we don’t want to restrict the tokens talking to the future tokens, we just get rid of the masking portion of the code with the lower triangular matrix. In the Decoder though, when we are trying to generate new tokens, we don’t want the future tokens communicating with the past tokens (otherwise they’d be giving away the answer, on top of this interaction not even being possible), which is why we’d bring the masking feature in there. 5. “Self-Attention” is specifically Attention when the Keys, Queries and Values are coming from the same source. Attention is more general than that: “Cross Attention” is when the Keys and the Values are coming from an external source outside the source of the Keys.\nNow to make this a proper class!\n\nclass Head(nn.Module):\n\n    def __init__(self, head_size, emb_dim=32, block_size=8):\n        super().__init__()\n        \n        self.key = nn.Linear(emb_dim, head_size, bias=False)\n        self.query = nn.Linear(emb_dim, head_size, bias=False)\n        self.value = nn.Linear(emb_dim, head_size, bias=False)\n\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n\n        B,T,C = x.shape # batch size, num tokens, embedding dim\n\n        k = self.key(x)\n        q = self.query(x)\n\n        weights = q @ (k.transpose(-1, -2)) / (C**0.5)\n        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # decoder block\n        weights = F.softmax(weights, dim=-1)\n\n        v = self.value(x)\n        out = weights @ v\n        return out\n    \n\nclass SABigramLanguageModel(nn.Module):\n    '''\n    Small Bigram language model using Self Attention\n    '''\n\n    def __init__(self, vocab_size=vocab_size, emb_dim=32):\n        super().__init__()\n\n        self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n\n        self.sa_head = Head(head_size=emb_dim)\n        self.lm_head = nn.Linear(emb_dim, vocab_size) # for prediction\n\n    def forward(self, idx, targets=None):\n\n        B, T = idx.shape # batch size, num tokens\n\n        tok_embs = self.token_embedding_table(idx) # (B, T, C)\n        pos_embs = self.position_embedding_table(torch.arange(T)) # (T, C)\n\n        x = tok_embs + pos_embs # (B, T, C) - right aligned broadcasting okay\n        x = self.sa_head(x) # only one head for now\n        logits = self.lm_head(x) # (B, T, C) -&gt; (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n\n        for _ in range(max_new_tokens):\n\n            # Crop idx to be maximum block_size long\n            idx_cropped = idx[:, -block_size:]\n\n            logits, _ = self(idx_cropped) # forward pass\n            logits = logits[:, -1, :] # focus only on last token\n            probs = F.softmax(logits, dim=-1) # get probabilities\n            idx_next = torch.multinomial(probs, num_samples=1) # sample from this distribution\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T) -&gt; (B, T+1) append to the right\n\n        return idx\n\nRecall that there is the notion of Positional Embedding that is “added” to the Embeddings of the inputs.\nThe Positional Embedding can be initialized as an Embedding table of size (block_size, pos_emb_dim), and the actual Token Embeddings can be initialized as an Embedding table of size (vocab_size, emb_dim).\nThe reasoning for the values at axis 0 is self explanatory."
  },
  {
    "objectID": "07-nanogpt.html#multi-headed-self-attention",
    "href": "07-nanogpt.html#multi-headed-self-attention",
    "title": "Tokenization",
    "section": "Multi-Headed Self Attention",
    "text": "Multi-Headed Self Attention\nTo allow for more independent communication channels, which means allowing for tokens to take on more than just one combination of context clues, we allow for the Transformer to use multiple of these Self Attention Heads.\nNote however that we change up the dimension of the Head Size, since at the end, we want the output to stay the same, after we concatenate information from all the heads.\nNote that the processing for each head will run in parallel so we suffer no cost in terms of time.\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)"
  },
  {
    "objectID": "07-nanogpt.html#feedforward-layers",
    "href": "07-nanogpt.html#feedforward-layers",
    "title": "Tokenization",
    "section": "Feedforward Layers",
    "text": "Feedforward Layers\nIn the paper, the authors did not immediately feed in the processed inputs to a classification layer, rather there were intermediate FeedForward layers that allowed for more of this intermediate processing of activations.\nThis can be thought of as Self-Attention and the multiple heads aggregating that information, and the Feedforward layers allowing these tokens to think on this new aggregated information.\n\nclass Feedforward(nn.Module):\n\n    def __init__(self, emb_dim=32):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(emb_dim, emb_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass SABigramLanguageModel(nn.Module):\n    '''\n    Small Bigram language model using Self Attention, now with multiple heads and a Feedforward layer\n    '''\n\n    def __init__(self, vocab_size=vocab_size, emb_dim=32):\n        super().__init__()\n\n        self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n\n        self.sa_heads = MultiHeadAttention(num_heads=4, head_size=emb_dim//4) # 4 heads of 8 emb dim each\n        self.ffwd = Feedforward(emb_dim=emb_dim)\n        self.lm_head = nn.Linear(emb_dim, vocab_size) # for prediction\n\n    def forward(self, idx, targets=None):\n\n        B, T = idx.shape # batch size, num tokens\n\n        tok_embs = self.token_embedding_table(idx) # (B, T, C)\n        pos_embs = self.position_embedding_table(torch.arange(T)) # (T, C)\n\n        x = tok_embs + pos_embs # (B, T, C) - right aligned broadcasting okay\n        x = self.sa_heads(x) # multiple heads this time\n        x = self.ffwd(x) # feedforward layer\n        logits = self.lm_head(x) # (B, T, C) -&gt; (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n\n        for _ in range(max_new_tokens):\n\n            # Crop idx to be maximum block_size long\n            idx_cropped = idx[:, -block_size:]\n\n            logits, _ = self(idx_cropped) # forward pass\n            logits = logits[:, -1, :] # focus only on last token\n            probs = F.softmax(logits, dim=-1) # get probabilities\n            idx_next = torch.multinomial(probs, num_samples=1) # sample from this distribution\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T) -&gt; (B, T+1) append to the right\n\n        return idx"
  },
  {
    "objectID": "07-nanogpt.html#blocks-residual-connections-and-layer-normalization",
    "href": "07-nanogpt.html#blocks-residual-connections-and-layer-normalization",
    "title": "Tokenization",
    "section": "Blocks, Residual Connections and Layer Normalization",
    "text": "Blocks, Residual Connections and Layer Normalization\nThe Transformer can be divided blocks that are further segmented into two separate components: * the MHSA that performs the communication * the Feedforward layers that perform the computation\n\nclass Block(nn.Module):\n    '''\n    Transformer Block - communication followed by computation\n    '''\n    def __init__(self, num_heads, emb_dim):\n        super().__init__()\n        head_size = emb_dim // num_heads\n        self.sa_heads = MultiHeadAttention(num_heads, head_size)\n        self.ffwd = Feedforward(emb_dim=emb_dim)\n\n    def forward(self, x):\n        x = self.sa_heads(x)\n        x = self.ffwd(x)\n        return x\n\nNow when we start to create a lot more blocks in the architecture, we find that this model ends up becoming rather deep.\nThis could pose certain issues regarding the optimization of the parameters.\nThe authors of the paper utilize two approaches that aim to resolve these optimization issues.\nFirst we have (1) Skip/Residual Connections that distributes gradients equally along both a residual pathway (going unimpeded from the inputs to the outputs) - we fork off, do some computation, then come back. We have to implement these skip connections (via addition) and projections in a few of our classes.\n\nclass Head(nn.Module): # no changes here, just for completeness\n\n    def __init__(self, head_size, emb_dim=32, block_size=8):\n        super().__init__()\n        \n        self.key = nn.Linear(emb_dim, head_size, bias=False)\n        self.query = nn.Linear(emb_dim, head_size, bias=False)\n        self.value = nn.Linear(emb_dim, head_size, bias=False)\n\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n\n        B,T,C = x.shape # batch size, num tokens, embedding dim\n\n        k = self.key(x)\n        q = self.query(x)\n\n        weights = q @ (k.transpose(-1, -2)) / (C**0.5)\n        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # decoder block\n        weights = F.softmax(weights, dim=-1)\n\n        v = self.value(x)\n        out = weights @ v\n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, num_heads, head_size, emb_dim, block_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, emb_dim=emb_dim, block_size=block_size) for _ in range(num_heads)])\n        self.emb_dim = num_heads * head_size\n        self.proj = nn.Linear(self.emb_dim, self.emb_dim) # project back to original embedding dim\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # output of raw MHSA: (B, T, C)\n        out = self.proj(out) # projection back to original embedding dim\n        return out\n    \n\nclass Feedforward(nn.Module):\n\n    def __init__(self, emb_dim):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(emb_dim, 4*emb_dim), # small change: 4x bigger hidden layer (from the paper)\n            nn.ReLU(),\n            nn.Linear(4*emb_dim, emb_dim), # projection back into residual pathway\n            nn.Dropout(0.2), # small change: dropout\n        )\n\n    def forward(self, x):\n        return self.net(x)\n        \n\nclass Block(nn.Module):\n\n    def __init__(self, num_heads, emb_dim, block_size):\n        super().__init__()\n        head_size = emb_dim // num_heads\n        self.sa_heads = MultiHeadAttention(num_heads, head_size, emb_dim, block_size)\n        self.ffwd = Feedforward(emb_dim=emb_dim)\n\n    def forward(self, x):\n        x = x + self.sa_heads(x) # fork off, do communication, come back\n        x = x + self.ffwd(x) # fork off, do computation, come back\n        return x\n\nThe other improvement we have is (2) Layer Norm. This is very similar to Batch Normalization, except that now, instead of normalizing along the columns, we normalize along the rows (for every single example).\nWe don’t need any distinction between training and test time, we don’t require any buffers for the running mean and variances, we can apply this simpler algorithm any time we wish with no previous state (except the parameters).\nSlight deviation from the initial paper: the LayerNorm is applied before each of the transformations, rather than after.\n\n## Good exercise to write this from scratch\n\nclass LayerNorm:\n\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n\n        xmean = x.mean(dim=1, keepdim=True)\n        xvar = x.var(dim=1, keepdim=True)\n        xhat = (x-xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\nclass Block(nn.Module):\n\n    def __init__(self, emb_dim, num_heads, head_size, block_size):\n        super().__init__()\n        head_size = emb_dim // num_heads\n        self.sa_heads = MultiHeadAttention(num_heads=num_heads, head_size=head_size, emb_dim=emb_dim, block_size=block_size)\n        self.ffwd = Feedforward(emb_dim=emb_dim)\n        self.ln1 = nn.LayerNorm(emb_dim) # for each MHSA layer\n        self.ln2 = nn.LayerNorm(emb_dim) # for the feedforward layer\n\n    def forward(self, x):\n        x = x + self.sa_heads(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x"
  },
  {
    "objectID": "07-nanogpt.html#the-final-language-model",
    "href": "07-nanogpt.html#the-final-language-model",
    "title": "Tokenization",
    "section": "The Final Language Model",
    "text": "The Final Language Model\nNow we can use all of these pieces to construct the final Transformer model.\nNote that the hyperparamters we have in defining the architecture are: * number of blocks * number of heads in each block * embedding dimension * block size/context length\nThe Head Size can be inferred from embedding dim // num heads.\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self, emb_dim, block_size, n_layer, vocab_size, num_heads):\n        super().__init__()\n\n        self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n        \n        self.blocks = nn.Sequential(*[Block(emb_dim, num_heads=num_heads, head_size=emb_dim//num_heads, block_size=block_size) for _ in range(n_layer)])\n        self.ln_final = nn.LayerNorm(emb_dim)\n        self.lm_head = nn.Linear(emb_dim, vocab_size) # for prediction\n\n        self.apply(self._init_weights) # better initialization of weights\n\n    def _init_weights(self, module):\n        \n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        \n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idxs, targets=None):\n\n        B, T = idxs.shape # batch size, num tokens\n\n        tok_embs = self.token_embedding_table(idxs) # (B, T, C)\n        pos_embs = self.position_embedding_table(torch.arange(T)) # (T, C)\n        \n        x = tok_embs + pos_embs # prepare input\n        x = self.blocks(x) # pass through transformer blocks\n        x = self.ln_final(x) # final layer norm\n        logits = self.lm_head(x) # classifier head into (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C) # flatten along first two dimensions\n            targets = targets.view(B*T) # flatten\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    def generate(self, idxs, max_new_tokens):\n        \n        # idxs is a B,T tensor of token indices\n        for _ in range(max_new_tokens):\n\n            # Crop the input to be some max length\n            idxs_cropped = idxs[:, -block_size:]\n\n            logits, _ = self(idxs_cropped) # forward pass\n            logits = logits[:, -1, :] # focus only on last token (Bigram model)\n            probs = F.softmax(logits, dim=-1) # get probabilities\n\n            next_idx = torch.multinomial(probs, num_samples=1) # sample from this distribution\n\n            idxs = torch.cat((idxs, next_idx), dim=1) # (B, T) -&gt; (B, T+1) append to the right\n\n        return idxs\n\n\nmodel = GPTLanguageModel(\n    emb_dim=384,\n    block_size=256, # max context length in generating logits\n    n_layer=6, # number of transformer blocks\n    vocab_size=vocab_size, \n    num_heads=6 # number of heads in each transformer block (MHSA)\n)\n\nprint(f\"The model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n\nThe model has 10,788,929 parameters\n\n\nLet’s make a few functions for training and evaluating our model.\n\n@torch.no_grad()\ndef estimate_loss(split=\"val\", eval_iters=200):\n    out = {}\n    model.eval()\n\n    for split in \"train\", \"val\":\n        losses = torch.zeros(eval_iters)\n\n        for k in range(eval_iters): # find the average loss over 200 batches\n        \n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        \n        out[split] = losses.mean().item()\n    model.train()\n    return out\n\ndef train_model(lr=1e-3, max_epochs=100, eval_interval=10):\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n    for epoch in range(max_epochs):\n\n        if epoch % eval_interval == 0 or epoch == max_epochs-1:\n            losses = estimate_loss()\n            print(f\"Epoch {epoch}: train loss {losses['train']:.3f}, val loss {losses['val']:.3f}\")\n\n        xb, yb = get_batch(\"train\")\n\n        # Evaluate loss\n        logits, loss = model(xb, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\nAnd to check whether the model works.\n\n# Generate from the model before training\ncontext = torch.zeros((1,1), dtype=torch.long)\nmodel_gen = model.generate(context, max_new_tokens=500)[0].tolist()\nprint(decode(model_gen))\n\n\np-LmuTEvh3Qp Eifg-RevUIyVJnkB$UNwpO!icNuN Q\nljGL$BuHG'QdorcNNWRbnHEyIC\nMIkWczhK E AaqGKnkDTBweCCZR;vkWLBLmv\ndRLQlZhT,,Bfma-zH-SLHyNzaZD:nDM\nGVIljiEz3&DeNrJhCVnqJ;HOlwIGIH.QUSMDnPGt?\ng3x!QzXiSbdgGr.' LcvA:tjbLlwy3GnZgBMzytpFaOQ;c:AnAbY''xQ!cEPgl\nZ.VNaVa!! lC:OnZ\n3p MGIzqGvwB\n QvkbqVltRDkYZSzNvXIqwmm\nJ'YlzTwxzMvy'sCwXCdk.Ngg'ovQmP,qFjIdCjYOIq?jyA?HCI,KY:kM-r\nSh:hYbM.QGVF\nAZRhKuNflKSrTEBgynmz.U;WjZIDTbvwwAi3Ko'q.d:HF'Ks:d\nS$qXIwfAKXKP\nSic$qwl!cRIlTKzIQvBMaqG!xL;bVYNf\n',IAFINdZYOgFoFu-Sd&l:l\n' sGqvS"
  },
  {
    "objectID": "07-nanogpt.html#fin",
    "href": "07-nanogpt.html#fin",
    "title": "Tokenization",
    "section": "FIN",
    "text": "FIN"
  },
  {
    "objectID": "posts/transformer-from-scratch/code.html",
    "href": "posts/transformer-from-scratch/code.html",
    "title": "Transformers from Scratch",
    "section": "",
    "text": "We will be implementing a Transformer, specifically a Decoder-only Transformer, from scratch. This will be taken from the paper Attention Is All You Need by Vaswani et al. (2017).\nThe Transformer is a model architecture that has been used in many NLP tasks, such as machine translation, text summarization, and question-answering. It is based on the idea of self-attention, which allows the model to weigh the importance of different words in a sentence when encoding or decoding it.\nThis has been greatly inspired from Andrej Karpathy’s video on building GPT. While this is a poor imitation at best, I hope to explore some aspects I initially found confusing a bit deeper, to play around with different ways to implement the same thing, and to tinker with some ideas that the video does not go into.\n# Import in our libraries\nimport os\nimport requests\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport tiktoken\nx = torch.tensor([1.2, 5.3, -0.8, 0.9, float(\"-inf\"), float(\"-inf\"), float(\"-inf\"), float(\"-inf\")])\ntorch.softmax(x, dim=0)\n\ntensor([0.0161, 0.9698, 0.0022, 0.0119, 0.0000, 0.0000, 0.0000, 0.0000])"
  },
  {
    "objectID": "posts/transformer-from-scratch/code.html#bringing-in-our-data",
    "href": "posts/transformer-from-scratch/code.html#bringing-in-our-data",
    "title": "Transformers from Scratch",
    "section": "Bringing in our data",
    "text": "Bringing in our data\nWe will be using the Tiny Shakespeare dataset, same as the video.\nLet’s load in our data in the cell below and work on processing it to feed it into our model.\n\n# Download the tiny shakespeare dataset\ninput_file_path = 'input.txt'\n\nif not os.path.exists(input_file_path):\n    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n    \n    with open(input_file_path, 'w', encoding='utf-8') as f:\n        f.write(requests.get(data_url).text)\n\n\n# Read in the data\nwith open(input_file_path, 'r', encoding='utf-8') as f:\n    text = f.read()\n\nprint(text[:250])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n\n\nNow that we’ve loaded in our data, let’s start by tokenizing it.\nThis can be very easily handled with the tiktoken library (read up here), which is also what the GPT family uses :p\nA tokenizer will convert a token (a piece of text - could be a word or part of a word) into an integer, which is what we need to feed into our model. We will use the same tokenizer as GPT-2 for this task, which has a vocabulary size of 50257 - this means that we have 50257 unique tokens in our vocabulary, i.e. 50257 unique integers that represent different words or parts of words.\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Small example\nsample_text = \"Hello, world!\"\nencoded = tokenizer.encode(sample_text)\ndecoded = tokenizer.decode(encoded)\n\nprint(f\"Original text: \\\"{sample_text}\\\" --&gt; Encoded: {encoded}\")\nprint(f\"List of tokens: {[tokenizer.decode([token]) for token in encoded]}\")\n\nOriginal text: \"Hello, world!\" --&gt; Encoded: [15496, 11, 995, 0]\nList of tokens: ['Hello', ',', ' world', '!']\n\n\n\n# Let's tokenize our entire dataset\nencoded_text = tokenizer.encode(text)\n# Convert to torch tensor of int64 (important for later on)\nencoded_text = torch.tensor(encoded_text).long()\n\nprint(f\"First 10 tokens: {encoded_text[:10]}\")\nprint(f\"Length of original dataset: {len(text)}\")\nprint(f\"Length of tokenized dataset: {len(encoded_text)}\")\n\nFirst 10 tokens: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11])\nLength of original dataset: 1115394\nLength of tokenized dataset: 338025\n\n\nNow that we have our textual data in numeric form, we can think about how to process it in a way to feed to our model.\nWe will start training our model to predict the next token in a sequence of tokens - the essence of language modeling.\nWe will do this by creating windows in our dataset, where each window is a sequence of tokens of a fixed length. We will then train our model to predict the next token in the sequence given the previous tokens.\nThis means that the input to our model will be a sequence of tokens, and the desired output will simply be that sequence shifted by one token.\n\n# Define the size of the window\nctx_len = 8\n\n# Create a window of size 8 to feed to our model\nx = encoded_text[:ctx_len]\ny = encoded_text[1: ctx_len+1]\n\nprint(f\"Context: {x}\")\nprint(f\"Target: {y}\")\nprint('-'*80)\n\nfor t in range(ctx_len):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"Context: {context} --&gt; Target: {target}\")\n\nContext: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597])\nTarget: tensor([22307,    25,   198,  8421,   356,  5120,   597,  2252])\n--------------------------------------------------------------------------------\nContext: tensor([5962]) --&gt; Target: 22307\nContext: tensor([ 5962, 22307]) --&gt; Target: 25\nContext: tensor([ 5962, 22307,    25]) --&gt; Target: 198\nContext: tensor([ 5962, 22307,    25,   198]) --&gt; Target: 8421\nContext: tensor([ 5962, 22307,    25,   198,  8421]) --&gt; Target: 356\nContext: tensor([ 5962, 22307,    25,   198,  8421,   356]) --&gt; Target: 5120\nContext: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120]) --&gt; Target: 597\nContext: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597]) --&gt; Target: 2252\n\n\nNow we can start thinking of what our model will actually do.\nIt works off taking a chunk of integers, whose length is at most ctx_len, and will predict the next integer in the sequence. Note that the chunks can be any length, we just have to specify the maximum context length for our model.\nWhen we take a chunk of 8 chars, we don’t just predict the next character after this sequence of 8 chars - we train our model to predict at each and every one of these positions. This means we have \\(n\\) different training examples for each context of length \\(n\\).\nThe model is being made to predict at contexts with sizes all the way from 1 till ctx_len; this means it has the ability to predict the next token and start generating when it’s been given just one token of context.\nNow we can begin to gather multiple windows at once to create minibatches to feed into our model.\n\nbatch_size = 16\n\ndef get_batch():\n    \"\"\"\n    Returns a batch of data for training\n    \"\"\"\n    # Sample batch_size number of starting indices to create our windows\n    idxs = torch.randint(0, len(encoded_text) - ctx_len, (batch_size,))\n\n    # Get our inputs and targets\n    x = torch.stack([encoded_text[idx:idx+ctx_len] for idx in idxs])\n    y = torch.stack([encoded_text[idx+1:idx+ctx_len+1] for idx in idxs])\n\n    return x, y\n\nxb, yb = get_batch()\nprint(xb.shape, yb.shape)\nprint('-'*80)\n\n# Print out window's examples - this is a single item in our batch\nfor b in range(1):\n    for t in range(ctx_len):\n        context = xb[b, :t+1]\n        target = yb[b, t]\n        print(f'Context: {context.tolist()} -&gt; Target: {target.item()}')\n\ntorch.Size([16, 8]) torch.Size([16, 8])\n--------------------------------------------------------------------------------\nContext: [21067] -&gt; Target: 11\nContext: [21067, 11] -&gt; Target: 3367\nContext: [21067, 11, 3367] -&gt; Target: 11\nContext: [21067, 11, 3367, 11] -&gt; Target: 466\nContext: [21067, 11, 3367, 11, 466] -&gt; Target: 26\nContext: [21067, 11, 3367, 11, 466, 26] -&gt; Target: 329\nContext: [21067, 11, 3367, 11, 466, 26, 329] -&gt; Target: 356\nContext: [21067, 11, 3367, 11, 466, 26, 329, 356] -&gt; Target: 1276"
  },
  {
    "objectID": "posts/transformer-from-scratch/code.html#language-model",
    "href": "posts/transformer-from-scratch/code.html#language-model",
    "title": "Transformers from Scratch",
    "section": "Language Model",
    "text": "Language Model\nThe output of the cell above shows exactly what our model would see and what it would try to predict for each item in the batch.\nOur model would take as input a \\((B, T)\\) tensor and output a \\((B, T, V)\\) tensor, where \\(B\\) is the batch size, \\(T\\) is the sequence length, and \\(V\\) is the vocabulary size.\nThis is because we are predicting the probability distribution of the next token for all \\(B \\times T\\) positions in the input. This means we will have exactly \\(B \\times T\\) different training examples for each batch."
  },
  {
    "objectID": "posts/transformer-from-scratch/code.html#self-attention",
    "href": "posts/transformer-from-scratch/code.html#self-attention",
    "title": "Transformers from Scratch",
    "section": "Self-Attention",
    "text": "Self-Attention\nThe whole point of Self-Attention is to get the tokens interacting and talking with one another. The idea is to bake context into the raw representations of the tokens themselves.\nThe way this is done is to take each token, and to extract three pieces of information from it: the Query, the Key, and the Value. These have the following ideas behind them:\n\nThe Query says “What am I looking for/Here’s what I’m interested in…”\nThe Key says “What do I contain/This is what I have…”\nThe Value says “If you find me interesting, here’s what I will communicate to you…”\n\nThe way we get the affinities between tokens now is to simply take a dot product between the Query and the Key.\nAs an example: if some token, say at position 8 (whose Query we take), finds that a token at postion 4 produces a Key that generates a high dot product value, then the model would have learned something important about the meaning of that 8th token. It now knows that to better understand the 8th token, it should look at the 4th token as well for context.\nThe way we extract these pieces is to simply perform a linear transformation on the input embeddings to get the Query, Key, and Value matrices."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Transformers from Scratch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst poost\n\n\n\n\n\n\nnlp\n\n\nfrom scratch\n\n\nfoundations\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\n\n\n\n\nNo matching items"
  }
]