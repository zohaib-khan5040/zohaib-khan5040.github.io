{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "-------------------------\n",
      "The first 100 characters:\n",
      "...\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('./input.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print('-'*25)\n",
    "print(f\"The first 100 characters:\\n...\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65 characters\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Get all the unique characters from this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Vocabulary size: {vocab_size} characters')\n",
    "print(''.join(chars))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "mii gustaa\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encoding and decoding functions\n",
    "encode = lambda string: [stoi[s] for s in string]\n",
    "decode = lambda tokens: ''.join([itos[t] for t in tokens])\n",
    "\n",
    "# Small example\n",
    "print(encode(\"Hello world!\"))\n",
    "print(decode(encode(\"mii gustaa\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numericalize the whole dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and dev splits\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [18] -> Target: 47\n",
      "Context: [18 47] -> Target: 56\n",
      "Context: [18 47 56] -> Target: 57\n",
      "Context: [18 47 56 57] -> Target: 58\n",
      "Context: [18 47 56 57 58] -> Target: 1\n",
      "Context: [18 47 56 57 58  1] -> Target: 15\n",
      "Context: [18 47 56 57 58  1 15] -> Target: 47\n",
      "Context: [18 47 56 57 58  1 15 47] -> Target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'Context: {context.numpy()} -> Target: {target.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "Context: [24] -> Target: 43\n",
      "Context: [24, 43] -> Target: 58\n",
      "Context: [24, 43, 58] -> Target: 5\n",
      "Context: [24, 43, 58, 5] -> Target: 57\n",
      "Context: [24, 43, 58, 5, 57] -> Target: 1\n",
      "Context: [24, 43, 58, 5, 57, 1] -> Target: 46\n",
      "Context: [24, 43, 58, 5, 57, 1, 46] -> Target: 43\n",
      "Context: [24, 43, 58, 5, 57, 1, 46, 43] -> Target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # max number of examples we process in parallel\n",
    "block_size = 8 # max context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "\n",
    "    # Stack these rows in a 4x8 tensor\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "for b in range(1):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'Context: {context.tolist()} -> Target: {target.item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65]) 4.658127307891846\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx, targets: (batch_size, block_size)\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        # F.cross_entropy expects channels last (synonymous with embedding dim)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Get predictions\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # Focus only on last time step\n",
    "            logits = logits[:, -1, :] # (B,T,C) -> (B, C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "            # Sample from this distribution to get new token\n",
    "            new_token = torch.multinomial(probs, num_samples=1) # (B, 1) single predictions\n",
    "\n",
    "            idx = torch.cat([idx, new_token], dim=-1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, loss.item())\n",
    "\n",
    "# Sample from the model\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "model_gen = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(model_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 4.692410945892334\n",
      "Step: 1000, Loss: 3.7637593746185303\n",
      "Step: 2000, Loss: 3.2342259883880615\n",
      "Step: 3000, Loss: 2.892245292663574\n",
      "Step: 4000, Loss: 2.703908681869507\n",
      "Step: 5000, Loss: 2.5153486728668213\n",
      "Step: 6000, Loss: 2.4889943599700928\n",
      "Step: 7000, Loss: 2.514069080352783\n",
      "Step: 8000, Loss: 2.444497585296631\n",
      "Step: 9000, Loss: 2.3975775241851807\n",
      "Step: 9999, Loss: 2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "# Training the bigram model\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "max_epochs = 10_000\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (max_epochs//10) == 0 or epoch==(max_epochs-1):\n",
    "        print(f'Step: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulsee\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "model_gen = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(model_gen))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mathematical Trick in Self-Attention\n",
    "\n",
    "In the above language model, the tokens weren't really \"speaking with one another\", i.e. they were decoupled and being processed independently of the others. Now we want to change this, to have them interact with one another but in a very specific way.\n",
    "\n",
    "We want to use the previous time-steps (or previous tokens) as context to try and predict the future. In this event, we don't want token 5 for example to be able to use tokens 6, 7, 8 for its processing. Information from previous time-steps should only be used in processing/predicting the current token.\n",
    "\n",
    "The easiest way for tokens to communicate is to simply **take an average of the previous tokens** in some way. This is a weak form of interaction, but it serves as a way for the current token to be defined in terms of the context surrounding it (preceding it more specifically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A toy example\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch size, time step (num tokens), channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original embeddings:\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "\n",
      "BOW embeddings:\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "# 1st version: for loops\n",
    "for b in range(B): # iterate through all the instances\n",
    "    for t in range(T): # iterate through the time steps\n",
    "        xprev = x[b, :t+1] # only take the previous tokens\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # average over the tokens\n",
    "\n",
    "# Compare the BOW representation and the original embeddings\n",
    "print(\"Original embeddings:\")\n",
    "print(x[0])\n",
    "print()\n",
    "print(\"BOW embeddings:\")\n",
    "print(xbow[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the first element in both matrices is the same (since it has no extra context). On top of this, the second row in the second matrix is an average of the first two rows in the first matrix, and so on. This is taking as much context as we can.\n",
    "\n",
    "There is a lot of information lost when we process things this way. We can be more efficient if we use Matrix Multiplication: a Triangular Matrix is perfect for our use since that has an extra non-zero element in each succeeding row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "----------\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----------\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Use a triangular matrix for a mask\n",
    "a = torch.tril(input=torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True) # normalize values to implement an average\n",
    "\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print('-'*10)\n",
    "print(b)\n",
    "print('-'*10)\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the first element not having any additional context retains its row vector. The next row is an average of the first two row vectors (the previous one and itself). The third vector is an average of all the rows in the input.\n",
    "\n",
    "We can consider these normalized values to be weights, that are initialized to give the same importance to every element that can be used for a given element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd version: Now to vectorize the original BOW model\n",
    "weights = torch.tril(input=torch.ones(T, T)) # square matrix of num_tokens x num_tokens\n",
    "weights = weights / weights.sum(dim=1, keepdim=True)\n",
    "xbow2 = weights @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we can do this is to use the Softmax activation.\n",
    "\n",
    "If we want a zero somewhere, we can recall that the exponential of neg. inf. is 0 (so we fill in the zeros for the weights with neg. inf.). Then if a row has the same elements (like all ones), then Softmax will average the values out itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]]) \n",
      "\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]]) \n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd version: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# Initialize this \"affinity\" matrix\n",
    "weights = torch.zeros((T, T))\n",
    "print(weights, '\\n')\n",
    "\n",
    "# Block out the tokens from the future\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "print(weights, '\\n')\n",
    "\n",
    "# Normalize the values to 1 (for a proper weighted sum)\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "print(weights, '\\n')\n",
    "\n",
    "xbow3 = weights @ x\n",
    "\n",
    "torch.allclose(xbow3, xbow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "We can use the tricks above to implement Self Attention properly now. Instead of using the simple average with the Lower Triangular Matrix (that implements the notion of *blocking out the future*), we want to more towards a more sophisticated form of these affinities (rather than relying on uniform numbers). **Self-Attention solves this in a data-driven way**.\n",
    "\n",
    "The idea is this: every token in the input sequence emits a **query** and a **key** (alongside the **value**). These have the following ideas behind them:\n",
    "* The Query says \"What am I looking for/Here's what I'm interested in...\"\n",
    "* The Key says \"What do I contain/This is what I have...\"\n",
    "* The Value says \"If you find me interesting, here's what I will communicate to you...\"\n",
    "\n",
    "The way we get the *affinities between tokens* now is to simply take a dot product between the Query and the Key. \n",
    "\n",
    "The Query for a *specific token* emits a certain value, representing what it's looking for. Now, all the tokens in the input sequence emit their Keys, representing what that token is offering. If that specific token, say at position 8 (whose Query we take), finds that a token at postion 4 produces a high value when we take the dot product of the Query and Key (at positions 8 and 4 respectively), then the model has learned something meaningful about the meaning of that 8th token (new information has been aggregated, so the model has learned more *about it*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for zeroth instance\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
      "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
      "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
      "       grad_fn=<SelectBackward0>) \n",
      "\n",
      "Final output shape torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# 4th version: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Implementing a single head for Self Attention\n",
    "head_size = 16 # call this H\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, C) -> (B, T, H)\n",
    "q = query(x) # (B, T, C) -> (B, T, H)\n",
    "\n",
    "weights = q @ k.transpose(-1, -2) # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "weights *= (head_size)**-0.5 # scale by sqrt(d_k) for smoother softmax output\n",
    "\n",
    "# Masking and normalization\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # specifically for the Decoder\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = weights @ v # attention weighted input\n",
    "\n",
    "print(\"Attention weights for zeroth instance\\n\", weights[0], '\\n')\n",
    "print(\"Final output shape\", out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes:\n",
    "1. Attention is a **communication mechanism** in that it allows nodes in any directed graph to aggregate information from other nodes, specifically those that *point to it*. We can think of text as being a specific type of directed graph where the first token only points to itself, the second is being pointed to by the first (and itself), and so on, until the last token is being pointed to by *all the previous nodes, and itself*.\n",
    "2. Attention has no notion of *space*, which is the reason why we implement the Positional Embeddings that we'll see later.\n",
    "3. Each instance in the batch is processed independently of one another. So if there are 32 sequences/examples in the batch, then we're really looking at 32 different pools/components of joined nodes. This means that one sequence cannot take any clues or talk to other examples in that mini-batch.\n",
    "4. In an Encoder, if we don't want to restrict the tokens talking to the future tokens, we just get rid of the masking portion of the code with the lower triangular matrix. In the Decoder though, when we are trying to generate new tokens, we don't want the future tokens communicating with the past tokens (otherwise they'd be giving away the answer, on top of this interaction not even being possible), which is why we'd bring the masking feature in there.\n",
    "5. \"Self-Attention\" is specifically Attention when the Keys, Queries and Values are coming from the same source. Attention is more general than that: \"Cross Attention\" is when the Keys and the Values are coming from an external source outside the source of the Keys.\n",
    "\n",
    "Now to make this a proper class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, emb_dim=32, block_size=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape # batch size, num tokens, embedding dim\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ (k.transpose(-1, -2)) / (C**0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # decoder block\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out\n",
    "    \n",
    "\n",
    "class SABigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    Small Bigram language model using Self Attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size=vocab_size, emb_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
    "\n",
    "        self.sa_head = Head(head_size=emb_dim)\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size) # for prediction\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape # batch size, num tokens\n",
    "\n",
    "        tok_embs = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_embs = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "\n",
    "        x = tok_embs + pos_embs # (B, T, C) - right aligned broadcasting okay\n",
    "        x = self.sa_head(x) # only one head for now\n",
    "        logits = self.lm_head(x) # (B, T, C) -> (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Crop idx to be maximum block_size long\n",
    "            idx_cropped = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cropped) # forward pass\n",
    "            logits = logits[:, -1, :] # focus only on last token\n",
    "            probs = F.softmax(logits, dim=-1) # get probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample from this distribution\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T) -> (B, T+1) append to the right\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that there is the notion of Positional Embedding that is \"added\" to the Embeddings of the inputs. \n",
    "\n",
    "The **Positional Embedding** can be initialized as an Embedding table of size `(block_size, pos_emb_dim)`, and the actual **Token Embeddings** can be initialized as an Embedding table of size `(vocab_size, emb_dim)`.\n",
    "\n",
    "The reasoning for the values at axis 0 is self explanatory.\n",
    "\n",
    "## Multi-Headed Self Attention\n",
    "\n",
    "To allow for **more independent communication channels**, which means allowing for tokens to take on more than just one combination of context clues, we allow for the Transformer to use multiple of these Self Attention Heads.\n",
    "\n",
    "Note however that we change up the dimension of the Head Size, since at the end, we want the output to stay the same, after we concatenate information from all the heads.\n",
    "\n",
    "Note that the processing for each head will run in parallel so we suffer no cost in terms of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Layers\n",
    "\n",
    "In the paper, the authors did not immediately feed in the processed inputs to a classification layer, rather there were intermediate FeedForward layers that allowed for more of this intermediate processing of activations.\n",
    "\n",
    "This can be thought of as Self-Attention and the multiple heads aggregating that information, and the Feedforward layers allowing these tokens to *think* on this new aggregated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SABigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    Small Bigram language model using Self Attention, now with multiple heads and a Feedforward layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size=vocab_size, emb_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
    "\n",
    "        self.sa_heads = MultiHeadAttention(num_heads=4, head_size=emb_dim//4) # 4 heads of 8 emb dim each\n",
    "        self.ffwd = Feedforward(emb_dim=emb_dim)\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size) # for prediction\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape # batch size, num tokens\n",
    "\n",
    "        tok_embs = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_embs = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "\n",
    "        x = tok_embs + pos_embs # (B, T, C) - right aligned broadcasting okay\n",
    "        x = self.sa_heads(x) # multiple heads this time\n",
    "        x = self.ffwd(x) # feedforward layer\n",
    "        logits = self.lm_head(x) # (B, T, C) -> (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Crop idx to be maximum block_size long\n",
    "            idx_cropped = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cropped) # forward pass\n",
    "            logits = logits[:, -1, :] # focus only on last token\n",
    "            probs = F.softmax(logits, dim=-1) # get probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample from this distribution\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T) -> (B, T+1) append to the right\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks, Residual Connections and Layer Normalization\n",
    "\n",
    "The Transformer can be divided blocks that are further segmented into two separate components:\n",
    "* the MHSA that performs the **communication**\n",
    "* the Feedforward layers that perform the **computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''\n",
    "    Transformer Block - communication followed by computation\n",
    "    '''\n",
    "    def __init__(self, num_heads, emb_dim):\n",
    "        super().__init__()\n",
    "        head_size = emb_dim // num_heads\n",
    "        self.sa_heads = MultiHeadAttention(num_heads, head_size)\n",
    "        self.ffwd = Feedforward(emb_dim=emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we start to create a lot more blocks in the architecture, we find that this model ends up becoming rather *deep*. \n",
    "\n",
    "This could pose certain issues regarding the optimization of the parameters.\n",
    "\n",
    "The authors of the paper utilize two approaches that aim to resolve these optimization issues.\n",
    "\n",
    "First we have (1) **Skip/Residual Connections** that distributes gradients equally along both a *residual pathway* (going unimpeded from the inputs to the outputs) - we fork off, do some computation, then come back. We have to implement these skip connections (via addition) and projections in a few of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module): # no changes here, just for completeness\n",
    "\n",
    "    def __init__(self, head_size, emb_dim=32, block_size=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape # batch size, num tokens, embedding dim\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ (k.transpose(-1, -2)) / (C**0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # decoder block\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, emb_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, emb_dim=emb_dim, block_size=block_size) for _ in range(num_heads)])\n",
    "        self.emb_dim = num_heads * head_size\n",
    "        self.proj = nn.Linear(self.emb_dim, self.emb_dim) # project back to original embedding dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # output of raw MHSA: (B, T, C)\n",
    "        out = self.proj(out) # projection back to original embedding dim\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4*emb_dim), # small change: 4x bigger hidden layer (from the paper)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*emb_dim, emb_dim), # projection back into residual pathway\n",
    "            nn.Dropout(0.2), # small change: dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, emb_dim, block_size):\n",
    "        super().__init__()\n",
    "        head_size = emb_dim // num_heads\n",
    "        self.sa_heads = MultiHeadAttention(num_heads, head_size, emb_dim, block_size)\n",
    "        self.ffwd = Feedforward(emb_dim=emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(x) # fork off, do communication, come back\n",
    "        x = x + self.ffwd(x) # fork off, do computation, come back\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other improvement we have is (2) **Layer Norm**. This is very similar to Batch Normalization, except that now, instead of normalizing along the columns, we normalize along the rows (for every single example).\n",
    "\n",
    "We don't need any distinction between training and test time, we don't require any buffers for the running mean and variances, we can apply this simpler algorithm any time we wish with no previous state (except the parameters).\n",
    "\n",
    "Slight deviation from the initial paper: the `LayerNorm` is applied *before* each of the transformations, rather than after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Good exercise to write this from scratch\n",
    "\n",
    "class LayerNorm:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        xmean = x.mean(dim=1, keepdim=True)\n",
    "        xvar = x.var(dim=1, keepdim=True)\n",
    "        xhat = (x-xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads, head_size, block_size):\n",
    "        super().__init__()\n",
    "        head_size = emb_dim // num_heads\n",
    "        self.sa_heads = MultiHeadAttention(num_heads=num_heads, head_size=head_size, emb_dim=emb_dim, block_size=block_size)\n",
    "        self.ffwd = Feedforward(emb_dim=emb_dim)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim) # for each MHSA layer\n",
    "        self.ln2 = nn.LayerNorm(emb_dim) # for the feedforward layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Language Model\n",
    "\n",
    "Now we can use all of these pieces to construct the final Transformer model.\n",
    "\n",
    "Note that the hyperparamters we have in defining the architecture are:\n",
    "* number of blocks\n",
    "* number of heads in each block\n",
    "* embedding dimension\n",
    "* block size/context length\n",
    "\n",
    "The Head Size can be inferred from `embedding dim // num heads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,788,929 parameters\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, block_size, n_layer, vocab_size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(emb_dim, num_heads=num_heads, head_size=emb_dim//num_heads, block_size=block_size) for _ in range(n_layer)])\n",
    "        self.ln_final = nn.LayerNorm(emb_dim)\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size) # for prediction\n",
    "\n",
    "        self.apply(self._init_weights) # better initialization of weights\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idxs, targets=None):\n",
    "\n",
    "        B, T = idxs.shape # batch size, num tokens\n",
    "\n",
    "        tok_embs = self.token_embedding_table(idxs) # (B, T, C)\n",
    "        pos_embs = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "        \n",
    "        x = tok_embs + pos_embs # prepare input\n",
    "        x = self.blocks(x) # pass through transformer blocks\n",
    "        x = self.ln_final(x) # final layer norm\n",
    "        logits = self.lm_head(x) # classifier head into (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # flatten along first two dimensions\n",
    "            targets = targets.view(B*T) # flatten\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idxs, max_new_tokens):\n",
    "        \n",
    "        # idxs is a B,T tensor of token indices\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Crop the input to be some max length\n",
    "            idxs_cropped = idxs[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idxs_cropped) # forward pass\n",
    "            logits = logits[:, -1, :] # focus only on last token (Bigram model)\n",
    "            probs = F.softmax(logits, dim=-1) # get probabilities\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # sample from this distribution\n",
    "\n",
    "            idxs = torch.cat((idxs, next_idx), dim=1) # (B, T) -> (B, T+1) append to the right\n",
    "\n",
    "        return idxs\n",
    "\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    emb_dim=384,\n",
    "    block_size=256, # max context length in generating logits\n",
    "    n_layer=6, # number of transformer blocks\n",
    "    vocab_size=vocab_size, \n",
    "    num_heads=6 # number of heads in each transformer block (MHSA)\n",
    ")\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a few functions for training and evaluating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(split=\"val\", eval_iters=200):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in \"train\", \"val\":\n",
    "        losses = torch.zeros(eval_iters)\n",
    "\n",
    "        for k in range(eval_iters): # find the average loss over 200 batches\n",
    "        \n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        \n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_model(lr=1e-3, max_epochs=100, eval_interval=10):\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        if epoch % eval_interval == 0 or epoch == max_epochs-1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"Epoch {epoch}: train loss {losses['train']:.3f}, val loss {losses['val']:.3f}\")\n",
    "\n",
    "        xb, yb = get_batch(\"train\")\n",
    "\n",
    "        # Evaluate loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to check whether the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "p-LmuTEvh3Qp Eifg-RevUIyVJnkB$UNwpO!icNuN Q\n",
      "ljGL$BuHG'QdorcNNWRbnHEyIC\n",
      "MIkWczhK E AaqGKnkDTBweCCZR;vkWLBLmv\n",
      "dRLQlZhT,,Bfma-zH-SLHyNzaZD:nDM\n",
      "GVIljiEz3&DeNrJhCVnqJ;HOlwIGIH.QUSMDnPGt?\n",
      "g3x!QzXiSbdgGr.' LcvA:tjbLlwy3GnZgBMzytpFaOQ;c:AnAbY''xQ!cEPgl\n",
      "Z.VNaVa!! lC:OnZ\n",
      "3p MGIzqGvwB\n",
      " QvkbqVltRDkYZSzNvXIqwmm\n",
      "J'YlzTwxzMvy'sCwXCdk.Ngg'ovQmP,qFjIdCjYOIq?jyA?HCI,KY:kM-r\n",
      "Sh:hYbM.QGVF\n",
      "AZRhKuNflKSrTEBgynmz.U;WjZIDTbvwwAi3Ko'q.d:HF'Ks:d\n",
      "S$qXIwfAKXKP\n",
      "Sic$qwl!cRIlTKzIQvBMaqG!xL;bVYNf\n",
      "',IAFINdZYOgFoFu-Sd&l:l\n",
      "' sGqvS\n"
     ]
    }
   ],
   "source": [
    "# Generate from the model before training\n",
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "model_gen = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(model_gen))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
