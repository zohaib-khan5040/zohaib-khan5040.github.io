{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"An Introduction to Pruning (Part 2)\"\n",
    "date: \"2024-08-15\"\n",
    "bibliography: references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Pruning with CNNs\n",
    "\n",
    "In the previous part of this blog series, we covered the basics of pruning, specifically fine-grained pruning. If you haven’t had a chance to read it yet, I encourage you to check out that notebook to familiarize yourself with the foundational concepts and understanding the motivation behind what we will do now.\n",
    "\n",
    "In this section, we will review the different types of pruning methods, focusing particularly on channel-wise pruning. Pruning can range from fine-grained approaches, where individual connections are zeroed out based on their importance, to more structured forms like channel-wise pruning. Channel-wise pruning is a more regular method that involves removing entire channels or layers from the network.\n",
    "\n",
    "Similar to fine-grained pruning, we will use magnitudes as a heuristic measure of importance to guide our pruning decisions. However, the key advantage of channel-wise pruning over fine-grained approaches is its structure. By removing entire channels, we achieve a more organized pruning process, which doesn't rely heavily on specialized hardware for efficient computation. This is because channel-wise pruning involves explicitly slicing out chunks of the parameter tensors, making it more straightforward and potentially more compatible with standard hardware.\n",
    "\n",
    "This article has been inspired from the labs of the [EfficientML.ai course by MIT](https://hanlab.mit.edu/courses/2024-fall-65940). In the following sections, we will delve into the specifics of channel-wise pruning and how it can be effectively implemented to achieve significant model compression while maintaining computational efficiency. This will be similar to the methodology followed in @han2015learningweightsconnectionsefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We will be using the same setup as the previous blog: a VGG-16 architecture that we will prune and evaluate on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "model = vgg16(weights=weights)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "root = \"./data\"\n",
    "train_ds = datasets.CIFAR10(root=root,\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=weights.transforms())\n",
    "test_ds = datasets.CIFAR10(root=root,\n",
    "                           train=False,\n",
    "                           download=True,\n",
    "                           transform=weights.transforms())\n",
    "train_dl = DataLoader(train_ds, \n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True)\n",
    "test_dl = DataLoader(test_ds, \n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=False)\n",
    "\n",
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "\n",
    "def get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    \n",
    "    :param data_width: #bits per element\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    return get_num_parameters(model, count_nonzero_only) * data_width\n",
    "\n",
    "def train_step(model, dataloader, criterion, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "\n",
    "    for step, (X, y) in tqdm(enumerate(dataloader), desc=\"Training\", leave=False):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred = torch.argmax(logits.detach(), dim=1)\n",
    "        train_acc += ((y_pred == y).sum().item() / len(y))\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_step(model, dataloader, criterion, device):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "\n",
    "    for (X, y) in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        y_pred = torch.argmax(logits.detach(), dim=1)\n",
    "        eval_acc += ((y_pred == y).sum().item() / len(y))\n",
    "\n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    eval_acc = eval_acc / len(dataloader)\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "ckpt_path = \"vgg16.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model accuracy: 0.90\n",
      "Original model size: 527.79 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Get original model size and benchmark accuracy\n",
    "val_loss, orig_acc = eval_step(model, test_dl, criterion, device)\n",
    "MB = 8 * 1024**2\n",
    "orig_model_size_mb = get_model_size(model) / MB\n",
    "\n",
    "print(f\"Original model accuracy: {orig_acc:.2f}\")\n",
    "print(f\"Original model size: {orig_model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel-wise Pruning\n",
    "\n",
    "To begin with channel-wise pruning, it's essential to understand the structure of the weight tensor in a convolutional block. A convolutional layer typically has a weight tensor with dimensions corresponding to the number of input channels, output channels, and the kernel size.\n",
    "\n",
    "When considering channel-wise pruning, we need to determine which axis of this tensor we will be pruning along. Specifically, our goal is to identify and remove the less important channels from either the input or output of the convolutional layer. We will see later what considerations we have to keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Channels: 32\n",
      "Out Channels: 64\n",
      "Kernel Size: 3\n",
      "Shape of Conv Layer weight tensor: torch.Size([64, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Examine the structure of a Convolution layer\n",
    "in_chans = 32\n",
    "out_chans = 64\n",
    "kernel_size = 3\n",
    "conv_layer = nn.Conv2d(in_channels=in_chans,\n",
    "                       out_channels=out_chans,\n",
    "                       kernel_size=kernel_size)\n",
    "\n",
    "conv_weight = conv_layer.weight.data\n",
    "\n",
    "print(f\"In Channels: {in_chans}\")\n",
    "print(f\"Out Channels: {out_chans}\")\n",
    "print(f\"Kernel Size: {kernel_size}\")\n",
    "print(f\"Shape of Conv Layer weight tensor: {conv_weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delve deeper into channel-wise pruning, it's crucial to examine the weight tensor's structure more closely. For a convolutional layer, the weight tensor typically has the shape $(c_{out}, c_{in}, k_h, k_w)$, where $c_{out}$ and $c_{in}$ denote the number of output and input channels respectively, and $k_h$ and $k_w$ represent the height and width of the kernels.\n",
    "\n",
    "When considering channel-wise pruning, our primary focus is on the second axis of the weight tensor — the $c_{in}$ dimension, which represents the input channels. Pruning channels involves slicing out certain portions along this axis. However, it's important to recognize that removing channels from this dimension will alter the shape of the activation maps that pass through the convolutional layer.\n",
    "\n",
    "To effectively implement channel-wise pruning, we need to understand how the changes to one layer's weight tensor will impact subsequent layers in the network. Specifically, if we prune input channels, the number of input channels to subsequent layers will be affected. This alteration requires us to adjust the subsequent layers accordingly to ensure that they can still process the modified activations.\n",
    "\n",
    "Examining the VGG architecture, we can develop a strategy for pruning that takes these dependencies into account. For example, VGG networks consist of a series of convolutional layers followed by fully connected layers. If we prune channels in one convolutional layer, we must update the subsequent layers to match the new input dimensions and ensure that the network remains functional.\n",
    "\n",
    "In summary, while pruning channels from the second axis of the weight tensor is straightforward, the resulting changes in activation shapes necessitate careful consideration of the network's overall architecture. By understanding and adjusting for these changes, we can effectively apply channel-wise pruning while maintaining the integrity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG architecture features alternating blocks of convolutional layers, ReLUs, and MaxPooling operations. To apply channel-wise pruning effectively, we should focus on each pair of adjacent convolutional blocks. \n",
    "\n",
    "For each pair, we adjust the output channels of the preceding convolutional block and the input channels of the subsequent convolutional block. This approach ensures that the changes made by pruning are consistent throughout the network, maintaining the correct flow of data and preserving model functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in block: 23136\n",
      "Output shape: torch.Size([64, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "# Dummy example\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_chans, hidden_size, out_chans):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_chans, out_channels=hidden_size, kernel_size=3)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=out_chans, kernel_size=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.mp(self.conv1(x)))\n",
    "\n",
    "in_chans = 16\n",
    "hidden_size = 32\n",
    "out_chans = 64\n",
    "img_size = 28\n",
    "\n",
    "x = torch.randn(in_chans, img_size, img_size)\n",
    "block = ConvBlock(in_chans, hidden_size, out_chans)\n",
    "\n",
    "out = block(x)\n",
    "block_orig_numparams = get_num_parameters(block)\n",
    "print(f\"Number of parameters in block: {block_orig_numparams}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_channels_to_keep(channels: int, prune_ratio: float) -> int:\n",
    "    \"\"\"\n",
    "    A function to calculate the number of layers to PRESERVE after pruning\n",
    "    \"\"\"\n",
    "    return int(round((1-prune_ratio)*channels))\n",
    "\n",
    "# Get the weights from the block\n",
    "conv1 = block.conv1\n",
    "mp = block.mp\n",
    "conv2 = block.conv2\n",
    "\n",
    "# Start pruning the channels\n",
    "prune_ratio = 0.6\n",
    "original_channels = conv1.out_channels\n",
    "n_keep = get_num_channels_to_keep(original_channels, prune_ratio)\n",
    "\n",
    "# 1. Prune the output channels of the first convolution layer\n",
    "with torch.no_grad():\n",
    "    conv1.weight = nn.Parameter(\n",
    "        conv1.weight.detach()[:n_keep, ...]\n",
    "    )\n",
    "    # Adjust the bias as well, if it exists\n",
    "    if conv1.bias is not None:\n",
    "        conv1.bias = nn.Parameter(\n",
    "            conv1.bias.detach()[:n_keep]\n",
    "        )\n",
    "\n",
    "# 2. Prune the affected input channels of the next convolution\n",
    "with torch.no_grad():\n",
    "    conv2.weight = nn.Parameter(\n",
    "        conv2.weight.detach()[:, :n_keep, ...]\n",
    "    )\n",
    "    # Bias does not need to be adjusted for this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in block (post-pruning): 9437\n",
      "Output shape: torch.Size([64, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in block (post-pruning): {get_num_parameters(block)}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experimenting with a dummy example of pruning input and output channels within a convolutional block, we've observed a reduction in the number of parameters while maintaining the same output shape. However, it's important to note that the number of parameters remaining is not precisely the product of the prune ratio and the total number of parameters, as we're only modifying the channels and not the entire network.\n",
    "\n",
    "To streamline this process, we'll define a function that automates channel-wise pruning for the entire model. This function will handle adjusting the number of input and output channels for each convolutional layer, ensuring that the pruned network remains consistent and functional. This automated approach will simplify the pruning procedure and facilitate efficient model compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n",
    "\n",
    "@torch.no_grad()\n",
    "def channel_prune(model: nn.Module,\n",
    "                  prune_ratio: Union[List, float]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Apply channel pruning to each convolutional layer in the backbone.\n",
    "\n",
    "    :param model: The model to be pruned\n",
    "    :param prune_ratio: Either a single float for uniform pruning across \n",
    "                        layers or a list of floats specifying per-layer pruning rates.\n",
    "\n",
    "    :return pruned_model: The model with pruned channels\n",
    "    \"\"\"\n",
    "    assert isinstance(prune_ratio, (float, list))\n",
    "\n",
    "    conv_layers = [m for m in model.features if isinstance(m, nn.Conv2d)]\n",
    "    n_conv = len(conv_layers)\n",
    "    \n",
    "    # The ratio affects the first conv's input channels, and the next one's out channels \n",
    "    if isinstance(prune_ratio, float):\n",
    "        prune_ratio = [prune_ratio] * (n_conv - 1)\n",
    "    else:\n",
    "        assert len(prune_ratio) == n_conv - 1, \"prune_ratio list length must be one less than the number of Conv2d layers.\"\n",
    "\n",
    "    # Create a deepcopy so we don't modify the original\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    conv_layers = [m for m in pruned_model.features if isinstance(m, nn.Conv2d)]\n",
    "\n",
    "    # Apply channel pruning to each pair of consecutive convolutional layers\n",
    "    for i, ratio in enumerate(prune_ratio):\n",
    "        prev_conv = conv_layers[i]\n",
    "        next_conv = conv_layers[i + 1]\n",
    "        prev_channels = prev_conv.out_channels\n",
    "        n_keep = get_num_channels_to_keep(prev_channels, ratio)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Prune the output channels of the previous convolution\n",
    "            prev_conv.weight = nn.Parameter(prev_conv.weight.detach()[:n_keep, ...])\n",
    "            if prev_conv.bias is not None:\n",
    "                prev_conv.bias = nn.Parameter(prev_conv.bias.detach()[:n_keep])\n",
    "\n",
    "            # Prune the input channels of the next convolution\n",
    "            next_conv.weight = nn.Parameter(next_conv.weight.detach()[:, :n_keep, ...])\n",
    "\n",
    "    print(\"Channel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\")\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\n",
      "Original model size: 527.79 MB\n",
      "Pruned model size: 494.03 MB\n"
     ]
    }
   ],
   "source": [
    "# Prune the model without considering the channel importances\n",
    "pruned_model_naive = channel_prune(model, 0.4)\n",
    "pruned_model_size_mb = get_model_size(pruned_model_naive) / MB\n",
    "\n",
    "print(f\"Original model size: {orig_model_size_mb:.2f} MB\")\n",
    "print(f\"Pruned model size: {pruned_model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after naive pruning: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model after this crude pruning\n",
    "_, acc = eval_step(pruned_model_naive, test_dl, criterion, device)\n",
    "\n",
    "print(f\"Accuracy after naive pruning: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning less-important channels\n",
    "\n",
    "When considering channel-wise pruning, it's crucial to avoid arbitrary removal of channels, as this could lead to significant performance degradation by discarding \"important\" channels, as was seen above. To address this, we need to prioritize channels based on their importance.\n",
    "\n",
    "A practical approach to determining channel importance is to use the norms of the weight tensors as a measure. The idea is that channels with larger norms are generally more critical for the network’s performance. By calculating these norms, we can rank the channels and selectively prune the less important ones.\n",
    "\n",
    "The process involves defining the importance of each channel, sorting them accordingly, and then retaining only the most important channels. We can integrate this approach into our existing pruning function, which previously kept the first `n_keep` channels. By incorporating channel importance into this function, we can ensure that the pruning is more strategic, preserving the channels that contribute most significantly to the network’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([64, 64, 3, 3])\n",
      "Importances of the 64 input channels:\n",
      "tensor([1.6980, 1.7297, 2.1539, 0.9993, 1.8365, 0.8037, 1.6834, 0.9603, 1.1872,\n",
      "        1.3582, 1.1134, 1.0917, 1.4087, 0.9050, 1.7213, 0.7823, 1.1833, 1.1185,\n",
      "        1.1565, 2.2874, 1.3824, 1.4115, 1.0174, 1.3120, 1.1977, 0.9108, 0.7976,\n",
      "        0.9291, 1.1520, 1.1238, 0.9578, 0.7938, 1.4062, 1.4817, 2.5130, 1.0180,\n",
      "        1.3782, 0.9571, 0.9826, 1.3465, 1.0445, 0.8921, 1.5498, 0.7251, 1.1079,\n",
      "        0.9550, 1.3082, 1.2728, 0.9647, 0.8078, 0.7796, 2.1746, 1.1919, 2.0185,\n",
      "        0.7407, 1.4707, 1.0315, 1.8911, 2.1096, 2.2035, 0.9893, 0.7218, 0.7914,\n",
      "        1.1358], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Grab a random convolution weight tensor to demonstrate computing channel importances\n",
    "rand_weight = model.features[2].weight\n",
    "print(f\"Shape: {rand_weight.shape}\")\n",
    "\n",
    "def get_input_channel_importance(weight):\n",
    "    in_channels = weight.shape[1]\n",
    "    importances = []\n",
    "    \n",
    "    # Compute the importance for each input channel\n",
    "    for i_c in range(in_channels):\n",
    "        channel_weight = weight.detach()[:, i_c] # (c_out, k, k)\n",
    "        importance = torch.norm(channel_weight) # take the Frobenius norm\n",
    "        importances.append(importance.view(1))\n",
    "    return torch.cat(importances)\n",
    "\n",
    "print(f\"Importances of the 64 input channels:\\n{get_input_channel_importance(rand_weight)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://hanlab.mit.edu/courses/2024-fall-65940\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_channel_sorting(model):\n",
    "    '''\n",
    "    Sorts the channels in decreasing order of importance for the given model\n",
    "\n",
    "    :param model: Model to apply the channel sorting to\n",
    "    '''\n",
    "    # Create a deep copy of the model to avoid modifying the original\n",
    "    sorted_model = copy.deepcopy(model)\n",
    "\n",
    "    # Fetch all the convolutional layers from the backbone\n",
    "    conv_layers = [m for m in sorted_model.features if isinstance(m, nn.Conv2d)]\n",
    "\n",
    "    # Iterate through the convolutional layers and sort channels by importance\n",
    "    for i in range(len(conv_layers) - 1):\n",
    "        prev_conv = conv_layers[i]\n",
    "        next_conv = conv_layers[i + 1]\n",
    "\n",
    "        # Compute the importance of input channels for the next convolutional layer\n",
    "        importance = get_input_channel_importance(next_conv.weight)\n",
    "        sort_idx = torch.argsort(importance, descending=True)\n",
    "\n",
    "        # Sort the output channels of the previous convolutional layer\n",
    "        prev_conv.weight = nn.Parameter(torch.index_select(prev_conv.weight.detach(), 0, sort_idx))\n",
    "        if prev_conv.bias is not None:\n",
    "            prev_conv.bias = nn.Parameter(torch.index_select(prev_conv.bias.detach(), 0, sort_idx))\n",
    "\n",
    "        # Sort the input channels of the next convolutional layer\n",
    "        next_conv.weight = nn.Parameter(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n",
    "\n",
    "    return sorted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We directly manipulate the slices of the corresponding channels within the weight and bias tensors. By sorting channels based on their importance in decreasing order, we can identify and retain the most critical channels as per our desired pruning ratio.\n",
    "\n",
    "After sorting, we keep the first `n_keep` slices, which ensures that the most important channels are preserved. This method leverages our previous function for pruning, allowing us to efficiently apply the revised approach. The rearrangement of tensor slices according to channel importance ensures that the pruned network maintains its effectiveness while reducing its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without sorting channels by importance...\n",
      "Channel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model accuracy: 0.10\n",
      "-------------------------\n",
      "With sorting channels by importance...\n",
      "Channel pruning completed. Note: The printed model structure may not reflect the pruned dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model accuracy: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "channel_pruning_ratio = 0.4  # pruned-out ratio\n",
    "\n",
    "print(\"Without sorting channels by importance...\")\n",
    "pruned_model = channel_prune(model, channel_pruning_ratio)\n",
    "_, acc = eval_step(pruned_model, test_dl, criterion, device)\n",
    "print(f\"Pruned model accuracy: {acc:.2f}\")\n",
    "\n",
    "print('-'*25)\n",
    "\n",
    "print(\"With sorting channels by importance...\")\n",
    "sorted_model = apply_channel_sorting(model)\n",
    "pruned_model = channel_prune(sorted_model, channel_pruning_ratio)\n",
    "_, acc = eval_step(pruned_model, test_dl, criterion, device)\n",
    "print(f\"Pruned model accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering performance with Finetuning\n",
    "\n",
    "With the sorting, we observe a somewhat smaller performance decrease compared to the initial approach, though the drop is still significant. This reduction in performance is typical with channel-wise pruning because entire chunks of the model are removed, which can affect its ability to generalize.\n",
    "\n",
    "To mitigate the performance loss, we again employ fine-tuning. This step is essential to allow the model to adjust to the new, pruned structure and recover some of its original performance. Fine-tuning helps recalibrate the remaining parameters and optimize the network for its reduced size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 1/3 [02:14<04:29, 134.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1... Train Accuracy: 0.87 | Validation Accuracy: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 2/3 [04:32<02:16, 136.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2... Train Accuracy: 0.94 | Validation Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 3/3 [06:47<00:00, 135.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3... Train Accuracy: 0.97 | Validation Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Finetune to recover performance\n",
    "learning_rate = 1e-4\n",
    "epochs = 3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(pruned_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    train_loss, train_acc = train_step(pruned_model, train_dl, criterion, optimizer, device)\n",
    "    val_loss, val_acc = eval_step(pruned_model, test_dl, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}... Train Accuracy: {train_acc:.2f} | Validation Accuracy: {val_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of original (dense) model: 0.898\n",
      "Final validation accuracy of pruned model: 0.899\n",
      "Original model size: 527.79 MB\n",
      "Pruned model size: 494.03 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "_, acc = eval_step(pruned_model, test_dl, criterion, device)\n",
    "\n",
    "print(f\"Validation accuracy of original (dense) model: {orig_acc:.3f}\")\n",
    "print(f\"Final validation accuracy of pruned model: {acc:.3f}\")\n",
    "\n",
    "pruned_model_size_mb = get_model_size(pruned_model) / MB\n",
    "\n",
    "print(f\"Original model size: {orig_model_size_mb:.2f} MB\")\n",
    "print(f\"Pruned model size: {pruned_model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.7 ms ± 85.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "35.2 ms ± 75.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Test the latency of the original and pruned models\n",
    "x = next(iter(train_dl))[0].cuda()\n",
    "\n",
    "%timeit model(x)\n",
    "%timeit pruned_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the outputs that the pruned model not only (virtually) matches the performance of the original model on the dataset, but is also **actually smaller** and takes **lesser time for inference**.\n",
    "\n",
    "One of the notable advantages of channel-wise pruning is that it does not require special hardware to handle the zeros introduced during fine-grained pruning. Since channel-wise pruning involves removing entire channels rather than zeroing out individual connections, the pruned model becomes inherently more efficient. This structured approach results in a more compact model that requires less storage and computation, making it easier to deploy and operate without the need for hardware designed to optimize sparse matrices. Thus, channel-wise pruning not only helps reduce the model size but also simplifies the computational requirements for inference, leading to overall efficiency gains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
