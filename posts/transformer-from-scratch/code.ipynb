{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers from Scratch\n",
    "\n",
    "We will be implementing a Transformer, specifically a Decoder-only Transformer, from scratch. This will be taken from the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017).\n",
    "\n",
    "The Transformer is a model architecture that has been used in many NLP tasks, such as machine translation, text summarization, and question-answering. It is based on the idea of self-attention, which allows the model to weigh the importance of different words in a sentence when encoding or decoding it.\n",
    "\n",
    "This has been greatly inspired from Andrej Karpathy's [video on building GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY). While this is a poor imitation at best, I hope to explore some aspects I initially found confusing a bit deeper, to play around with different ways to implement the same thing, and to tinker with some ideas that the video does not go into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in our libraries\n",
    "import os\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0161, 0.9698, 0.0022, 0.0119, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.2, 5.3, -0.8, 0.9, float(\"-inf\"), float(\"-inf\"), float(\"-inf\"), float(\"-inf\")])\n",
    "torch.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing in our data\n",
    "\n",
    "We will be using the Tiny Shakespeare dataset, same as the video.\n",
    "\n",
    "Let's load in our data in the cell below and work on processing it to feed it into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tiny shakespeare dataset\n",
    "input_file_path = 'input.txt'\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    \n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded in our data, let's start by tokenizing it.\n",
    "\n",
    "This can be very easily handled with the `tiktoken` library (read up [here](https://github.com/openai/tiktoken)), which is also what the GPT family uses :p\n",
    "\n",
    "A tokenizer will convert a token (a piece of text - could be a word or part of a word) into an integer, which is what we need to feed into our model. We will use the same tokenizer as GPT-2 for this task, which has a vocabulary size of 50257 - this means that we have 50257 unique tokens in our vocabulary, i.e. 50257 unique integers that represent different words or parts of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \"Hello, world!\" --> Encoded: [15496, 11, 995, 0]\n",
      "List of tokens: ['Hello', ',', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Small example\n",
    "sample_text = \"Hello, world!\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Original text: \\\"{sample_text}\\\" --> Encoded: {encoded}\")\n",
    "print(f\"List of tokens: {[tokenizer.decode([token]) for token in encoded]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11])\n",
      "Length of original dataset: 1115394\n",
      "Length of tokenized dataset: 338025\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenize our entire dataset\n",
    "encoded_text = tokenizer.encode(text)\n",
    "# Convert to torch tensor of int64 (important for later on)\n",
    "encoded_text = torch.tensor(encoded_text).long()\n",
    "\n",
    "print(f\"First 10 tokens: {encoded_text[:10]}\")\n",
    "print(f\"Length of original dataset: {len(text)}\")\n",
    "print(f\"Length of tokenized dataset: {len(encoded_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our textual data in numeric form, we can think about how to process it in a way to feed to our model.\n",
    "\n",
    "We will start training our model to predict the next token in a sequence of tokens - the essence of language modeling.\n",
    "\n",
    "We will do this by creating *windows* in our dataset, where each window is a sequence of tokens of a fixed length. We will then train our model to predict the next token in the sequence given the previous tokens.\n",
    "\n",
    "This means that the input to our model will be a sequence of tokens, and the desired output will simply be that sequence shifted by one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597])\n",
      "Target: tensor([22307,    25,   198,  8421,   356,  5120,   597,  2252])\n",
      "--------------------------------------------------------------------------------\n",
      "Context: tensor([5962]) --> Target: 22307\n",
      "Context: tensor([ 5962, 22307]) --> Target: 25\n",
      "Context: tensor([ 5962, 22307,    25]) --> Target: 198\n",
      "Context: tensor([ 5962, 22307,    25,   198]) --> Target: 8421\n",
      "Context: tensor([ 5962, 22307,    25,   198,  8421]) --> Target: 356\n",
      "Context: tensor([ 5962, 22307,    25,   198,  8421,   356]) --> Target: 5120\n",
      "Context: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120]) --> Target: 597\n",
      "Context: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597]) --> Target: 2252\n"
     ]
    }
   ],
   "source": [
    "# Define the size of the window\n",
    "ctx_len = 8\n",
    "\n",
    "# Create a window of size 8 to feed to our model\n",
    "x = encoded_text[:ctx_len]\n",
    "y = encoded_text[1: ctx_len+1]\n",
    "\n",
    "print(f\"Context: {x}\")\n",
    "print(f\"Target: {y}\")\n",
    "print('-'*80)\n",
    "\n",
    "for t in range(ctx_len):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context} --> Target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start thinking of what our model will actually do.\n",
    "\n",
    "It works off taking a chunk of integers, whose length is at most `ctx_len`, and will predict the next integer in the sequence. Note that the chunks can be any length, we just have to specify the maximum context length for our model.\n",
    "\n",
    "When we take a chunk of 8 chars, we don't just predict the next character after this sequence of 8 chars - we train our model to predict **at each and every one of these positions**. This means we have $n$ different training examples for each context of length $n$.\n",
    "\n",
    "The model is being made to predict at contexts with sizes all the way from 1 till `ctx_len`; this means it has the ability to predict the next token and start generating when it's been given just one token of context.\n",
    "\n",
    "Now we can begin to gather multiple windows at once to create minibatches to feed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8]) torch.Size([16, 8])\n",
      "--------------------------------------------------------------------------------\n",
      "Context: [21067] -> Target: 11\n",
      "Context: [21067, 11] -> Target: 3367\n",
      "Context: [21067, 11, 3367] -> Target: 11\n",
      "Context: [21067, 11, 3367, 11] -> Target: 466\n",
      "Context: [21067, 11, 3367, 11, 466] -> Target: 26\n",
      "Context: [21067, 11, 3367, 11, 466, 26] -> Target: 329\n",
      "Context: [21067, 11, 3367, 11, 466, 26, 329] -> Target: 356\n",
      "Context: [21067, 11, 3367, 11, 466, 26, 329, 356] -> Target: 1276\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "def get_batch():\n",
    "    \"\"\"\n",
    "    Returns a batch of data for training\n",
    "    \"\"\"\n",
    "    # Sample batch_size number of starting indices to create our windows\n",
    "    idxs = torch.randint(0, len(encoded_text) - ctx_len, (batch_size,))\n",
    "\n",
    "    # Get our inputs and targets\n",
    "    x = torch.stack([encoded_text[idx:idx+ctx_len] for idx in idxs])\n",
    "    y = torch.stack([encoded_text[idx+1:idx+ctx_len+1] for idx in idxs])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(xb.shape, yb.shape)\n",
    "print('-'*80)\n",
    "\n",
    "# Print out window's examples - this is a single item in our batch\n",
    "for b in range(1):\n",
    "    for t in range(ctx_len):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'Context: {context.tolist()} -> Target: {target.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "The output of the cell above shows exactly what our model would see and what it would try to predict for each item in the batch.\n",
    "\n",
    "Our model would take as input a $(B, T)$ tensor and output a $(B, T, V)$ tensor, where $B$ is the batch size, $T$ is the sequence length, and $V$ is the vocabulary size. \n",
    "\n",
    "This is because we are predicting the probability distribution of the next token for all $B \\times T$ **positions** in the input. This means we will have exactly $B \\times T$ different training examples for each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "The whole point of Self-Attention is to get the tokens interacting and talking with one another. The idea is to bake context into the raw representations of the tokens themselves.\n",
    "\n",
    "The way this is done is to take each token, and to extract three pieces of information from it: the Query, the Key, and the Value. These have the following ideas behind them:\n",
    "\n",
    "* The Query says \"What am I looking for/Here's what I'm interested in...\"\n",
    "\n",
    "* The Key says \"What do I contain/This is what I have...\"\n",
    "\n",
    "* The Value says \"If you find me interesting, here's what I will communicate to you...\"\n",
    "\n",
    "The way we get the *affinities between tokens* now is to simply take a dot product between the Query and the Key. \n",
    "\n",
    "As an example: if some token, say at position 8 (whose Query we take), finds that a token at postion 4 produces a Key that generates a high dot product value, then the model would have learned something important about the meaning of that 8th token. It now knows that to better understand the 8th token, it should look at the 4th token as well for context.\n",
    "\n",
    "The way we extract these pieces is to simply perform a linear transformation on the input embeddings to get the Query, Key, and Value matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
