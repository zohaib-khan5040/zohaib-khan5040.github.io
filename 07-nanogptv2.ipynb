{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build GPT\n",
    "\n",
    "## Loading and Tokenizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394 65\n"
     ]
    }
   ],
   "source": [
    "# Read in the text file\n",
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Get all the unique chars in this list to create our vocab\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(len(text), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "stoi = {ch: i for i,ch in enumerate(chars)}\n",
    "itos = {i: ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]             # encode: string -> list of ints\n",
    "decode = lambda l: ''.join([itos[i] for i in l])    # decode: list of ints -> string\n",
    "\n",
    "print(encode(\"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the entire piece of text and store as torch.tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data.shape  # vector of ints corresponding to each and every char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start thinking of what our model will actually do.\n",
    "\n",
    "It works off taking a chunk of integers, whose length is at most `ctx_len`, and will predict the next integer in the sequence. Note that the chunks can be any length, we just have to specify the maximum context length for our model.\n",
    "\n",
    "When we take a chunk of 8 chars, we don't just predict the next character after this sequence of 8 chars - we train our model to predict **at each and every one of these positions**. This means we have $n$ different training examples for each context of length $n$.\n",
    "\n",
    "The model is being made to predict at contexts with sizes all the way from 1 till `ctx_len`; this means it has the ability to predict the next token and start generating when it's been given just one token of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] --> 47\n",
      "[18, 47] --> 56\n",
      "[18, 47, 56] --> 57\n",
      "[18, 47, 56, 57] --> 58\n",
      "[18, 47, 56, 57, 58] --> 1\n",
      "[18, 47, 56, 57, 58, 1] --> 15\n",
      "[18, 47, 56, 57, 58, 1, 15] --> 47\n",
      "[18, 47, 56, 57, 58, 1, 15, 47] --> 58\n"
     ]
    }
   ],
   "source": [
    "ctx_len = 8\n",
    "\n",
    "x = data[:ctx_len]\n",
    "y = data[1:ctx_len+1]   # the target is the window offset by 1 token\n",
    "\n",
    "for t in range(ctx_len):\n",
    "    context = x[:t+1]   # grab the first t tokens in x\n",
    "    target = y[t]       # since y is just x shifted by 1\n",
    "    print(f\"{context.tolist()} --> {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[41, 58, 47, 53, 52,  1, 42, 47],\n",
      "        [56, 47, 52, 45,  1, 46, 43, 50],\n",
      "        [15, 39, 50, 39, 47, 57,  0, 16],\n",
      "        [58, 46, 53, 59, 57, 39, 52, 42]])\n",
      "\n",
      "tensor([[58, 47, 53, 52,  1, 42, 47, 45],\n",
      "        [47, 52, 45,  1, 46, 43, 50, 51],\n",
      "        [39, 50, 39, 47, 57,  0, 16, 47],\n",
      "        [46, 53, 59, 57, 39, 52, 42,  1]])\n",
      "\n",
      "[41] --> 58\n",
      "[41, 58] --> 47\n",
      "[41, 58, 47] --> 53\n",
      "[41, 58, 47, 53] --> 52\n",
      "[41, 58, 47, 53, 52] --> 1\n",
      "[41, 58, 47, 53, 52, 1] --> 42\n",
      "[41, 58, 47, 53, 52, 1, 42] --> 47\n",
      "[41, 58, 47, 53, 52, 1, 42, 47] --> 45\n"
     ]
    }
   ],
   "source": [
    "# Now to create a function to generate a batch of these chunks\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch():\n",
    "    # All we really need is a set of batch_size random indices\n",
    "    idxs = torch.randint(len(data)-ctx_len, (batch_size,))  # (B,)\n",
    "    x = torch.stack([data[i:i+ctx_len] for i in idxs])      # (B, ctx_len)\n",
    "    y = torch.stack([data[i+1:i+ctx_len+1] for i in idxs])  # (B, ctx_len) (x offset by 1)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch()\n",
    "print(x)\n",
    "print()\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "# Print out the examples for the first sequence in the batch\n",
    "for t in range(ctx_len):\n",
    "    context = x[0, :t+1]   # grab the first t tokens in x\n",
    "    target = y[0, t]       # since y is just x shifted by 1\n",
    "    print(f\"{context.tolist()} --> {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Language Model\n",
    "\n",
    "The simplest model we can start with is the  language model, where we predict the next character in the sequence *purely* by looking at the current character.\n",
    "\n",
    "This can be formulated as using Embeddings - a simple lookup table - to get the representation of the current character. We could set the embedding dimensionality to be the same as the vocab size so that our output from the table can be interpreted as a probability distribution over the vocabulary.\n",
    "\n",
    "So, we're taking as input a $(B,T)$ tensor, and the model will output a $(B,T,V)$ tensor, where $V$ is the vocabulary size. This is the case because we are predicting a token (i.e. predicting the probability distribution over the entire vocab), for all $B \\times T$ positions in the input.\n",
    "\n",
    "The point of interest is that the tokens are treated independently of each other, in a way, i.e. only one token is used to predict the next token. We'd like to let the tokens *interact* with each other in a way, so that the model can learn more complex patterns, with the context windows we've defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "4.483732223510742\n"
     ]
    }
   ],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \n",
    "        logits = self.emb_table(x) # (B, T, C)\n",
    "\n",
    "        if targets is not None:\n",
    "            # Get the loss here too\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, vocab_size), # (B*T, C) - predicting for each token\n",
    "                targets.view(-1)            # (B*T)   - the true target\n",
    "            )\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, ctx, max_new_tokens):\n",
    "        '''\n",
    "        Generate a sequence of new tokens given a context.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx: torch.tensor (B, T)\n",
    "            The starting context to condition on.\n",
    "\n",
    "        max_new_tokens: int\n",
    "            The maximum number of new tokens to generate.\n",
    "        '''\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Get the predictions\n",
    "            logits, _ = self(ctx)\n",
    "            logits = logits[:, -1, :]  # focus on the last token to predict what comes next - (B,C) now\n",
    "\n",
    "            # Normalize and sample the next token\n",
    "            probas = F.softmax(logits, dim=-1) # (B, C)\n",
    "            next_token = torch.multinomial(probas, 1) # (B, 1)\n",
    "\n",
    "            # Update context\n",
    "            ctx = torch.cat([ctx, next_token], dim=1)\n",
    "        \n",
    "        return ctx\n",
    "\n",
    "    \n",
    "model = LanguageModel(vocab_size)\n",
    "logits, loss = model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n.a!BmqpFup'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate from the model with an empty context - 0 is newline char\n",
    "idx = torch.tensor([0]).view(1,1) # (B,T)\n",
    "\n",
    "preds = model.generate(idx, 10)\n",
    "\n",
    "decode(preds[0].tolist())           # decode the first sequence in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's improve upon this design.\n",
    "\n",
    "Let's use the aforementioned Embedding table just to get the embeddings or representations for the tokens themselves. Then, we can use a simple feedforward network to predict the next token, using the embeddings of the tokens as input.\n",
    "\n",
    "Alongside this, we will put in an additional Embedding table to encode the position of the token in the sequence. This is because the model should be able to learn that the first token in the sequence is different from the second token, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelv2(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim=32):\n",
    "\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(ctx_len, emb_dim)\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Get the logits (incorporating positional information too)\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(\n",
    "            torch.arange(T, device=x.device) # (T,) - the position of each token\n",
    "        )\n",
    "        x = tok_emb + pos_emb   # (B, T, C)\n",
    "        logits = self.lm_head(x)    # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            # Get the loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, vocab_size), # (B*T, C) - predicting for each token\n",
    "                targets.view(-1)            # (B*T)   - the true target\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, ctx, max_new_tokens):\n",
    "        '''\n",
    "        Generate a sequence of new tokens given a context.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx: torch.tensor (B, T)\n",
    "            The starting context to condition on.\n",
    "\n",
    "        max_new_tokens: int\n",
    "            The maximum number of new tokens to generate.\n",
    "        '''\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Get the predictions\n",
    "            logits, _ = self(ctx)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Normalize and sample the next token\n",
    "            probas = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probas, 1)\n",
    "\n",
    "            # Update context\n",
    "            ctx = torch.cat([ctx, next_token], dim=1)\n",
    "\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematical trick in Self-Attention\n",
    "\n",
    "The whole point of Self-Attention is to get the tokens interacting and talking with one another. In the previous implementation of just using the last token to predict the next one, the tokens were decoupled in a way, and incapable of interacting with each other.\n",
    "\n",
    "The simplest way of getting other tokens to mingle with one another is to compute the average of the $t$-th token's representation with all the other tokens' representations.\n",
    "\n",
    "We want to do this in a very specific way, as to only incorporate information that is *not* from the future, i.e. only the previous tokens in the context can be used for this aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2 # batch size, context length, channels/hidden size\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1816,  0.1048],\n",
      "        [ 0.3941,  0.5729],\n",
      "        [ 1.5386,  0.4016],\n",
      "        [ 0.8023,  1.2386],\n",
      "        [ 0.6774, -1.2224],\n",
      "        [-1.0634,  0.3195],\n",
      "        [-0.4746,  0.6998],\n",
      "        [ 0.4340,  0.5938]])\n",
      "\n",
      "tensor([[-0.1816,  0.1048],\n",
      "        [ 0.1062,  0.3389],\n",
      "        [ 0.5837,  0.3598],\n",
      "        [ 0.6384,  0.5795],\n",
      "        [ 0.6462,  0.2191],\n",
      "        [ 0.3612,  0.2358],\n",
      "        [ 0.2418,  0.3021],\n",
      "        [ 0.2659,  0.3386]])\n"
     ]
    }
   ],
   "source": [
    "# Compute the new representation of each token as the average of all the previous tokens\n",
    "# Version 1: naive implementation\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]              # grab the first t tokens in x - (t+1, C)\n",
    "        xbow[b, t] = xprev.mean(dim=0)  # average across the time dimension - (B, T, C)\n",
    "\n",
    "# Note how the first t rows of x are averaged to give the t-th row of xbow\n",
    "print(x[0])\n",
    "print()\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the first element in both matrices is the same (since it has no extra context). On top of this, the second row in the second matrix is an average of the first two rows in the first matrix, and so on. This is taking as much context as we can.\n",
    "\n",
    "There is a lot of information lost when we process things this way. We can be more efficient if we use Matrix Multiplication.\n",
    "\n",
    "If we can set up the matrices as being of shape $(T, C)$, this means creating a matrix of weights of shape $(T, T)$, and multiplying the two matrices together, we can get the same result as the previous method. \n",
    "\n",
    "This is because each row in the weight matrix (having a uniform distribution) will be multiplied with a column of the input matrix (representing a single feature across all tokens).\n",
    "\n",
    "A lower-triangular matrix is perfect for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2\n",
    "ws = torch.tril(torch.ones(T, T))\n",
    "ws = ws / ws.sum(-1, keepdim=True)  # normalize the ws to sum to 1 along the time dimension\n",
    "print(ws)\n",
    "\n",
    "# ws @ x --> (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "xavg = ws @ x\n",
    "torch.allclose(xavg, xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even cooler way of doing this is with the Softmax Activation Function, in that we perform it across the rows of each row in the weight matrix.\n",
    "\n",
    "If we start off with a matrix of all ones, we can set up the post-softmax matrix as getting zeros in the upper region, by filling it with negative infinity in the upper region, and then applying the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# Create a weights matrix that is filled with -inf in the top right corner\n",
    "# This is what measures the affinities between tokens when we perform the aggregation\n",
    "ws = torch.zeros((T, T))\n",
    "ws[tril == 0] = float('-inf')\n",
    "print(ws)\n",
    "\n",
    "# Apply softmax to the ws\n",
    "# The -inf masking will say \"tokens cannot communicate with anything in the future\"\n",
    "ws = F.softmax(ws, dim=-1)\n",
    "print(ws)\n",
    "\n",
    "# Apply our matmul again\n",
    "# ws @ x --> (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "xavg = ws @ x\n",
    "torch.allclose(xavg, xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "We can use the tricks above to implement Self Attention properly now. Instead of using the simple average with the Lower Triangular Matrix (that implements the notion of *blocking out the future*), we want to more towards a more sophisticated form of these affinities (rather than relying on uniform numbers). **Self-Attention solves this in a data-driven way**.\n",
    "\n",
    "The idea is this: every token in the input sequence emits a **query** and a **key** (alongside the **value**). These have the following ideas behind them:\n",
    "* The Query says \"What am I looking for/Here's what I'm interested in...\"\n",
    "* The Key says \"What do I contain/This is what I have...\"\n",
    "* The Value says \"If you find me interesting, here's what I will communicate to you...\"\n",
    "\n",
    "The way we get the *affinities between tokens* now is to simply take a dot product between the Query and the Key. \n",
    "\n",
    "The Query for a *specific token* emits a certain value, representing what it's looking for. Now, all the tokens in the input sequence emit their Keys, representing what that token is offering. If that specific token, say at position 8 (whose Query we take), finds that a token at postion 4 produces a high value when we take the dot product of the Query and Key (at positions 8 and 4 respectively), then the model has learned something meaningful about the meaning of that 8th token (new information has been aggregated, so the model has learned more *about it*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attention weights: torch.Size([4, 8, 8])\n",
      "Shape of aggregated values: torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# Create an input matrix whose token representations we wish to refine\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Create linear functions to get the query and key matrices\n",
    "head_size = 16  # the size of the query and key matrices\n",
    "\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "q = query(x)  # (B, T, head_size) - produce all the keys of the tokens independently of each other\n",
    "k = key(x)    # (B, T, head_size) - produce all the queries of the tokens independently of each other\n",
    "v = value(x)  # (B, T, head_size) - produce all the values of the tokens independently of each other\n",
    "\n",
    "# Compute the attention scores/affinities (that function as the ws from before)\n",
    "# This is where they start mingling with each other\n",
    "# q @ k.T --> (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "ws = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "print(f\"Shape of attention weights: {ws.shape}\")\n",
    "\n",
    "# With our attention scores, now we can do the same masking+softmax procedure as before\n",
    "mask = torch.tril(torch.ones(T, T))\n",
    "ws = ws.masked_fill(mask == 0, float('-inf'))\n",
    "ws = F.softmax(ws, dim=-1)\n",
    "\n",
    "# Use these attention weights to aggregate the values (what each token has to offer)\n",
    "out = ws @ v  # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)\n",
    "print(f\"Shape of aggregated values: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.2145e-01, 3.7855e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.0093e-01, 4.2380e-01, 3.7527e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.6418e-01, 3.2049e-01, 2.6588e-01, 2.4945e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.6415e-01, 2.0907e-01, 2.3029e-01, 1.8816e-01, 2.0834e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.4858e-01, 1.3391e-01, 8.5627e-02, 2.0435e-01, 1.3707e-01,\n",
       "          9.0452e-02, 0.0000e+00, 0.0000e+00],\n",
       "         [1.1828e-01, 1.5938e-01, 5.2063e-02, 1.5644e-01, 1.9018e-01,\n",
       "          1.1777e-01, 2.0589e-01, 0.0000e+00],\n",
       "         [2.1552e-01, 1.1294e-01, 8.1352e-02, 1.5061e-01, 1.1517e-01,\n",
       "          8.5889e-02, 3.5706e-02, 2.0280e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [7.5342e-01, 2.4658e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.5640e-02, 6.6041e-01, 3.2395e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.9742e-01, 2.3395e-01, 2.2253e-01, 2.4610e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [7.3252e-02, 1.7358e-01, 4.5756e-01, 1.4860e-01, 1.4701e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.9234e-01, 1.5837e-01, 1.5045e-01, 1.6494e-01, 1.6525e-01,\n",
       "          1.6866e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [2.6379e-02, 1.9916e-01, 1.3305e-01, 1.2642e-01, 1.2426e-01,\n",
       "          1.3929e-01, 2.5144e-01, 0.0000e+00],\n",
       "         [1.6238e-01, 1.1107e-01, 1.5701e-02, 1.1294e-01, 1.1400e-01,\n",
       "          2.2634e-01, 1.7201e-01, 8.5567e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [8.2961e-01, 1.7039e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.5479e-01, 1.2077e-02, 8.3313e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.4634e-01, 1.9325e-01, 3.7714e-01, 1.8327e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [8.3693e-02, 3.3405e-01, 6.0898e-02, 2.6841e-01, 2.5295e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.5232e-02, 2.7612e-01, 4.9553e-02, 2.1677e-01, 2.1548e-01,\n",
       "          1.7684e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [1.7053e-01, 1.0851e-01, 2.0595e-01, 1.1413e-01, 1.2655e-01,\n",
       "          1.2716e-01, 1.4717e-01, 0.0000e+00],\n",
       "         [1.0383e-01, 6.3937e-03, 5.8486e-01, 7.5932e-03, 2.5099e-02,\n",
       "          1.8876e-02, 1.9880e-01, 5.4548e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [4.5356e-01, 5.4644e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.6181e-01, 7.3380e-01, 4.3913e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.0160e-01, 7.3547e-02, 7.7035e-01, 5.4499e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.8555e-01, 2.4556e-01, 6.3503e-02, 2.6611e-01, 2.3928e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.0220e-03, 3.1959e-04, 9.9449e-01, 2.3598e-04, 4.9655e-04,\n",
       "          2.4335e-03, 0.0000e+00, 0.0000e+00],\n",
       "         [5.9158e-02, 4.3681e-02, 4.3205e-01, 3.2226e-02, 3.4650e-02,\n",
       "          1.9151e-01, 2.0672e-01, 0.0000e+00],\n",
       "         [2.4390e-02, 6.2191e-03, 8.8002e-01, 6.4671e-03, 1.1816e-02,\n",
       "          8.5374e-03, 3.3710e-02, 2.8837e-02]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note how this time the weights are different across batches - i.e. they aren't uniform anymore\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes:\n",
    "* Attention is a **communication mechanism** in that it allows nodes in any directed graph to aggregate information from other nodes, specifically those that *point to it*. We can think of text as being a specific type of directed graph where the first token only points to itself, the second is being pointed to by the first (and itself), and so on, until the last token is being pointed to by *all the previous nodes, and itself*.\n",
    "\n",
    "* Attention has no notion of *space*, which is the reason why we implement the Positional Embeddings that we'll see later.\n",
    "\n",
    "* Each instance in the batch is processed independently of one another. So if we have 4 batches each having a max context length of 8, then we really have a total of 32 nodes in our graph. But because each batch is processed independently, the nodes in one batch don't communicate with the nodes in another batch - i.e. there are no edges between nodes of different batches.\n",
    "\n",
    "* In an Encoder, if we don't want to restrict the tokens talking to the future tokens, we just get rid of the masking portion of the code with the lower triangular matrix. In the Decoder though, when we are trying to generate new tokens, we don't want the future tokens communicating with the past tokens (otherwise they'd be giving away the answer, on top of this interaction not even being possible), which is why we'd bring the masking feature in there.\n",
    "\n",
    "* \"Self-Attention\" is specifically Attention when the Keys, Queries and Values are coming from the same source. Attention is more general than that: \"Cross Attention\" is when the Keys and the Values are coming from an external source outside the source of the Keys.\n",
    "\n",
    "**Side note:** why is it we divide by $\\sqrt{d_k}$ in computing the attention scores?\n",
    "\n",
    "We find that if we don't do this, then the result of the pre-softmax Attention Scores incur a very high variance, on the order of $d_k$. When they incur this high variance, this means some values are highly positive while others are much smaller - this means that post-Softmax, we may find a vector that is similar to a one-hot vector (which means that each token will only aggregate information from a single node only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of query: 0.9703316688537598\n",
      "Variance of key: 0.9714754819869995\n",
      "Variance of Q @ K.T: 13.188318252563477\n",
      "Variance of (Q @ K.T)/sqrt(head_size): 0.8242698907852173\n"
     ]
    }
   ],
   "source": [
    "# Instantiate query and keys as being unit-gaussian\n",
    "q = torch.randn(B, T, 16)\n",
    "k = torch.randn(B, T, 16)\n",
    "\n",
    "print(f\"Variance of query: {q.var()}\")\n",
    "print(f\"Variance of key: {k.var()}\")\n",
    "print(f\"Variance of Q @ K.T: {(q @ k.transpose(-2, -1)).var()}\")\n",
    "\n",
    "# Note how the variance of the dot prod is on the order of head size\n",
    "# We divide by that amount to keep the variance of the dot prod at 1\n",
    "ws = q @ k.transpose(-2, -1)\n",
    "ws = ws / (16**0.5)\n",
    "print(f\"Variance of (Q @ K.T)/sqrt(head_size): {ws.var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "We start off by incorporating this mechanism as its own module.\n",
    "\n",
    "The procedure is the following:\n",
    "\n",
    "1. Take the input embeddings as a `(B, T, C)` matrix.\n",
    "\n",
    "2. Extract the Query, Key, and Value matrices from it.\n",
    "\n",
    "3. Compute the scaled Attention Scores, with the dot product and constant factor.\n",
    "\n",
    "4. Apply the masking to the upper triangular region, filling in with `-inf`, before passing through a Softmax.\n",
    "\n",
    "5. Take the matmul with the Value matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2]) ---> torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "emb_dim = head_size\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
    "\n",
    "        # Register a buffer that we use for masking (Decoder structure)\n",
    "        self.register_buffer(\n",
    "            \"tril\",\n",
    "            torch.tril(torch.ones((ctx_len, ctx_len)))  # same thing as before\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Get the qkv matrices\n",
    "        q = self.query(x)   # (B, T, C)\n",
    "        k = self.key(x)     # (B, T, C)\n",
    "        v = self.value(x)   # (B, T, C)\n",
    "\n",
    "        # Compute the (scaled) Attention Scores\n",
    "        # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        att_scores = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "\n",
    "        # Perform the masking\n",
    "        att_scores = att_scores.masked_fill(\n",
    "            self.tril[:T, :T] == 0,             # be careful if T < ctx_len\n",
    "            float('-inf')\n",
    "        )\n",
    "        att_scores = F.softmax(att_scores, dim=-1)\n",
    "\n",
    "        # Aggregate with the value vectors\n",
    "        # (B, T, T) @ (B, T, C) --> (B, T, T)\n",
    "        out = att_scores @ v\n",
    "        \n",
    "        return out\n",
    "    \n",
    "x = torch.randn(4, 8, 2)\n",
    "sa_head = Head(2, 2)\n",
    "out = sa_head(x)\n",
    "print(f\"{x.shape} ---> {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've made this class, we can incorporate it into our Language Model implementation.\n",
    "\n",
    "This is just one additional line of code, where we pass our embedded inputs, with positional encodings, into this module - this just _refines_ the representations, making no changes to the actual shapes (provided we let `emb_dim == head_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelv3(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim=32):\n",
    "\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(ctx_len, emb_dim)\n",
    "        self.sa_head = Head(emb_dim, head_size)\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Get the logits (incorporating positional information too)\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(\n",
    "            torch.arange(T, device=x.device) # (T,) - the position of each token\n",
    "        )\n",
    "        x = tok_emb + pos_emb   # (B, T, C)\n",
    "\n",
    "        # New line here: refine the representations with Self-Attention\n",
    "        x = self.sa_head(x)\n",
    "        # END\n",
    "\n",
    "        logits = self.lm_head(x)    # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            # Get the loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, vocab_size), # (B*T, C) - predicting for each token\n",
    "                targets.view(-1)            # (B*T)   - the true target\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, ctx, max_new_tokens):\n",
    "        '''\n",
    "        Generate a sequence of new tokens given a context.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx: torch.tensor (B, T)\n",
    "            The starting context to condition on.\n",
    "\n",
    "        max_new_tokens: int\n",
    "            The maximum number of new tokens to generate.\n",
    "        '''\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Crop to the last ctx_len tokens\n",
    "            idx_cond = idx[:, -ctx_len:]\n",
    "\n",
    "            # Get the predictions\n",
    "            logits, _ = self(ctx)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Normalize and sample the next token\n",
    "            probas = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probas, 1)\n",
    "\n",
    "            # Update context\n",
    "            ctx = torch.cat([ctx, next_token], dim=1)\n",
    "\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Self Attention\n",
    "\n",
    "To allow for **more independent communication channels**, which means allowing for tokens to take on more than just one combination of context clues, we allow for the Transformer to use multiple of these Self Attention Heads.\n",
    "\n",
    "Note however that we change up the dimension of the Head Size, since at the end, we want the output to stay the same, after we concatenate information from all the heads.\n",
    "\n",
    "Note that the processing for each head will run in parallel so we suffer no cost in terms of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32]) ---> torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads, head_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(emb_dim, head_size) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([\n",
    "            head(x) for head in self.heads      # compute output for each head in parallel\n",
    "        ], dim=-1)                              # concatenate along the C channel\n",
    "\n",
    "x = torch.randn(4, 8, 32)\n",
    "mhsa = MultiHeadSelfAttention(emb_dim=32, num_heads=4, head_size=32//4)\n",
    "out = mhsa(x)\n",
    "print(f\"{x.shape} ---> {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we created this using our single `Head` class, we could have also done this in one go.\n",
    "\n",
    "The following snippet is by Copilot:\n",
    "\n",
    "```python\n",
    "class MultiHeadSelfAttentionv0(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(emb_dim, emb_dim)\n",
    "        self.key = nn.Linear(emb_dim, emb_dim)\n",
    "        self.value = nn.Linear(emb_dim, emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        queries = self.query(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        keys = self.key(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        values = self.value(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        \n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, T, T)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)  # (B, num_heads, T, T)\n",
    "        \n",
    "        attended_values = torch.matmul(attention_probs, values)  # (B, num_heads, T, head_dim)\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, emb_dim)\n",
    "        \n",
    "        output = self.fc(attended_values)  # (B, T, emb_dim)\n",
    "        \n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Layers\n",
    "\n",
    "In the paper, the authors did not immediately feed in the processed inputs to a classification layer, rather there were intermediate FeedForward layers that allowed for more of this intermediate processing of activations.\n",
    "\n",
    "This can be thought of as Self-Attention and the multiple heads aggregating that information, and the Feedforward layers allowing these tokens to *think* on this new aggregated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4*emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*emb_dim, emb_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks, Residual Connections and Layer Normalization\n",
    "\n",
    "The Transformer can be divided blocks that are further segmented into two separate components:\n",
    "\n",
    "* the MHSA that performs the **communication**\n",
    "\n",
    "* the Feedforward layers that perform the **computation**\n",
    "\n",
    "Each block consists of the MHSA module, followed by the Feedforward network on all the tokens independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "\n",
    "        super().__init__()\n",
    "        self.head_size = emb_dim // num_heads\n",
    "        self.mhsa = MultiHeadSelfAttention(emb_dim, num_heads, self.head_size)\n",
    "        self.ff = FeedForward(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.mhsa(x)\n",
    "        x = self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we start to create a lot more blocks in the architecture, we find that this model ends up becoming rather *deep*. \n",
    "\n",
    "This could pose certain issues regarding the optimization of the parameters.\n",
    "\n",
    "The authors of the paper utilize two approaches that aim to resolve these optimization issues.\n",
    "\n",
    "First we have **(1) Skip/Residual Connections** that distributes gradients equally along both a *residual pathway* (going unimpeded from the inputs to the outputs) - we fork off, do some computation, then come back. We have to implement these skip connections (via addition) and projections in a few of our classes.\n",
    "\n",
    "The other improvement we have is **(2) Layer Norm**. This is very similar to Batch Normalization, except that now, instead of normalizing along the columns, we normalize along the rows (for every single example).\n",
    "\n",
    "We don't need any distinction between training and test time, we don't require any buffers for the running mean and variances, we can apply this simpler algorithm any time we wish with no previous state (except the parameters).\n",
    "\n",
    "Slight deviation from the initial paper: the `LayerNorm` is applied *before* each of the transformations, rather than after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the old classes here again\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, emb_dim=32, ctx_len=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(ctx_len, ctx_len)))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape # batch size, num tokens, embedding dim\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ (k.transpose(-1, -2)) / (C**0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # decoder block\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads, ctx_len):\n",
    "        super().__init__()\n",
    "        head_size = emb_dim // num_heads\n",
    "        self.heads = nn.ModuleList([Head(head_size, emb_dim=emb_dim, ctx_len=ctx_len) for _ in range(num_heads)])\n",
    "\n",
    "        # NEW: add in a projection layer to let the model \"think\" on the aggregated information even more\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim) # project back to original embedding dim\n",
    "        # -----\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # output of raw MHSA: (B, T, C)\n",
    "        out = self.proj(out) # projection back to original embedding dim\n",
    "        return out\n",
    "\n",
    "class LayerNorm:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Normalize the rows for each example\n",
    "        xmean = x.mean(dim=1, keepdim=True)\n",
    "        xvar = x.var(dim=1, keepdim=True)\n",
    "        xhat = (x-xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4*emb_dim), # small change: 4x bigger hidden layer (from the paper)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*emb_dim, emb_dim), # projection back into residual pathway\n",
    "            nn.Dropout(0.2), # NEW: dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we bring all of this together to form one `Block`, which contains the MHSA module, the FF net, the LayerNorm modules, and the skip connections.\n",
    "\n",
    "Note the output shape of this: its still really just refining the input representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 384]) ---> torch.Size([64, 256, 384])\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads, ctx_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sa_heads = MultiHeadAttention(emb_dim=emb_dim, num_heads=num_heads, ctx_len=ctx_len)\n",
    "        \n",
    "        self.ffwd = Feedforward(emb_dim=emb_dim)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim) # for each MHSA layer\n",
    "        self.ln2 = nn.LayerNorm(emb_dim) # for the feedforward layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # NEW: Add in the skip connections, and the LayerNorms\n",
    "        x = x + self.sa_heads(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(64, 256, 384)\n",
    "block = Block(384, 8, 256)\n",
    "out = block(x)\n",
    "print(f\"{x.shape} ---> {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final Transformer Decoder\n",
    "\n",
    "Now we can take all of these pieces to make our complete Transformer Decoder.\n",
    "\n",
    "The hyperparameters to take note of here are:\n",
    "\n",
    "* `num_layers`\n",
    "\n",
    "* `num_heads`\n",
    "\n",
    "* `emb_dim`\n",
    "\n",
    "* `ctx_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,468,993 parameters\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, num_heads, emb_dim, ctx_len):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the two embedding tables\n",
    "        self.tok_emb_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_emb_table = nn.Embedding(ctx_len, emb_dim)\n",
    "\n",
    "        # Create the blocks for communication + computation\n",
    "        self.blocks = nn.Sequential(*[Block(emb_dim, num_heads, ctx_len) for _ in range(num_layers)])\n",
    "        self.ln_final = nn.LayerNorm(emb_dim) # for right after all the blocks\n",
    "\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        B, T = x.shape\n",
    "\n",
    "        # Create the inputs for the actual Decoder module\n",
    "        tok_embs = self.tok_emb_table(x)                    # (B, T, C)\n",
    "        pos_embs = self.pos_emb_table(torch.arange(T))      # (T, C)\n",
    "        x = tok_embs + pos_embs                             # (B, T, C)\n",
    "\n",
    "        # Pass through the blocks, and the final LayerNorm\n",
    "        x = self.blocks(x)                                  # (B, T, C)\n",
    "        x = self.ln_final(x)                                # (B, T, C)\n",
    "\n",
    "        # Get the logits\n",
    "        logits = self.lm_head(x)                                 # (B, T, V)\n",
    "\n",
    "        # Ready the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(B*T, V),\n",
    "                targets.view(-1)\n",
    "            )\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idxs, max_new_tokens):\n",
    "        \n",
    "        # idxs is a B,T tensor of token indices\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Crop the input to be some max length\n",
    "            idxs_cropped = idxs[:, -ctx_len:]\n",
    "\n",
    "            logits, _ = self(idxs_cropped) # forward pass\n",
    "            logits = logits[:, -1, :] # focus only on last token (Bigram model)\n",
    "            probs = F.softmax(logits, dim=-1) # get probabilities\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # sample from this distribution\n",
    "\n",
    "            idxs = torch.cat((idxs, next_idx), dim=1) # (B, T) -> (B, T+1) append to the right\n",
    "\n",
    "        return idxs\n",
    "\n",
    "model = GPT(\n",
    "    num_layers=3, \n",
    "    num_heads=8, \n",
    "    emb_dim=384, \n",
    "    ctx_len=256\n",
    ")\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PDf--apdsNfKJlmlmQZTtOoe?DXhwpvZy3\n",
      "J:KKRPO!gdFykhdCQUMccBmSKoUeXnOdnxhGB:LGrYqCnAfqOpnfhj:xHPBfI,$3!VgZTdCrsI bjXAH?J:ZMdNRy:g$tGMbqLj?YA-XQBSbZASiXlua\n",
      ".$.ILVr\n",
      "d$NCHKjvkoqu,xmx,v:3CYz?dv!xj3'DZgvsx?C.Igf;laF HO!vpMf wjK&BRk-.\n",
      "jxiZ'hWjsBc'y3GmGKxvIRc:3:.AVF,D\n",
      ".AV3Wv?GVAO3-ddv-\n",
      "?L:w:sDLEE\n",
      "BwGj$gdr3XNgzolYZskdj$\n",
      "xp xt\n",
      ",KqKEO?;MzVEjK!N3-cdZmLRvsBC-h:LqvjKPJZrHXwrtHE$&ZeOskmT$H:dfq h'xzwKESg$mgJY\n",
      "AOXilyuXmkR$voBt3HMVat':QRFedYWrk$ S XNJa.wq:p\n",
      "\n",
      "-esrmFbEv'$gk&-R\n",
      "ojEBB:so\n",
      "TLXEJr$Z:'zDfc:JiPJ;Nq--G&eLZOz\n"
     ]
    }
   ],
   "source": [
    "# Check whether the model is able to generate something\n",
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "model_gen = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(model_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finish, let's create some utility functions to kick off training and evaluation for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(split=\"val\", eval_iters=200):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    losses = torch.zeros(eval_iters)\n",
    "\n",
    "    for k in range(eval_iters): # find the average loss over 200 batches\n",
    "        \n",
    "        X, Y = get_batch(split)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "        \n",
    "    model.train()\n",
    "    return losses.mean().item()\n",
    "\n",
    "def train_model(lr=1e-3, max_epochs=100, eval_interval=10):\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        if epoch % eval_interval == 0 or epoch == max_epochs-1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"Epoch {epoch}: train loss {losses['train']:.3f}, val loss {losses['val']:.3f}\")\n",
    "\n",
    "        xb, yb = get_batch()\n",
    "\n",
    "        # Evaluate loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
